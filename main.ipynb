{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a530c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theo\\anaconda3\\envs\\newenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gmean\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboard_logger import Logger\n",
    "\n",
    "from resnet import resnet50\n",
    "from loss import *\n",
    "from datasets import AgeDB\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db1f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite previous folder: checkpoint\\agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64 ? [Y/n] :Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:18:54,072 | Args: Namespace(lds=True, lds_kernel='gaussian', lds_ks=9, lds_sigma=1, fds=False, fds_kernel='gaussian', fds_ks=9, fds_sigma=1, start_update=0, start_smooth=1, bucket_num=100, bucket_start=3, fds_mmt=0.9, reweight='inverse', retrain_fc=False, dataset='agedb', data_dir='./Faces/UTKFace/', model='resnet50', store_root='checkpoint', store_name='agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64', gpu=None, optimizer='adam', loss='l1', lr=0.001, epoch=30, momentum=0.9, weight_decay=0.0001, schedule=[60, 80], batch_size=64, print_freq=10, img_size=224, workers=16, resume='', pretrained='', evaluate=False, augment=True, start_epoch=0, best_loss=100000.0)\n",
      "2022-11-30 18:18:54,073 | Store name: agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\\agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64 removed.\n",
      "===> Creating folder: checkpoint\\agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# imbalanced related\n",
    "# LDS\n",
    "parser.add_argument('--lds', action='store_true', default=True, help='whether to enable LDS')\n",
    "parser.add_argument('--lds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='LDS kernel type')\n",
    "parser.add_argument('--lds_ks', type=int, default=9, help='LDS kernel size: should be odd number')\n",
    "parser.add_argument('--lds_sigma', type=float, default=1, help='LDS gaussian/laplace kernel sigma')\n",
    "# FDS\n",
    "parser.add_argument('--fds', action='store_true', default=False, help='whether to enable FDS')\n",
    "parser.add_argument('--fds_kernel', type=str, default='gaussian',\n",
    "                    choices=['gaussian', 'triang', 'laplace'], help='FDS kernel type')\n",
    "parser.add_argument('--fds_ks', type=int, default=9, help='FDS kernel size: should be odd number')\n",
    "parser.add_argument('--fds_sigma', type=float, default=1, help='FDS gaussian/laplace kernel sigma')\n",
    "parser.add_argument('--start_update', type=int, default=0, help='which epoch to start FDS updating')\n",
    "parser.add_argument('--start_smooth', type=int, default=1, help='which epoch to start using FDS to smooth features')\n",
    "parser.add_argument('--bucket_num', type=int, default=100, help='maximum bucket considered for FDS')\n",
    "parser.add_argument('--bucket_start', type=int, default=3, choices=[0, 3],\n",
    "                    help='minimum(starting) bucket for FDS, 0 for IMDBWIKI, 3 for AgeDB')\n",
    "parser.add_argument('--fds_mmt', type=float, default=0.9, help='FDS momentum')\n",
    "\n",
    "# re-weighting: SQRT_INV / INV\n",
    "parser.add_argument('--reweight', type=str, default='inverse', choices=['none', 'sqrt_inv', 'inverse'], help='cost-sensitive reweighting scheme')\n",
    "# two-stage training: RRT\n",
    "parser.add_argument('--retrain_fc', action='store_true', default=False, help='whether to retrain last regression layer (regressor)')\n",
    "\n",
    "# training/optimization related\n",
    "parser.add_argument('--dataset', type=str, default='agedb', choices=['imdb_wiki', 'agedb'], help='dataset name')\n",
    "parser.add_argument('--data_dir', type=str, default='./Faces/UTKFace/', help='data directory')\n",
    "parser.add_argument('--model', type=str, default='resnet50', help='model name')\n",
    "parser.add_argument('--store_root', type=str, default='checkpoint', help='root path for storing checkpoints, logs')\n",
    "parser.add_argument('--store_name', type=str, default='', help='experiment store name')\n",
    "parser.add_argument('--gpu', type=int, default=None)\n",
    "parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'], help='optimizer type')\n",
    "parser.add_argument('--loss', type=str, default='l1', choices=['mse', 'l1', 'focal_l1', 'focal_mse', 'huber'], help='training loss type')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\n",
    "parser.add_argument('--epoch', type=int, default=30, help='number of epochs to train')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='optimizer momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='optimizer weight decay')\n",
    "parser.add_argument('--schedule', type=int, nargs='*', default=[60, 80], help='lr schedule (when to drop lr by 10x)')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--print_freq', type=int, default=10, help='logging frequency')\n",
    "parser.add_argument('--img_size', type=int, default=224, help='image size used in training')\n",
    "parser.add_argument('--workers', type=int, default=16, help='number of workers used in data loading')\n",
    "# checkpoints\n",
    "parser.add_argument('--resume', type=str, default='', help='checkpoint file path to resume training')\n",
    "parser.add_argument('--pretrained', type=str, default='', help='checkpoint file path to load backbone weights')\n",
    "parser.add_argument('--evaluate', action='store_true', help='evaluate only flag')\n",
    "\n",
    "parser.set_defaults(augment=True)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "args.start_epoch, args.best_loss = 0, 1e5\n",
    "PATH = \"./Faces/UTKFace/\"\n",
    "\n",
    "if len(args.store_name):\n",
    "    args.store_name = f'_{args.store_name}'\n",
    "if not args.lds and args.reweight != 'none':\n",
    "    args.store_name += f'_{args.reweight}'\n",
    "if args.lds:\n",
    "    args.store_name += f'_lds_{args.lds_kernel[:3]}_{args.lds_ks}'\n",
    "    if args.lds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.lds_sigma}'\n",
    "if args.fds:\n",
    "    args.store_name += f'_fds_{args.fds_kernel[:3]}_{args.fds_ks}'\n",
    "    if args.fds_kernel in ['gaussian', 'laplace']:\n",
    "        args.store_name += f'_{args.fds_sigma}'\n",
    "    args.store_name += f'_{args.start_update}_{args.start_smooth}_{args.fds_mmt}'\n",
    "if args.retrain_fc:\n",
    "    args.store_name += f'_retrain_fc'\n",
    "args.store_name = f\"{args.dataset}_{args.model}{args.store_name}_{args.optimizer}_{args.loss}_{args.lr}_{args.batch_size}\"\n",
    "\n",
    "prepare_folders(args)\n",
    "\n",
    "logging.root.handlers = []\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(args.store_root, args.store_name, 'training.log')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "print = logging.info\n",
    "print(f\"Args: {args}\")\n",
    "print(f\"Store name: {args.store_name}\")\n",
    "\n",
    "tb_logger = Logger(logdir=os.path.join(args.store_root, args.store_name), flush_secs=2)\n",
    "\n",
    "def reload_data():\n",
    "    age_list = []\n",
    "    gender_list = []\n",
    "    race_list = []\n",
    "    datetime_list = []\n",
    "    filename_list = []\n",
    "\n",
    "    for filename in os.listdir(\"./Faces/UTKFace/\"):\n",
    "\n",
    "        args = filename.split(\"_\")\n",
    "\n",
    "        if len(args)<4:\n",
    "            age = int(args[0])\n",
    "            gender = int(args[1])\n",
    "            race = 4\n",
    "            datetime = args[2].split(\".\")[0]\n",
    "        else:\n",
    "            age = int(args[0])\n",
    "            gender = int(args[1])\n",
    "            race = int(args[2])\n",
    "            datetime = args[3].split(\".\")[0]\n",
    "\n",
    "        age_list.append(age)\n",
    "        gender_list.append(gender)\n",
    "        race_list.append(race)\n",
    "        datetime_list.append(datetime)\n",
    "        filename_list.append(filename)\n",
    "\n",
    "\n",
    "    d = {'age': age_list, 'gender': gender_list, 'race': race_list, 'datetime': datetime_list, 'filename': filename_list}\n",
    "    return pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f39c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.2f')\n",
    "    data_time = AverageMeter('Data', ':6.4f')\n",
    "    losses = AverageMeter(f'Loss ({args.loss.upper()})', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, (inputs, targets, weights) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        inputs, targets, weights = \\\n",
    "            inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True), weights.cuda(non_blocking=True)\n",
    "        if args.fds:\n",
    "            outputs, _ = model(inputs, targets, epoch)\n",
    "        else:\n",
    "            outputs = model(inputs, targets, epoch)\n",
    "\n",
    "        loss = globals()[f\"weighted_{args.loss}_loss\"](outputs, targets, weights)\n",
    "        assert not (np.isnan(loss.item()) or loss.item() > 1e6), f\"Loss explosion: {loss.item()}\"\n",
    "\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if idx % args.print_freq == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    if args.fds and epoch >= args.start_update:\n",
    "        print(f\"Create Epoch [{epoch}] features of all training data...\")\n",
    "        encodings, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (inputs, targets, _) in tqdm(train_loader):\n",
    "                inputs = inputs.cuda(non_blocking=True)\n",
    "                outputs, feature = model(inputs, targets, epoch)\n",
    "                encodings.extend(feature.data.squeeze().cpu().numpy())\n",
    "                labels.extend(targets.data.squeeze().cpu().numpy())\n",
    "\n",
    "        encodings, labels = torch.from_numpy(np.vstack(encodings)).cuda(), torch.from_numpy(np.hstack(labels)).cuda()\n",
    "        model.module.FDS.update_last_epoch_stats(epoch)\n",
    "        model.module.FDS.update_running_stats(encodings, labels, epoch)\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def validate(val_loader, model, train_labels=None, prefix='Val',mode= None):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses_mse = AverageMeter('Loss (MSE)', ':.3f')\n",
    "    losses_l1 = AverageMeter('Loss (L1)', ':.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses_mse, losses_l1],\n",
    "        prefix=f'{prefix}: '\n",
    "    )\n",
    "\n",
    "    criterion_mse = nn.MSELoss()\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    criterion_gmean = nn.L1Loss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    losses_all = []\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.cuda(non_blocking=True), targets.cuda(non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            preds.extend(outputs.data.cpu().numpy())\n",
    "            labels.extend(targets.data.cpu().numpy())\n",
    "\n",
    "            loss_mse = criterion_mse(outputs, targets)\n",
    "            loss_l1 = criterion_l1(outputs, targets)\n",
    "            loss_all = criterion_gmean(outputs, targets)\n",
    "            losses_all.extend(loss_all.cpu().numpy())\n",
    "\n",
    "            losses_mse.update(loss_mse.item(), inputs.size(0))\n",
    "            losses_l1.update(loss_l1.item(), inputs.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if idx % args.print_freq == 0:\n",
    "                progress.display(idx)\n",
    "\n",
    "        shot_dict = shot_metrics(np.hstack(preds), np.hstack(labels), train_labels,mode = mode)\n",
    "        loss_gmean = gmean(np.hstack(losses_all), axis=None).astype(float)\n",
    "        print(f\" * Overall: MSE {losses_mse.avg:.3f}\\tL1 {losses_l1.avg:.3f}\\tG-Mean {loss_gmean:.3f}\")\n",
    "        print(f\" * Many: MSE {shot_dict['many']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['many']['l1']:.3f}\\tG-Mean {shot_dict['many']['gmean']:.3f}\")\n",
    "        print(f\" * Median: MSE {shot_dict['median']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['median']['l1']:.3f}\\tG-Mean {shot_dict['median']['gmean']:.3f}\")\n",
    "        print(f\" * Low: MSE {shot_dict['low']['mse']:.3f}\\t\"\n",
    "              f\"L1 {shot_dict['low']['l1']:.3f}\\tG-Mean {shot_dict['low']['gmean']:.3f}\")\n",
    "\n",
    "    return losses_mse.avg, losses_l1.avg, loss_gmean\n",
    "\n",
    "\n",
    "def shot_metrics(preds, labels, train_labels, many_shot_thr=100, low_shot_thr=20,mode = None):\n",
    "    train_labels = np.array(train_labels).astype(int)\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    elif isinstance(preds, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f'Type ({type(preds)}) of predictions not supported')\n",
    "\n",
    "    train_class_count, test_class_count = [], []\n",
    "    mse_per_class, l1_per_class, l1_all_per_class = [], [], []\n",
    "    for l in np.unique(labels):\n",
    "        train_class_count.append(len(train_labels[train_labels == l]))\n",
    "        test_class_count.append(len(labels[labels == l]))\n",
    "        mse_per_class.append(np.sum((preds[labels == l] - labels[labels == l]) ** 2))\n",
    "        l1_per_class.append(np.sum(np.abs(preds[labels == l] - labels[labels == l])))\n",
    "        l1_all_per_class.append(np.abs(preds[labels == l] - labels[labels == l]))\n",
    "\n",
    "    many_shot_mse, median_shot_mse, low_shot_mse = [], [], []\n",
    "    many_shot_l1, median_shot_l1, low_shot_l1 = [], [], []\n",
    "    many_shot_gmean, median_shot_gmean, low_shot_gmean = [], [], []\n",
    "    many_shot_cnt, median_shot_cnt, low_shot_cnt = [], [], []\n",
    "\n",
    "    for i in range(len(train_class_count)):\n",
    "        if train_class_count[i] > many_shot_thr:\n",
    "            many_shot_mse.append(mse_per_class[i])\n",
    "            many_shot_l1.append(l1_per_class[i])\n",
    "            many_shot_gmean += list(l1_all_per_class[i])\n",
    "            many_shot_cnt.append(test_class_count[i])\n",
    "        elif train_class_count[i] < low_shot_thr:\n",
    "            low_shot_mse.append(mse_per_class[i])\n",
    "            low_shot_l1.append(l1_per_class[i])\n",
    "            low_shot_gmean += list(l1_all_per_class[i])\n",
    "            low_shot_cnt.append(test_class_count[i])\n",
    "        else:\n",
    "            median_shot_mse.append(mse_per_class[i])\n",
    "            median_shot_l1.append(l1_per_class[i])\n",
    "            median_shot_gmean += list(l1_all_per_class[i])\n",
    "            median_shot_cnt.append(test_class_count[i])\n",
    "\n",
    "    shot_dict = defaultdict(dict)\n",
    "    shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
    "    shot_dict['many']['gmean'] = 0 if mode == 'high' else gmean(np.hstack(many_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['median']['mse'] = np.sum(median_shot_mse) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['l1'] = np.sum(median_shot_l1) / np.sum(median_shot_cnt)\n",
    "    shot_dict['median']['gmean'] = gmean(np.hstack(median_shot_gmean), axis=None).astype(float)\n",
    "    shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
    "    shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
    "\n",
    "    shot_dict['low']['gmean'] = 0 if mode == 'low' else gmean(np.hstack(low_shot_gmean), axis=None).astype(float)\n",
    "\n",
    "    return shot_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3cbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if args.gpu is not None:\n",
    "        print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "    # Data\n",
    "    print('=====> Preparing data...')\n",
    "    print(\"UTKFaces\")\n",
    "    df = reload_data()\n",
    "    \n",
    "    split_size1=0.8\n",
    "    split_size2=0.5\n",
    "    shuffle_dataset = True\n",
    "    random_seed = 42\n",
    "\n",
    "    df_train = df.sample(frac=split_size1, random_state=random_seed)\n",
    "    test_val_df = df.drop(df_train.index)\n",
    "    df_val = test_val_df.sample(frac=split_size2, random_state=random_seed)\n",
    "    df_test = test_val_df.drop(df_val.index)\n",
    "    df_test_18, df_test_80 = df_test[df_test['age'] <18],df_test[df_test['age'] >=80]\n",
    "    train_labels = df_train['age']\n",
    "\n",
    "    train_dataset = AgeDB(data_dir=PATH, df=df_train, img_size=args.img_size, split='train',\n",
    "                          reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "    val_dataset = AgeDB(data_dir=PATH, df=df_val, img_size=args.img_size, split='val')\n",
    "    test_dataset = AgeDB(data_dir=PATH, df=df_test, img_size=args.img_size, split='test')\n",
    "    test_dataset_18 = AgeDB(data_dir=PATH, df=df_test_18, img_size=args.img_size, split='test')\n",
    "    test_dataset_80 = AgeDB(data_dir=PATH, df=df_test_80, img_size=args.img_size, split='test')\n",
    "    \n",
    "    print('worker = ' +str(args.workers) )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                              num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset_18, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    #test_loader_18 = DataLoader(test_dataset_18, batch_size=args.batch_size, shuffle=False,\n",
    "    #                         num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    #test_loader_80 = DataLoader(test_dataset_80, batch_size=args.batch_size, shuffle=False,\n",
    "    #                         num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    print(f\"Training data size: {len(train_dataset)}\")\n",
    "    print(f\"Validation data size: {len(val_dataset)}\")\n",
    "    #print(f\"Test data size: {len(test_dataset)}\")\n",
    "    print(f\"Test data size age < 18: {len(test_dataset_18)}\")\n",
    "    #print(f\"Test data size age >= 80: {len(test_dataset_80)}\")\n",
    "\n",
    "    # Model\n",
    "    print('=====> Building model...')\n",
    "    \n",
    "    model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                     start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                     kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "    \"\"\"\n",
    "    model=resnet50(fds=False, bucket_num=100, bucket_start=3,\n",
    "                     start_update=0, start_smooth=1,\n",
    "                     kernel='gaussian', ks=9, sigma=1, momentum=0.9)\n",
    "    \"\"\"\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # evaluate only\n",
    "    if args.evaluate:\n",
    "        assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "        validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "        return\n",
    "\n",
    "    if args.retrain_fc:\n",
    "        assert args.reweight != 'none' and args.pretrained\n",
    "        print('===> Retrain last regression layer only!')\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'fc' not in name and 'linear' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Loss and optimizer\n",
    "    if not args.retrain_fc:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "            torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        # optimize only the last linear layer\n",
    "        parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "        names = list(filter(lambda k: k is not None, [k if v.requires_grad else None for k, v in model.module.named_parameters()]))\n",
    "        assert 1 <= len(parameters) <= 2  # fc.weight, fc.bias\n",
    "        print(f'===> Only optimize parameters: {names}')\n",
    "        optimizer = torch.optim.Adam(parameters, lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "            torch.optim.SGD(parameters, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.pretrained:\n",
    "        checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in checkpoint['state_dict'].items():\n",
    "            if 'linear' not in k and 'fc' not in k:\n",
    "                new_state_dict[k] = v\n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        print(f'===> Pretrained weights found in total: [{len(list(new_state_dict.keys()))}]')\n",
    "        print(f'===> Pre-trained model loaded: {args.pretrained}')\n",
    "\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "            checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "                torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.best_loss = checkpoint['best_loss']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "        else:\n",
    "            print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epoch):\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        train_loss = train(train_loader, model, optimizer, epoch)\n",
    "        val_loss_mse, val_loss_l1, val_loss_gmean = validate(val_loader, model, train_labels=train_labels)\n",
    "\n",
    "        loss_metric = val_loss_mse if args.loss == 'mse' else val_loss_l1\n",
    "        is_best = loss_metric < args.best_loss\n",
    "        args.best_loss = min(loss_metric, args.best_loss)\n",
    "        print(f\"Best {'L1' if 'l1' in args.loss else 'MSE'} Loss: {args.best_loss:.3f}\")\n",
    "        save_checkpoint(args, {\n",
    "            'epoch': epoch + 1,\n",
    "            'model': args.model,\n",
    "            'best_loss': args.best_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "        print(f\"Epoch #{epoch}: Train loss [{train_loss:.4f}]; \"\n",
    "              f\"Val loss: MSE [{val_loss_mse:.4f}], L1 [{val_loss_l1:.4f}], G-Mean [{val_loss_gmean:.4f}]\")\n",
    "\n",
    "        tb_logger.log_value('train_loss', train_loss, epoch)\n",
    "        tb_logger.log_value('val_loss_mse', val_loss_mse, epoch)\n",
    "        tb_logger.log_value('val_loss_l1', val_loss_l1, epoch)\n",
    "        tb_logger.log_value('val_loss_gmean', val_loss_gmean, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dab369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:18:54,155 | =====> Preparing data...\n",
      "2022-11-30 18:18:54,155 | UTKFaces\n",
      "2022-11-30 18:18:54,241 | Using re-weighting: [INVERSE]\n",
      "2022-11-30 18:18:54,242 | Using LDS: [GAUSSIAN] (9/1)\n",
      "2022-11-30 18:18:54,261 | worker = 16\n",
      "2022-11-30 18:18:54,261 | Training data size: 18966\n",
      "2022-11-30 18:18:54,262 | Validation data size: 2371\n",
      "2022-11-30 18:18:54,262 | Test data size age < 18: 432\n",
      "2022-11-30 18:18:54,262 | =====> Building model...\n",
      "2022-11-30 18:19:29,052 | Epoch: [0][  0/297]\tTime  34.52 ( 34.52)\tData 29.5915 (29.5915)\tLoss (L1) 37.809 (37.809)\n",
      "2022-11-30 18:19:31,169 | Epoch: [0][ 10/297]\tTime   0.22 (  3.33)\tData 0.0000 (2.6903)\tLoss (L1) 18.116 (41.372)\n",
      "2022-11-30 18:19:33,394 | Epoch: [0][ 20/297]\tTime   0.22 (  1.85)\tData 0.0000 (1.4092)\tLoss (L1) 13.541 (34.737)\n",
      "2022-11-30 18:19:35,628 | Epoch: [0][ 30/297]\tTime   0.22 (  1.33)\tData 0.0010 (0.9547)\tLoss (L1) 19.258 (33.083)\n",
      "2022-11-30 18:19:37,851 | Epoch: [0][ 40/297]\tTime   0.22 (  1.06)\tData 0.0000 (0.7219)\tLoss (L1) 20.901 (30.625)\n",
      "2022-11-30 18:19:40,079 | Epoch: [0][ 50/297]\tTime   0.22 (  0.89)\tData 0.0000 (0.5804)\tLoss (L1) 34.593 (28.825)\n",
      "2022-11-30 18:19:42,308 | Epoch: [0][ 60/297]\tTime   0.22 (  0.78)\tData 0.0000 (0.4853)\tLoss (L1) 39.408 (27.907)\n",
      "2022-11-30 18:19:44,534 | Epoch: [0][ 70/297]\tTime   0.22 (  0.70)\tData 0.0000 (0.4170)\tLoss (L1) 32.985 (27.728)\n",
      "2022-11-30 18:19:46,973 | Epoch: [0][ 80/297]\tTime   0.28 (  0.65)\tData 0.0010 (0.3655)\tLoss (L1) 27.603 (27.485)\n",
      "2022-11-30 18:19:49,455 | Epoch: [0][ 90/297]\tTime   0.25 (  0.60)\tData 0.0000 (0.3254)\tLoss (L1) 19.368 (26.843)\n",
      "2022-11-30 18:19:51,941 | Epoch: [0][100/297]\tTime   0.23 (  0.57)\tData 0.0000 (0.2932)\tLoss (L1) 15.922 (27.099)\n",
      "2022-11-30 18:19:54,177 | Epoch: [0][110/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.2668)\tLoss (L1) 22.523 (26.845)\n",
      "2022-11-30 18:19:56,427 | Epoch: [0][120/297]\tTime   0.23 (  0.51)\tData 0.0000 (0.2448)\tLoss (L1) 20.656 (27.050)\n",
      "2022-11-30 18:19:58,708 | Epoch: [0][130/297]\tTime   0.27 (  0.49)\tData 0.0000 (0.2261)\tLoss (L1) 20.673 (27.112)\n",
      "2022-11-30 18:20:00,984 | Epoch: [0][140/297]\tTime   0.23 (  0.47)\tData 0.0010 (0.2101)\tLoss (L1) 19.591 (27.257)\n",
      "2022-11-30 18:20:03,382 | Epoch: [0][150/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.1962)\tLoss (L1) 52.763 (27.298)\n",
      "2022-11-30 18:20:05,759 | Epoch: [0][160/297]\tTime   0.24 (  0.44)\tData 0.0000 (0.1840)\tLoss (L1) 40.018 (27.229)\n",
      "2022-11-30 18:20:08,135 | Epoch: [0][170/297]\tTime   0.26 (  0.43)\tData 0.0000 (0.1733)\tLoss (L1) 16.255 (26.655)\n",
      "2022-11-30 18:20:10,486 | Epoch: [0][180/297]\tTime   0.23 (  0.42)\tData 0.0010 (0.1637)\tLoss (L1) 22.733 (26.555)\n",
      "2022-11-30 18:20:12,890 | Epoch: [0][190/297]\tTime   0.25 (  0.41)\tData 0.0000 (0.1552)\tLoss (L1) 23.643 (26.512)\n",
      "2022-11-30 18:20:15,159 | Epoch: [0][200/297]\tTime   0.25 (  0.40)\tData 0.0000 (0.1475)\tLoss (L1) 14.092 (26.184)\n",
      "2022-11-30 18:20:17,455 | Epoch: [0][210/297]\tTime   0.24 (  0.39)\tData 0.0000 (0.1405)\tLoss (L1) 62.939 (26.180)\n",
      "2022-11-30 18:20:19,852 | Epoch: [0][220/297]\tTime   0.24 (  0.39)\tData 0.0000 (0.1341)\tLoss (L1) 22.781 (25.993)\n",
      "2022-11-30 18:20:22,155 | Epoch: [0][230/297]\tTime   0.24 (  0.38)\tData 0.0000 (0.1283)\tLoss (L1) 19.403 (25.851)\n",
      "2022-11-30 18:20:24,481 | Epoch: [0][240/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1230)\tLoss (L1) 36.021 (25.863)\n",
      "2022-11-30 18:20:26,770 | Epoch: [0][250/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1181)\tLoss (L1) 12.387 (25.471)\n",
      "2022-11-30 18:20:28,996 | Epoch: [0][260/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1136)\tLoss (L1) 53.144 (25.411)\n",
      "2022-11-30 18:20:31,223 | Epoch: [0][270/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1094)\tLoss (L1) 14.312 (25.197)\n",
      "2022-11-30 18:20:33,452 | Epoch: [0][280/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1055)\tLoss (L1) 19.392 (25.137)\n",
      "2022-11-30 18:20:35,690 | Epoch: [0][290/297]\tTime   0.22 (  0.35)\tData 0.0010 (0.1019)\tLoss (L1) 20.281 (25.006)\n",
      "2022-11-30 18:21:09,455 | Val: [ 0/38]\tTime 30.519 (30.519)\tLoss (MSE) 387.939 (387.939)\tLoss (L1) 16.059 (16.059)\n",
      "2022-11-30 18:21:10,116 | Val: [10/38]\tTime  0.066 ( 2.835)\tLoss (MSE) 456.069 (442.648)\tLoss (L1) 17.417 (17.237)\n",
      "2022-11-30 18:21:10,763 | Val: [20/38]\tTime  0.064 ( 1.516)\tLoss (MSE) 386.917 (431.592)\tLoss (L1) 16.370 (17.003)\n",
      "2022-11-30 18:21:11,413 | Val: [30/38]\tTime  0.068 ( 1.048)\tLoss (MSE) 433.598 (431.415)\tLoss (L1) 16.170 (16.992)\n",
      "2022-11-30 18:21:13,041 |  * Overall: MSE 427.966\tL1 16.921\tG-Mean 11.764\n",
      "2022-11-30 18:21:13,042 |  * Many: MSE 405.083\tL1 16.438\tG-Mean 11.382\n",
      "2022-11-30 18:21:13,042 |  * Median: MSE 593.044\tL1 20.462\tG-Mean 15.256\n",
      "2022-11-30 18:21:13,043 |  * Low: MSE 1352.197\tL1 35.704\tG-Mean 34.683\n",
      "2022-11-30 18:21:13,044 | Best L1 Loss: 16.921\n",
      "2022-11-30 18:21:13,219 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:21:13,284 | Epoch #0: Train loss [24.9530]; Val loss: MSE [427.9663], L1 [16.9211], G-Mean [11.7643]\n",
      "2022-11-30 18:21:42,123 | Epoch: [1][  0/297]\tTime  28.84 ( 28.84)\tData 28.6180 (28.6180)\tLoss (L1) 40.174 (40.174)\n",
      "2022-11-30 18:21:44,393 | Epoch: [1][ 10/297]\tTime   0.22 (  2.83)\tData 0.0000 (2.6023)\tLoss (L1) 26.595 (18.902)\n",
      "2022-11-30 18:21:46,620 | Epoch: [1][ 20/297]\tTime   0.23 (  1.59)\tData 0.0000 (1.3631)\tLoss (L1) 15.463 (21.554)\n",
      "2022-11-30 18:21:48,849 | Epoch: [1][ 30/297]\tTime   0.22 (  1.15)\tData 0.0000 (0.9235)\tLoss (L1) 20.532 (22.605)\n",
      "2022-11-30 18:21:51,073 | Epoch: [1][ 40/297]\tTime   0.22 (  0.92)\tData 0.0000 (0.6983)\tLoss (L1) 18.738 (22.165)\n",
      "2022-11-30 18:21:53,311 | Epoch: [1][ 50/297]\tTime   0.22 (  0.78)\tData 0.0000 (0.5614)\tLoss (L1) 15.427 (21.410)\n",
      "2022-11-30 18:21:55,540 | Epoch: [1][ 60/297]\tTime   0.22 (  0.69)\tData 0.0000 (0.4694)\tLoss (L1) 25.714 (20.979)\n",
      "2022-11-30 18:21:57,781 | Epoch: [1][ 70/297]\tTime   0.23 (  0.63)\tData 0.0010 (0.4033)\tLoss (L1) 14.648 (20.631)\n",
      "2022-11-30 18:22:00,008 | Epoch: [1][ 80/297]\tTime   0.23 (  0.58)\tData 0.0000 (0.3536)\tLoss (L1) 16.644 (20.386)\n",
      "2022-11-30 18:22:02,238 | Epoch: [1][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3148)\tLoss (L1) 14.878 (20.316)\n",
      "2022-11-30 18:22:04,471 | Epoch: [1][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2836)\tLoss (L1) 16.582 (20.162)\n",
      "2022-11-30 18:22:06,703 | Epoch: [1][110/297]\tTime   0.21 (  0.48)\tData 0.0000 (0.2581)\tLoss (L1) 16.135 (20.295)\n",
      "2022-11-30 18:22:08,932 | Epoch: [1][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2368)\tLoss (L1) 26.943 (20.399)\n",
      "2022-11-30 18:22:11,165 | Epoch: [1][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2187)\tLoss (L1) 37.930 (20.481)\n",
      "2022-11-30 18:22:13,401 | Epoch: [1][140/297]\tTime   0.23 (  0.43)\tData 0.0000 (0.2032)\tLoss (L1) 48.385 (20.571)\n",
      "2022-11-30 18:22:15,633 | Epoch: [1][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1897)\tLoss (L1) 16.143 (20.331)\n",
      "2022-11-30 18:22:17,869 | Epoch: [1][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1780)\tLoss (L1) 14.068 (20.658)\n",
      "2022-11-30 18:22:20,107 | Epoch: [1][170/297]\tTime   0.22 (  0.39)\tData 0.0010 (0.1676)\tLoss (L1) 15.961 (20.674)\n",
      "2022-11-30 18:22:22,342 | Epoch: [1][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1583)\tLoss (L1) 15.116 (20.707)\n",
      "2022-11-30 18:22:24,572 | Epoch: [1][190/297]\tTime   0.22 (  0.37)\tData 0.0010 (0.1500)\tLoss (L1) 15.926 (20.767)\n",
      "2022-11-30 18:22:27,030 | Epoch: [1][200/297]\tTime   0.27 (  0.37)\tData 0.0000 (0.1426)\tLoss (L1) 17.076 (20.770)\n",
      "2022-11-30 18:22:29,507 | Epoch: [1][210/297]\tTime   0.24 (  0.36)\tData 0.0000 (0.1358)\tLoss (L1) 25.010 (20.676)\n",
      "2022-11-30 18:22:32,033 | Epoch: [1][220/297]\tTime   0.22 (  0.36)\tData 0.0010 (0.1297)\tLoss (L1) 14.747 (20.648)\n",
      "2022-11-30 18:22:34,397 | Epoch: [1][230/297]\tTime   0.26 (  0.35)\tData 0.0010 (0.1241)\tLoss (L1) 13.544 (20.467)\n",
      "2022-11-30 18:22:36,970 | Epoch: [1][240/297]\tTime   0.26 (  0.35)\tData 0.0000 (0.1190)\tLoss (L1) 14.543 (20.440)\n",
      "2022-11-30 18:22:39,246 | Epoch: [1][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1142)\tLoss (L1) 16.976 (20.259)\n",
      "2022-11-30 18:22:41,483 | Epoch: [1][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1099)\tLoss (L1) 17.345 (20.321)\n",
      "2022-11-30 18:22:43,716 | Epoch: [1][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1058)\tLoss (L1) 23.998 (20.376)\n",
      "2022-11-30 18:22:45,956 | Epoch: [1][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1021)\tLoss (L1) 34.352 (20.335)\n",
      "2022-11-30 18:22:48,197 | Epoch: [1][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.0986)\tLoss (L1) 23.026 (20.360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:23:18,667 | Val: [ 0/38]\tTime 28.496 (28.496)\tLoss (MSE) 2106.736 (2106.736)\tLoss (L1) 38.371 (38.371)\n",
      "2022-11-30 18:23:19,370 | Val: [10/38]\tTime  0.063 ( 2.654)\tLoss (MSE) 1606.832 (1674.452)\tLoss (L1) 33.155 (34.066)\n",
      "2022-11-30 18:23:20,022 | Val: [20/38]\tTime  0.064 ( 1.421)\tLoss (MSE) 1366.217 (1646.996)\tLoss (L1) 30.064 (33.503)\n",
      "2022-11-30 18:23:20,681 | Val: [30/38]\tTime  0.067 ( 0.984)\tLoss (MSE) 1273.193 (1630.146)\tLoss (L1) 30.147 (33.326)\n",
      "2022-11-30 18:23:21,743 |  * Overall: MSE 1635.779\tL1 33.432\tG-Mean 24.105\n",
      "2022-11-30 18:23:21,744 |  * Many: MSE 1689.201\tL1 34.279\tG-Mean 25.332\n",
      "2022-11-30 18:23:21,745 |  * Median: MSE 1147.687\tL1 25.406\tG-Mean 14.825\n",
      "2022-11-30 18:23:21,745 |  * Low: MSE 704.745\tL1 22.152\tG-Mean 14.947\n",
      "2022-11-30 18:23:21,746 | Best L1 Loss: 16.921\n",
      "2022-11-30 18:23:21,923 | Epoch #1: Train loss [20.4424]; Val loss: MSE [1635.7790], L1 [33.4321], G-Mean [24.1047]\n",
      "2022-11-30 18:23:51,109 | Epoch: [2][  0/297]\tTime  29.19 ( 29.19)\tData 28.9402 (28.9402)\tLoss (L1) 15.523 (15.523)\n",
      "2022-11-30 18:23:53,355 | Epoch: [2][ 10/297]\tTime   0.22 (  2.86)\tData 0.0010 (2.6314)\tLoss (L1) 17.046 (19.666)\n",
      "2022-11-30 18:23:55,594 | Epoch: [2][ 20/297]\tTime   0.23 (  1.60)\tData 0.0010 (1.3784)\tLoss (L1) 12.810 (20.680)\n",
      "2022-11-30 18:23:57,827 | Epoch: [2][ 30/297]\tTime   0.22 (  1.16)\tData 0.0010 (0.9339)\tLoss (L1) 18.041 (19.207)\n",
      "2022-11-30 18:24:00,071 | Epoch: [2][ 40/297]\tTime   0.22 (  0.93)\tData 0.0000 (0.7062)\tLoss (L1) 12.021 (19.556)\n",
      "2022-11-30 18:24:02,303 | Epoch: [2][ 50/297]\tTime   0.22 (  0.79)\tData 0.0000 (0.5678)\tLoss (L1) 13.088 (18.935)\n",
      "2022-11-30 18:24:04,538 | Epoch: [2][ 60/297]\tTime   0.22 (  0.70)\tData 0.0000 (0.4747)\tLoss (L1) 29.035 (18.679)\n",
      "2022-11-30 18:24:06,773 | Epoch: [2][ 70/297]\tTime   0.22 (  0.63)\tData 0.0000 (0.4079)\tLoss (L1) 19.897 (18.898)\n",
      "2022-11-30 18:24:09,010 | Epoch: [2][ 80/297]\tTime   0.22 (  0.58)\tData 0.0000 (0.3576)\tLoss (L1) 13.146 (18.628)\n",
      "2022-11-30 18:24:11,242 | Epoch: [2][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3183)\tLoss (L1) 9.870 (18.149)\n",
      "2022-11-30 18:24:13,478 | Epoch: [2][100/297]\tTime   0.22 (  0.51)\tData 0.0010 (0.2868)\tLoss (L1) 21.176 (18.098)\n",
      "2022-11-30 18:24:15,706 | Epoch: [2][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2610)\tLoss (L1) 29.897 (18.130)\n",
      "2022-11-30 18:24:17,940 | Epoch: [2][120/297]\tTime   0.22 (  0.46)\tData 0.0010 (0.2394)\tLoss (L1) 16.950 (18.050)\n",
      "2022-11-30 18:24:20,175 | Epoch: [2][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2212)\tLoss (L1) 14.252 (17.685)\n",
      "2022-11-30 18:24:22,409 | Epoch: [2][140/297]\tTime   0.23 (  0.43)\tData 0.0000 (0.2055)\tLoss (L1) 13.918 (17.764)\n",
      "2022-11-30 18:24:24,638 | Epoch: [2][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1919)\tLoss (L1) 11.773 (17.649)\n",
      "2022-11-30 18:24:26,872 | Epoch: [2][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1800)\tLoss (L1) 17.212 (17.512)\n",
      "2022-11-30 18:24:29,102 | Epoch: [2][170/297]\tTime   0.22 (  0.39)\tData 0.0010 (0.1695)\tLoss (L1) 17.988 (17.567)\n",
      "2022-11-30 18:24:31,338 | Epoch: [2][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1601)\tLoss (L1) 19.710 (17.586)\n",
      "2022-11-30 18:24:33,577 | Epoch: [2][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1517)\tLoss (L1) 13.141 (17.687)\n",
      "2022-11-30 18:24:35,817 | Epoch: [2][200/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1442)\tLoss (L1) 15.933 (18.191)\n",
      "2022-11-30 18:24:38,049 | Epoch: [2][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1374)\tLoss (L1) 15.665 (18.273)\n",
      "2022-11-30 18:24:40,289 | Epoch: [2][220/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1312)\tLoss (L1) 13.657 (18.236)\n",
      "2022-11-30 18:24:42,527 | Epoch: [2][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1255)\tLoss (L1) 12.315 (18.393)\n",
      "2022-11-30 18:24:44,763 | Epoch: [2][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1203)\tLoss (L1) 13.889 (18.466)\n",
      "2022-11-30 18:24:47,006 | Epoch: [2][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1155)\tLoss (L1) 25.800 (18.561)\n",
      "2022-11-30 18:24:49,243 | Epoch: [2][260/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1111)\tLoss (L1) 21.477 (18.623)\n",
      "2022-11-30 18:24:51,478 | Epoch: [2][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1070)\tLoss (L1) 14.169 (18.508)\n",
      "2022-11-30 18:24:53,716 | Epoch: [2][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1032)\tLoss (L1) 16.247 (18.896)\n",
      "2022-11-30 18:24:55,958 | Epoch: [2][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0997)\tLoss (L1) 17.602 (18.865)\n",
      "2022-11-30 18:25:26,026 | Val: [ 0/38]\tTime 28.110 (28.110)\tLoss (MSE) 262.643 (262.643)\tLoss (L1) 12.378 (12.378)\n",
      "2022-11-30 18:25:26,722 | Val: [10/38]\tTime  0.064 ( 2.619)\tLoss (MSE) 457.113 (406.147)\tLoss (L1) 16.839 (15.252)\n",
      "2022-11-30 18:25:27,375 | Val: [20/38]\tTime  0.065 ( 1.403)\tLoss (MSE) 296.457 (385.322)\tLoss (L1) 13.880 (15.122)\n",
      "2022-11-30 18:25:28,020 | Val: [30/38]\tTime  0.063 ( 0.971)\tLoss (MSE) 347.213 (376.478)\tLoss (L1) 14.679 (14.964)\n",
      "2022-11-30 18:25:29,106 |  * Overall: MSE 378.240\tL1 15.035\tG-Mean 9.834\n",
      "2022-11-30 18:25:29,107 |  * Many: MSE 329.186\tL1 14.212\tG-Mean 9.370\n",
      "2022-11-30 18:25:29,107 |  * Median: MSE 803.439\tL1 22.312\tG-Mean 15.174\n",
      "2022-11-30 18:25:29,108 |  * Low: MSE 1507.696\tL1 32.191\tG-Mean 24.949\n",
      "2022-11-30 18:25:29,109 | Best L1 Loss: 15.035\n",
      "2022-11-30 18:25:29,282 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:25:29,355 | Epoch #2: Train loss [18.8560]; Val loss: MSE [378.2404], L1 [15.0347], G-Mean [9.8339]\n",
      "2022-11-30 18:25:58,818 | Epoch: [3][  0/297]\tTime  29.45 ( 29.45)\tData 29.0824 (29.0824)\tLoss (L1) 12.893 (12.893)\n",
      "2022-11-30 18:26:01,061 | Epoch: [3][ 10/297]\tTime   0.23 (  2.88)\tData 0.0000 (2.6442)\tLoss (L1) 14.836 (17.273)\n",
      "2022-11-30 18:26:03,302 | Epoch: [3][ 20/297]\tTime   0.22 (  1.62)\tData 0.0000 (1.3851)\tLoss (L1) 14.955 (15.760)\n",
      "2022-11-30 18:26:05,535 | Epoch: [3][ 30/297]\tTime   0.22 (  1.17)\tData 0.0000 (0.9384)\tLoss (L1) 15.429 (16.920)\n",
      "2022-11-30 18:26:07,770 | Epoch: [3][ 40/297]\tTime   0.22 (  0.94)\tData 0.0000 (0.7095)\tLoss (L1) 10.205 (16.362)\n",
      "2022-11-30 18:26:10,002 | Epoch: [3][ 50/297]\tTime   0.22 (  0.80)\tData 0.0000 (0.5704)\tLoss (L1) 30.175 (16.335)\n",
      "2022-11-30 18:26:12,236 | Epoch: [3][ 60/297]\tTime   0.22 (  0.70)\tData 0.0000 (0.4769)\tLoss (L1) 13.588 (16.864)\n",
      "2022-11-30 18:26:14,473 | Epoch: [3][ 70/297]\tTime   0.22 (  0.64)\tData 0.0000 (0.4098)\tLoss (L1) 13.108 (17.321)\n",
      "2022-11-30 18:26:16,707 | Epoch: [3][ 80/297]\tTime   0.22 (  0.58)\tData 0.0000 (0.3592)\tLoss (L1) 17.749 (17.251)\n",
      "2022-11-30 18:26:18,943 | Epoch: [3][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3197)\tLoss (L1) 14.446 (17.418)\n",
      "2022-11-30 18:26:21,179 | Epoch: [3][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2881)\tLoss (L1) 11.876 (17.159)\n",
      "2022-11-30 18:26:23,422 | Epoch: [3][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2621)\tLoss (L1) 17.495 (16.880)\n",
      "2022-11-30 18:26:25,661 | Epoch: [3][120/297]\tTime   0.23 (  0.47)\tData 0.0000 (0.2405)\tLoss (L1) 21.256 (17.391)\n",
      "2022-11-30 18:26:27,896 | Epoch: [3][130/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2222)\tLoss (L1) 12.237 (17.381)\n",
      "2022-11-30 18:26:30,133 | Epoch: [3][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2064)\tLoss (L1) 12.840 (17.125)\n",
      "2022-11-30 18:26:32,372 | Epoch: [3][150/297]\tTime   0.23 (  0.42)\tData 0.0000 (0.1928)\tLoss (L1) 12.747 (17.351)\n",
      "2022-11-30 18:26:34,608 | Epoch: [3][160/297]\tTime   0.23 (  0.41)\tData 0.0000 (0.1808)\tLoss (L1) 14.958 (17.253)\n",
      "2022-11-30 18:26:36,843 | Epoch: [3][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1702)\tLoss (L1) 15.620 (17.011)\n",
      "2022-11-30 18:26:39,073 | Epoch: [3][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1608)\tLoss (L1) 14.464 (16.891)\n",
      "2022-11-30 18:26:41,309 | Epoch: [3][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1524)\tLoss (L1) 30.829 (17.150)\n",
      "2022-11-30 18:26:43,540 | Epoch: [3][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1448)\tLoss (L1) 14.937 (17.067)\n",
      "2022-11-30 18:26:45,778 | Epoch: [3][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1380)\tLoss (L1) 19.662 (17.166)\n",
      "2022-11-30 18:26:48,017 | Epoch: [3][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1318)\tLoss (L1) 13.378 (17.042)\n",
      "2022-11-30 18:26:50,256 | Epoch: [3][230/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1261)\tLoss (L1) 15.220 (17.012)\n",
      "2022-11-30 18:26:52,491 | Epoch: [3][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1208)\tLoss (L1) 11.241 (16.938)\n",
      "2022-11-30 18:26:54,728 | Epoch: [3][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1160)\tLoss (L1) 16.331 (16.839)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:26:56,968 | Epoch: [3][260/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1116)\tLoss (L1) 14.839 (16.777)\n",
      "2022-11-30 18:26:59,222 | Epoch: [3][270/297]\tTime   0.24 (  0.33)\tData 0.0000 (0.1075)\tLoss (L1) 12.681 (16.775)\n",
      "2022-11-30 18:27:01,447 | Epoch: [3][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1037)\tLoss (L1) 12.403 (16.732)\n",
      "2022-11-30 18:27:03,685 | Epoch: [3][290/297]\tTime   0.23 (  0.32)\tData 0.0000 (0.1001)\tLoss (L1) 11.499 (16.614)\n",
      "2022-11-30 18:27:33,017 | Val: [ 0/38]\tTime 27.368 (27.368)\tLoss (MSE) 335.162 (335.162)\tLoss (L1) 15.277 (15.277)\n",
      "2022-11-30 18:27:33,724 | Val: [10/38]\tTime  0.064 ( 2.552)\tLoss (MSE) 546.741 (477.940)\tLoss (L1) 18.171 (17.229)\n",
      "2022-11-30 18:27:34,370 | Val: [20/38]\tTime  0.065 ( 1.368)\tLoss (MSE) 414.046 (466.063)\tLoss (L1) 15.986 (17.301)\n",
      "2022-11-30 18:27:35,018 | Val: [30/38]\tTime  0.064 ( 0.947)\tLoss (MSE) 604.209 (470.482)\tLoss (L1) 19.301 (17.402)\n",
      "2022-11-30 18:27:36,072 |  * Overall: MSE 470.923\tL1 17.464\tG-Mean 11.933\n",
      "2022-11-30 18:27:36,073 |  * Many: MSE 417.771\tL1 16.553\tG-Mean 11.431\n",
      "2022-11-30 18:27:36,073 |  * Median: MSE 920.475\tL1 25.198\tG-Mean 17.082\n",
      "2022-11-30 18:27:36,074 |  * Low: MSE 1828.104\tL1 40.305\tG-Mean 38.013\n",
      "2022-11-30 18:27:36,075 | Best L1 Loss: 15.035\n",
      "2022-11-30 18:27:36,253 | Epoch #3: Train loss [16.5916]; Val loss: MSE [470.9231], L1 [17.4637], G-Mean [11.9331]\n",
      "2022-11-30 18:28:07,265 | Epoch: [4][  0/297]\tTime  31.01 ( 31.01)\tData 30.6843 (30.6843)\tLoss (L1) 12.521 (12.521)\n",
      "2022-11-30 18:28:09,503 | Epoch: [4][ 10/297]\tTime   0.22 (  3.02)\tData 0.0000 (2.7896)\tLoss (L1) 10.243 (13.303)\n",
      "2022-11-30 18:28:11,736 | Epoch: [4][ 20/297]\tTime   0.23 (  1.69)\tData 0.0000 (1.4613)\tLoss (L1) 13.065 (15.715)\n",
      "2022-11-30 18:28:13,962 | Epoch: [4][ 30/297]\tTime   0.22 (  1.22)\tData 0.0000 (0.9899)\tLoss (L1) 17.608 (16.596)\n",
      "2022-11-30 18:28:16,189 | Epoch: [4][ 40/297]\tTime   0.22 (  0.97)\tData 0.0000 (0.7485)\tLoss (L1) 22.837 (16.252)\n",
      "2022-11-30 18:28:18,421 | Epoch: [4][ 50/297]\tTime   0.22 (  0.83)\tData 0.0000 (0.6018)\tLoss (L1) 15.677 (16.254)\n",
      "2022-11-30 18:28:20,659 | Epoch: [4][ 60/297]\tTime   0.22 (  0.73)\tData 0.0000 (0.5032)\tLoss (L1) 32.715 (15.871)\n",
      "2022-11-30 18:28:22,896 | Epoch: [4][ 70/297]\tTime   0.22 (  0.66)\tData 0.0010 (0.4323)\tLoss (L1) 14.701 (15.811)\n",
      "2022-11-30 18:28:25,126 | Epoch: [4][ 80/297]\tTime   0.22 (  0.60)\tData 0.0000 (0.3790)\tLoss (L1) 11.101 (16.025)\n",
      "2022-11-30 18:28:27,360 | Epoch: [4][ 90/297]\tTime   0.21 (  0.56)\tData 0.0000 (0.3374)\tLoss (L1) 12.383 (15.908)\n",
      "2022-11-30 18:28:29,594 | Epoch: [4][100/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.3040)\tLoss (L1) 12.365 (15.541)\n",
      "2022-11-30 18:28:31,825 | Epoch: [4][110/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2766)\tLoss (L1) 33.399 (15.533)\n",
      "2022-11-30 18:28:34,059 | Epoch: [4][120/297]\tTime   0.23 (  0.48)\tData 0.0000 (0.2538)\tLoss (L1) 17.064 (16.153)\n",
      "2022-11-30 18:28:36,289 | Epoch: [4][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2344)\tLoss (L1) 18.913 (16.137)\n",
      "2022-11-30 18:28:38,526 | Epoch: [4][140/297]\tTime   0.23 (  0.44)\tData 0.0000 (0.2178)\tLoss (L1) 19.629 (15.948)\n",
      "2022-11-30 18:28:40,758 | Epoch: [4][150/297]\tTime   0.22 (  0.43)\tData 0.0010 (0.2034)\tLoss (L1) 26.602 (16.198)\n",
      "2022-11-30 18:28:42,988 | Epoch: [4][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1908)\tLoss (L1) 10.273 (16.049)\n",
      "2022-11-30 18:28:45,221 | Epoch: [4][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1796)\tLoss (L1) 13.998 (16.284)\n",
      "2022-11-30 18:28:47,456 | Epoch: [4][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1697)\tLoss (L1) 18.173 (16.176)\n",
      "2022-11-30 18:28:49,689 | Epoch: [4][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1608)\tLoss (L1) 7.976 (16.137)\n",
      "2022-11-30 18:28:51,918 | Epoch: [4][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1528)\tLoss (L1) 16.087 (16.054)\n",
      "2022-11-30 18:28:54,166 | Epoch: [4][210/297]\tTime   0.24 (  0.37)\tData 0.0000 (0.1456)\tLoss (L1) 19.030 (16.034)\n",
      "2022-11-30 18:28:56,397 | Epoch: [4][220/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1390)\tLoss (L1) 10.794 (15.938)\n",
      "2022-11-30 18:28:58,644 | Epoch: [4][230/297]\tTime   0.23 (  0.36)\tData 0.0010 (0.1330)\tLoss (L1) 9.586 (15.912)\n",
      "2022-11-30 18:29:00,886 | Epoch: [4][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1275)\tLoss (L1) 27.758 (15.854)\n",
      "2022-11-30 18:29:03,136 | Epoch: [4][250/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1224)\tLoss (L1) 17.209 (15.864)\n",
      "2022-11-30 18:29:05,371 | Epoch: [4][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1177)\tLoss (L1) 14.403 (15.822)\n",
      "2022-11-30 18:29:07,610 | Epoch: [4][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1134)\tLoss (L1) 11.044 (15.733)\n",
      "2022-11-30 18:29:09,848 | Epoch: [4][280/297]\tTime   0.22 (  0.33)\tData 0.0010 (0.1094)\tLoss (L1) 19.505 (15.775)\n",
      "2022-11-30 18:29:12,089 | Epoch: [4][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1056)\tLoss (L1) 12.916 (15.795)\n",
      "2022-11-30 18:29:41,824 | Val: [ 0/38]\tTime 27.794 (27.794)\tLoss (MSE) 450.815 (450.815)\tLoss (L1) 16.587 (16.587)\n",
      "2022-11-30 18:29:42,505 | Val: [10/38]\tTime  0.064 ( 2.589)\tLoss (MSE) 625.358 (612.318)\tLoss (L1) 19.429 (19.208)\n",
      "2022-11-30 18:29:43,153 | Val: [20/38]\tTime  0.064 ( 1.387)\tLoss (MSE) 520.267 (601.566)\tLoss (L1) 19.294 (19.361)\n",
      "2022-11-30 18:29:43,802 | Val: [30/38]\tTime  0.064 ( 0.960)\tLoss (MSE) 640.917 (589.646)\tLoss (L1) 20.298 (19.217)\n",
      "2022-11-30 18:29:44,847 |  * Overall: MSE 592.444\tL1 19.264\tG-Mean 12.533\n",
      "2022-11-30 18:29:44,848 |  * Many: MSE 504.873\tL1 18.007\tG-Mean 11.874\n",
      "2022-11-30 18:29:44,848 |  * Median: MSE 1357.236\tL1 30.416\tG-Mean 20.283\n",
      "2022-11-30 18:29:44,849 |  * Low: MSE 2540.232\tL1 45.187\tG-Mean 37.030\n",
      "2022-11-30 18:29:44,850 | Best L1 Loss: 15.035\n",
      "2022-11-30 18:29:45,031 | Epoch #4: Train loss [15.7874]; Val loss: MSE [592.4436], L1 [19.2643], G-Mean [12.5328]\n",
      "2022-11-30 18:30:15,780 | Epoch: [5][  0/297]\tTime  30.75 ( 30.75)\tData 30.4318 (30.4318)\tLoss (L1) 33.232 (33.232)\n",
      "2022-11-30 18:30:18,022 | Epoch: [5][ 10/297]\tTime   0.22 (  3.00)\tData 0.0000 (2.7669)\tLoss (L1) 11.931 (20.374)\n",
      "2022-11-30 18:30:20,248 | Epoch: [5][ 20/297]\tTime   0.22 (  1.68)\tData 0.0000 (1.4494)\tLoss (L1) 16.887 (18.343)\n",
      "2022-11-30 18:30:22,482 | Epoch: [5][ 30/297]\tTime   0.22 (  1.21)\tData 0.0000 (0.9819)\tLoss (L1) 14.252 (16.637)\n",
      "2022-11-30 18:30:24,715 | Epoch: [5][ 40/297]\tTime   0.22 (  0.97)\tData 0.0000 (0.7425)\tLoss (L1) 12.786 (15.628)\n",
      "2022-11-30 18:30:26,949 | Epoch: [5][ 50/297]\tTime   0.22 (  0.82)\tData 0.0000 (0.5969)\tLoss (L1) 23.098 (15.148)\n",
      "2022-11-30 18:30:29,176 | Epoch: [5][ 60/297]\tTime   0.22 (  0.72)\tData 0.0000 (0.4991)\tLoss (L1) 11.652 (15.010)\n",
      "2022-11-30 18:30:31,410 | Epoch: [5][ 70/297]\tTime   0.22 (  0.65)\tData 0.0000 (0.4288)\tLoss (L1) 15.072 (15.235)\n",
      "2022-11-30 18:30:33,643 | Epoch: [5][ 80/297]\tTime   0.22 (  0.60)\tData 0.0000 (0.3759)\tLoss (L1) 11.180 (14.892)\n",
      "2022-11-30 18:30:35,874 | Epoch: [5][ 90/297]\tTime   0.22 (  0.56)\tData 0.0000 (0.3346)\tLoss (L1) 12.669 (15.331)\n",
      "2022-11-30 18:30:38,105 | Epoch: [5][100/297]\tTime   0.23 (  0.53)\tData 0.0000 (0.3015)\tLoss (L1) 50.710 (15.545)\n",
      "2022-11-30 18:30:40,336 | Epoch: [5][110/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2743)\tLoss (L1) 16.021 (15.614)\n",
      "2022-11-30 18:30:42,570 | Epoch: [5][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2516)\tLoss (L1) 14.755 (15.572)\n",
      "2022-11-30 18:30:44,803 | Epoch: [5][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2324)\tLoss (L1) 11.090 (15.409)\n",
      "2022-11-30 18:30:47,038 | Epoch: [5][140/297]\tTime   0.23 (  0.44)\tData 0.0000 (0.2160)\tLoss (L1) 21.472 (15.319)\n",
      "2022-11-30 18:30:49,271 | Epoch: [5][150/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2017)\tLoss (L1) 16.149 (15.273)\n",
      "2022-11-30 18:30:51,503 | Epoch: [5][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1891)\tLoss (L1) 11.333 (15.131)\n",
      "2022-11-30 18:30:53,735 | Epoch: [5][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1781)\tLoss (L1) 12.402 (14.937)\n",
      "2022-11-30 18:30:55,982 | Epoch: [5][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1683)\tLoss (L1) 10.056 (14.984)\n",
      "2022-11-30 18:30:58,220 | Epoch: [5][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1595)\tLoss (L1) 15.157 (15.120)\n",
      "2022-11-30 18:31:00,464 | Epoch: [5][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1516)\tLoss (L1) 17.559 (15.280)\n",
      "2022-11-30 18:31:02,703 | Epoch: [5][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1444)\tLoss (L1) 11.334 (15.177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:31:04,936 | Epoch: [5][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1378)\tLoss (L1) 15.837 (15.153)\n",
      "2022-11-30 18:31:07,168 | Epoch: [5][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1319)\tLoss (L1) 8.486 (15.022)\n",
      "2022-11-30 18:31:09,404 | Epoch: [5][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1264)\tLoss (L1) 12.740 (15.053)\n",
      "2022-11-30 18:31:11,642 | Epoch: [5][250/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1214)\tLoss (L1) 15.966 (14.966)\n",
      "2022-11-30 18:31:13,877 | Epoch: [5][260/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1168)\tLoss (L1) 10.926 (14.895)\n",
      "2022-11-30 18:31:16,139 | Epoch: [5][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1124)\tLoss (L1) 10.140 (14.804)\n",
      "2022-11-30 18:31:18,392 | Epoch: [5][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1085)\tLoss (L1) 10.288 (14.940)\n",
      "2022-11-30 18:31:20,631 | Epoch: [5][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1047)\tLoss (L1) 12.551 (14.966)\n",
      "2022-11-30 18:31:50,494 | Val: [ 0/38]\tTime 27.774 (27.774)\tLoss (MSE) 248.681 (248.681)\tLoss (L1) 12.393 (12.393)\n",
      "2022-11-30 18:31:51,189 | Val: [10/38]\tTime  0.065 ( 2.588)\tLoss (MSE) 461.317 (344.837)\tLoss (L1) 16.683 (14.263)\n",
      "2022-11-30 18:31:51,834 | Val: [20/38]\tTime  0.065 ( 1.386)\tLoss (MSE) 378.963 (346.207)\tLoss (L1) 14.120 (14.216)\n",
      "2022-11-30 18:31:52,478 | Val: [30/38]\tTime  0.064 ( 0.960)\tLoss (MSE) 375.315 (350.164)\tLoss (L1) 14.959 (14.457)\n",
      "2022-11-30 18:31:53,531 |  * Overall: MSE 355.344\tL1 14.527\tG-Mean 9.231\n",
      "2022-11-30 18:31:53,531 |  * Many: MSE 320.696\tL1 13.983\tG-Mean 9.000\n",
      "2022-11-30 18:31:53,532 |  * Median: MSE 653.588\tL1 19.105\tG-Mean 11.395\n",
      "2022-11-30 18:31:53,533 |  * Low: MSE 1177.897\tL1 28.601\tG-Mean 18.592\n",
      "2022-11-30 18:31:53,534 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:31:53,743 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:31:53,830 | Epoch #5: Train loss [15.0167]; Val loss: MSE [355.3437], L1 [14.5266], G-Mean [9.2315]\n",
      "2022-11-30 18:32:23,838 | Epoch: [6][  0/297]\tTime  30.01 ( 30.01)\tData 29.6507 (29.6507)\tLoss (L1) 9.302 (9.302)\n",
      "2022-11-30 18:32:26,093 | Epoch: [6][ 10/297]\tTime   0.22 (  2.93)\tData 0.0010 (2.6959)\tLoss (L1) 9.488 (12.419)\n",
      "2022-11-30 18:32:28,321 | Epoch: [6][ 20/297]\tTime   0.22 (  1.64)\tData 0.0010 (1.4123)\tLoss (L1) 14.468 (12.711)\n",
      "2022-11-30 18:32:30,547 | Epoch: [6][ 30/297]\tTime   0.22 (  1.18)\tData 0.0000 (0.9568)\tLoss (L1) 11.775 (13.944)\n",
      "2022-11-30 18:32:32,784 | Epoch: [6][ 40/297]\tTime   0.22 (  0.95)\tData 0.0010 (0.7235)\tLoss (L1) 10.129 (13.687)\n",
      "2022-11-30 18:32:35,023 | Epoch: [6][ 50/297]\tTime   0.23 (  0.81)\tData 0.0010 (0.5817)\tLoss (L1) 16.864 (13.289)\n",
      "2022-11-30 18:32:37,251 | Epoch: [6][ 60/297]\tTime   0.22 (  0.71)\tData 0.0000 (0.4864)\tLoss (L1) 10.054 (13.124)\n",
      "2022-11-30 18:32:39,477 | Epoch: [6][ 70/297]\tTime   0.22 (  0.64)\tData 0.0000 (0.4179)\tLoss (L1) 10.781 (13.228)\n",
      "2022-11-30 18:32:41,711 | Epoch: [6][ 80/297]\tTime   0.22 (  0.59)\tData 0.0000 (0.3663)\tLoss (L1) 11.046 (13.603)\n",
      "2022-11-30 18:32:43,945 | Epoch: [6][ 90/297]\tTime   0.22 (  0.55)\tData 0.0000 (0.3261)\tLoss (L1) 12.867 (13.431)\n",
      "2022-11-30 18:32:46,179 | Epoch: [6][100/297]\tTime   0.22 (  0.52)\tData 0.0000 (0.2938)\tLoss (L1) 13.493 (13.508)\n",
      "2022-11-30 18:32:48,405 | Epoch: [6][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2673)\tLoss (L1) 11.188 (13.466)\n",
      "2022-11-30 18:32:50,636 | Epoch: [6][120/297]\tTime   0.22 (  0.47)\tData 0.0000 (0.2453)\tLoss (L1) 11.074 (13.465)\n",
      "2022-11-30 18:32:52,870 | Epoch: [6][130/297]\tTime   0.23 (  0.45)\tData 0.0010 (0.2266)\tLoss (L1) 10.839 (13.456)\n",
      "2022-11-30 18:32:55,126 | Epoch: [6][140/297]\tTime   0.24 (  0.43)\tData 0.0000 (0.2105)\tLoss (L1) 12.896 (13.569)\n",
      "2022-11-30 18:32:57,348 | Epoch: [6][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1966)\tLoss (L1) 14.102 (13.552)\n",
      "2022-11-30 18:32:59,588 | Epoch: [6][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1844)\tLoss (L1) 8.071 (13.641)\n",
      "2022-11-30 18:33:01,824 | Epoch: [6][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1736)\tLoss (L1) 19.863 (13.699)\n",
      "2022-11-30 18:33:04,059 | Epoch: [6][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1640)\tLoss (L1) 36.019 (14.036)\n",
      "2022-11-30 18:33:06,289 | Epoch: [6][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1554)\tLoss (L1) 34.228 (14.156)\n",
      "2022-11-30 18:33:08,527 | Epoch: [6][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1477)\tLoss (L1) 11.022 (14.024)\n",
      "2022-11-30 18:33:10,767 | Epoch: [6][210/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1407)\tLoss (L1) 11.110 (14.043)\n",
      "2022-11-30 18:33:12,999 | Epoch: [6][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1344)\tLoss (L1) 12.298 (14.011)\n",
      "2022-11-30 18:33:15,259 | Epoch: [6][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1285)\tLoss (L1) 11.026 (14.130)\n",
      "2022-11-30 18:33:17,516 | Epoch: [6][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1232)\tLoss (L1) 35.791 (14.327)\n",
      "2022-11-30 18:33:19,749 | Epoch: [6][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1183)\tLoss (L1) 12.791 (14.408)\n",
      "2022-11-30 18:33:21,986 | Epoch: [6][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1138)\tLoss (L1) 10.348 (14.426)\n",
      "2022-11-30 18:33:24,219 | Epoch: [6][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1096)\tLoss (L1) 12.135 (14.416)\n",
      "2022-11-30 18:33:26,452 | Epoch: [6][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1057)\tLoss (L1) 13.823 (14.489)\n",
      "2022-11-30 18:33:28,694 | Epoch: [6][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1021)\tLoss (L1) 12.258 (14.457)\n",
      "2022-11-30 18:33:59,575 | Val: [ 0/38]\tTime 28.928 (28.928)\tLoss (MSE) 309.062 (309.062)\tLoss (L1) 14.071 (14.071)\n",
      "2022-11-30 18:34:00,296 | Val: [10/38]\tTime  0.065 ( 2.695)\tLoss (MSE) 477.530 (376.675)\tLoss (L1) 17.545 (15.227)\n",
      "2022-11-30 18:34:00,950 | Val: [20/38]\tTime  0.064 ( 1.443)\tLoss (MSE) 389.741 (379.043)\tLoss (L1) 15.880 (15.396)\n",
      "2022-11-30 18:34:01,600 | Val: [30/38]\tTime  0.064 ( 0.999)\tLoss (MSE) 311.758 (378.877)\tLoss (L1) 13.591 (15.401)\n",
      "2022-11-30 18:34:02,667 |  * Overall: MSE 374.872\tL1 15.298\tG-Mean 10.012\n",
      "2022-11-30 18:34:02,668 |  * Many: MSE 376.029\tL1 15.415\tG-Mean 10.119\n",
      "2022-11-30 18:34:02,668 |  * Median: MSE 365.981\tL1 14.186\tG-Mean 9.169\n",
      "2022-11-30 18:34:02,669 |  * Low: MSE 334.607\tL1 13.794\tG-Mean 7.446\n",
      "2022-11-30 18:34:02,670 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:34:02,845 | Epoch #6: Train loss [14.4803]; Val loss: MSE [374.8721], L1 [15.2979], G-Mean [10.0116]\n",
      "2022-11-30 18:34:32,023 | Epoch: [7][  0/297]\tTime  29.18 ( 29.18)\tData 28.8423 (28.8423)\tLoss (L1) 11.967 (11.967)\n",
      "2022-11-30 18:34:34,361 | Epoch: [7][ 10/297]\tTime   0.22 (  2.86)\tData 0.0000 (2.6221)\tLoss (L1) 16.217 (12.648)\n",
      "2022-11-30 18:34:36,638 | Epoch: [7][ 20/297]\tTime   0.22 (  1.61)\tData 0.0000 (1.3735)\tLoss (L1) 14.422 (13.511)\n",
      "2022-11-30 18:34:38,868 | Epoch: [7][ 30/297]\tTime   0.22 (  1.16)\tData 0.0000 (0.9305)\tLoss (L1) 13.390 (14.066)\n",
      "2022-11-30 18:34:41,106 | Epoch: [7][ 40/297]\tTime   0.22 (  0.93)\tData 0.0000 (0.7036)\tLoss (L1) 16.722 (14.990)\n",
      "2022-11-30 18:34:43,347 | Epoch: [7][ 50/297]\tTime   0.22 (  0.79)\tData 0.0000 (0.5657)\tLoss (L1) 11.131 (14.903)\n",
      "2022-11-30 18:34:45,576 | Epoch: [7][ 60/297]\tTime   0.22 (  0.70)\tData 0.0000 (0.4730)\tLoss (L1) 11.559 (14.306)\n",
      "2022-11-30 18:34:47,848 | Epoch: [7][ 70/297]\tTime   0.22 (  0.63)\tData 0.0000 (0.4064)\tLoss (L1) 11.649 (14.338)\n",
      "2022-11-30 18:34:50,123 | Epoch: [7][ 80/297]\tTime   0.23 (  0.58)\tData 0.0000 (0.3562)\tLoss (L1) 11.385 (14.787)\n",
      "2022-11-30 18:34:52,358 | Epoch: [7][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3171)\tLoss (L1) 32.111 (14.993)\n",
      "2022-11-30 18:34:54,600 | Epoch: [7][100/297]\tTime   0.23 (  0.51)\tData 0.0000 (0.2857)\tLoss (L1) 12.018 (14.790)\n",
      "2022-11-30 18:34:57,115 | Epoch: [7][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2600)\tLoss (L1) 13.832 (14.518)\n",
      "2022-11-30 18:34:59,520 | Epoch: [7][120/297]\tTime   0.26 (  0.47)\tData 0.0000 (0.2386)\tLoss (L1) 15.957 (14.440)\n",
      "2022-11-30 18:35:01,934 | Epoch: [7][130/297]\tTime   0.23 (  0.45)\tData 0.0000 (0.2204)\tLoss (L1) 14.558 (14.582)\n",
      "2022-11-30 18:35:04,364 | Epoch: [7][140/297]\tTime   0.23 (  0.44)\tData 0.0000 (0.2047)\tLoss (L1) 13.771 (14.379)\n",
      "2022-11-30 18:35:06,742 | Epoch: [7][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1912)\tLoss (L1) 15.531 (14.545)\n",
      "2022-11-30 18:35:08,975 | Epoch: [7][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1793)\tLoss (L1) 9.382 (14.422)\n",
      "2022-11-30 18:35:11,215 | Epoch: [7][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1688)\tLoss (L1) 41.253 (14.436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:35:13,455 | Epoch: [7][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1595)\tLoss (L1) 15.869 (14.421)\n",
      "2022-11-30 18:35:15,768 | Epoch: [7][190/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1512)\tLoss (L1) 11.238 (14.378)\n",
      "2022-11-30 18:35:18,025 | Epoch: [7][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1437)\tLoss (L1) 7.553 (14.292)\n",
      "2022-11-30 18:35:20,271 | Epoch: [7][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1369)\tLoss (L1) 10.605 (14.258)\n",
      "2022-11-30 18:35:22,513 | Epoch: [7][220/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1307)\tLoss (L1) 9.924 (14.178)\n",
      "2022-11-30 18:35:24,752 | Epoch: [7][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1250)\tLoss (L1) 19.413 (14.151)\n",
      "2022-11-30 18:35:26,996 | Epoch: [7][240/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1198)\tLoss (L1) 9.357 (14.129)\n",
      "2022-11-30 18:35:29,237 | Epoch: [7][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1151)\tLoss (L1) 8.779 (14.064)\n",
      "2022-11-30 18:35:31,482 | Epoch: [7][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1107)\tLoss (L1) 9.686 (14.062)\n",
      "2022-11-30 18:35:33,721 | Epoch: [7][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1066)\tLoss (L1) 11.161 (14.066)\n",
      "2022-11-30 18:35:35,967 | Epoch: [7][280/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1028)\tLoss (L1) 9.307 (13.967)\n",
      "2022-11-30 18:35:38,207 | Epoch: [7][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.0993)\tLoss (L1) 11.593 (13.982)\n",
      "2022-11-30 18:36:09,243 | Val: [ 0/38]\tTime 29.075 (29.075)\tLoss (MSE) 471.704 (471.704)\tLoss (L1) 16.788 (16.788)\n",
      "2022-11-30 18:36:09,933 | Val: [10/38]\tTime  0.063 ( 2.706)\tLoss (MSE) 676.317 (585.310)\tLoss (L1) 20.279 (18.731)\n",
      "2022-11-30 18:36:10,582 | Val: [20/38]\tTime  0.066 ( 1.448)\tLoss (MSE) 495.281 (566.867)\tLoss (L1) 18.913 (18.757)\n",
      "2022-11-30 18:36:11,229 | Val: [30/38]\tTime  0.066 ( 1.002)\tLoss (MSE) 629.715 (561.888)\tLoss (L1) 19.305 (18.735)\n",
      "2022-11-30 18:36:12,282 |  * Overall: MSE 559.280\tL1 18.741\tG-Mean 12.454\n",
      "2022-11-30 18:36:12,283 |  * Many: MSE 513.254\tL1 18.199\tG-Mean 12.277\n",
      "2022-11-30 18:36:12,283 |  * Median: MSE 960.541\tL1 23.681\tG-Mean 14.414\n",
      "2022-11-30 18:36:12,284 |  * Low: MSE 1591.390\tL1 28.258\tG-Mean 13.337\n",
      "2022-11-30 18:36:12,285 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:36:12,461 | Epoch #7: Train loss [13.9780]; Val loss: MSE [559.2803], L1 [18.7410], G-Mean [12.4543]\n",
      "2022-11-30 18:36:40,880 | Epoch: [8][  0/297]\tTime  28.42 ( 28.42)\tData 28.0700 (28.0700)\tLoss (L1) 12.540 (12.540)\n",
      "2022-11-30 18:36:43,128 | Epoch: [8][ 10/297]\tTime   0.22 (  2.79)\tData 0.0000 (2.5522)\tLoss (L1) 10.544 (13.303)\n",
      "2022-11-30 18:36:45,363 | Epoch: [8][ 20/297]\tTime   0.22 (  1.57)\tData 0.0000 (1.3370)\tLoss (L1) 9.599 (12.767)\n",
      "2022-11-30 18:36:47,596 | Epoch: [8][ 30/297]\tTime   0.22 (  1.13)\tData 0.0000 (0.9057)\tLoss (L1) 17.566 (12.604)\n",
      "2022-11-30 18:36:49,830 | Epoch: [8][ 40/297]\tTime   0.22 (  0.91)\tData 0.0000 (0.6848)\tLoss (L1) 12.458 (13.521)\n",
      "2022-11-30 18:36:52,068 | Epoch: [8][ 50/297]\tTime   0.23 (  0.78)\tData 0.0010 (0.5506)\tLoss (L1) 11.396 (13.432)\n",
      "2022-11-30 18:36:54,311 | Epoch: [8][ 60/297]\tTime   0.22 (  0.69)\tData 0.0000 (0.4604)\tLoss (L1) 13.714 (13.557)\n",
      "2022-11-30 18:36:56,561 | Epoch: [8][ 70/297]\tTime   0.22 (  0.62)\tData 0.0010 (0.3956)\tLoss (L1) 12.262 (13.498)\n",
      "2022-11-30 18:36:58,800 | Epoch: [8][ 80/297]\tTime   0.23 (  0.57)\tData 0.0000 (0.3467)\tLoss (L1) 9.965 (13.572)\n",
      "2022-11-30 18:37:01,045 | Epoch: [8][ 90/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.3086)\tLoss (L1) 12.866 (13.530)\n",
      "2022-11-30 18:37:03,286 | Epoch: [8][100/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2781)\tLoss (L1) 16.138 (13.424)\n",
      "2022-11-30 18:37:05,521 | Epoch: [8][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2531)\tLoss (L1) 10.750 (13.322)\n",
      "2022-11-30 18:37:07,759 | Epoch: [8][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2321)\tLoss (L1) 12.886 (13.513)\n",
      "2022-11-30 18:37:09,999 | Epoch: [8][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2144)\tLoss (L1) 10.696 (13.614)\n",
      "2022-11-30 18:37:12,242 | Epoch: [8][140/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1992)\tLoss (L1) 7.299 (13.544)\n",
      "2022-11-30 18:37:14,507 | Epoch: [8][150/297]\tTime   0.25 (  0.41)\tData 0.0000 (0.1860)\tLoss (L1) 10.916 (13.348)\n",
      "2022-11-30 18:37:16,767 | Epoch: [8][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1745)\tLoss (L1) 11.436 (13.282)\n",
      "2022-11-30 18:37:19,007 | Epoch: [8][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1643)\tLoss (L1) 10.858 (13.141)\n",
      "2022-11-30 18:37:21,249 | Epoch: [8][180/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1552)\tLoss (L1) 9.221 (13.252)\n",
      "2022-11-30 18:37:23,485 | Epoch: [8][190/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1471)\tLoss (L1) 12.421 (13.293)\n",
      "2022-11-30 18:37:25,724 | Epoch: [8][200/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1398)\tLoss (L1) 12.844 (13.271)\n",
      "2022-11-30 18:37:27,971 | Epoch: [8][210/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1332)\tLoss (L1) 9.909 (13.304)\n",
      "2022-11-30 18:37:30,215 | Epoch: [8][220/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1272)\tLoss (L1) 16.037 (13.317)\n",
      "2022-11-30 18:37:32,460 | Epoch: [8][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1217)\tLoss (L1) 11.006 (13.271)\n",
      "2022-11-30 18:37:34,704 | Epoch: [8][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1166)\tLoss (L1) 30.577 (13.270)\n",
      "2022-11-30 18:37:36,943 | Epoch: [8][250/297]\tTime   0.22 (  0.34)\tData 0.0010 (0.1120)\tLoss (L1) 11.113 (13.250)\n",
      "2022-11-30 18:37:39,184 | Epoch: [8][260/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1077)\tLoss (L1) 7.457 (13.182)\n",
      "2022-11-30 18:37:41,427 | Epoch: [8][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1037)\tLoss (L1) 24.654 (13.140)\n",
      "2022-11-30 18:37:43,669 | Epoch: [8][280/297]\tTime   0.23 (  0.32)\tData 0.0000 (0.1001)\tLoss (L1) 11.552 (13.160)\n",
      "2022-11-30 18:37:45,912 | Epoch: [8][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0966)\tLoss (L1) 7.236 (13.236)\n",
      "2022-11-30 18:38:18,352 | Val: [ 0/38]\tTime 30.491 (30.491)\tLoss (MSE) 856.078 (856.078)\tLoss (L1) 25.168 (25.168)\n",
      "2022-11-30 18:38:19,057 | Val: [10/38]\tTime  0.064 ( 2.836)\tLoss (MSE) 992.197 (991.824)\tLoss (L1) 26.101 (26.005)\n",
      "2022-11-30 18:38:19,705 | Val: [20/38]\tTime  0.064 ( 1.516)\tLoss (MSE) 828.795 (965.162)\tLoss (L1) 24.871 (25.833)\n",
      "2022-11-30 18:38:20,353 | Val: [30/38]\tTime  0.064 ( 1.048)\tLoss (MSE) 996.886 (946.236)\tLoss (L1) 26.051 (25.632)\n",
      "2022-11-30 18:38:21,410 |  * Overall: MSE 944.244\tL1 25.674\tG-Mean 18.748\n",
      "2022-11-30 18:38:21,410 |  * Many: MSE 838.000\tL1 24.440\tG-Mean 18.064\n",
      "2022-11-30 18:38:21,411 |  * Median: MSE 1878.690\tL1 36.768\tG-Mean 26.603\n",
      "2022-11-30 18:38:21,411 |  * Low: MSE 3228.843\tL1 49.353\tG-Mean 31.817\n",
      "2022-11-30 18:38:21,412 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:38:21,592 | Epoch #8: Train loss [13.5791]; Val loss: MSE [944.2442], L1 [25.6743], G-Mean [18.7482]\n",
      "2022-11-30 18:38:50,044 | Epoch: [9][  0/297]\tTime  28.45 ( 28.45)\tData 28.3240 (28.3240)\tLoss (L1) 18.152 (18.152)\n",
      "2022-11-30 18:38:52,405 | Epoch: [9][ 10/297]\tTime   0.23 (  2.80)\tData 0.0000 (2.5751)\tLoss (L1) 13.662 (15.830)\n",
      "2022-11-30 18:38:54,923 | Epoch: [9][ 20/297]\tTime   0.24 (  1.59)\tData 0.0010 (1.3490)\tLoss (L1) 10.805 (14.739)\n",
      "2022-11-30 18:38:57,230 | Epoch: [9][ 30/297]\tTime   0.22 (  1.15)\tData 0.0000 (0.9140)\tLoss (L1) 14.551 (14.789)\n",
      "2022-11-30 18:38:59,461 | Epoch: [9][ 40/297]\tTime   0.22 (  0.92)\tData 0.0000 (0.6911)\tLoss (L1) 13.858 (14.874)\n",
      "2022-11-30 18:39:01,701 | Epoch: [9][ 50/297]\tTime   0.22 (  0.79)\tData 0.0000 (0.5556)\tLoss (L1) 30.105 (15.114)\n",
      "2022-11-30 18:39:03,936 | Epoch: [9][ 60/297]\tTime   0.22 (  0.69)\tData 0.0000 (0.4646)\tLoss (L1) 13.902 (14.897)\n",
      "2022-11-30 18:39:06,165 | Epoch: [9][ 70/297]\tTime   0.22 (  0.63)\tData 0.0000 (0.3992)\tLoss (L1) 26.780 (14.846)\n",
      "2022-11-30 18:39:08,397 | Epoch: [9][ 80/297]\tTime   0.22 (  0.58)\tData 0.0000 (0.3499)\tLoss (L1) 9.271 (14.473)\n",
      "2022-11-30 18:39:10,629 | Epoch: [9][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3115)\tLoss (L1) 23.156 (14.764)\n",
      "2022-11-30 18:39:12,865 | Epoch: [9][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2806)\tLoss (L1) 11.251 (14.754)\n",
      "2022-11-30 18:39:15,123 | Epoch: [9][110/297]\tTime   0.23 (  0.48)\tData 0.0000 (0.2554)\tLoss (L1) 13.014 (14.878)\n",
      "2022-11-30 18:39:17,386 | Epoch: [9][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2343)\tLoss (L1) 11.760 (14.600)\n",
      "2022-11-30 18:39:19,622 | Epoch: [9][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2164)\tLoss (L1) 8.850 (14.419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:39:21,858 | Epoch: [9][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2011)\tLoss (L1) 12.520 (14.511)\n",
      "2022-11-30 18:39:24,093 | Epoch: [9][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1878)\tLoss (L1) 15.860 (14.330)\n",
      "2022-11-30 18:39:26,324 | Epoch: [9][160/297]\tTime   0.22 (  0.40)\tData 0.0010 (0.1761)\tLoss (L1) 15.817 (14.236)\n",
      "2022-11-30 18:39:28,557 | Epoch: [9][170/297]\tTime   0.22 (  0.39)\tData 0.0010 (0.1658)\tLoss (L1) 11.484 (14.055)\n",
      "2022-11-30 18:39:30,796 | Epoch: [9][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1567)\tLoss (L1) 10.079 (13.892)\n",
      "2022-11-30 18:39:33,029 | Epoch: [9][190/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1485)\tLoss (L1) 16.351 (13.782)\n",
      "2022-11-30 18:39:35,263 | Epoch: [9][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1411)\tLoss (L1) 10.351 (13.684)\n",
      "2022-11-30 18:39:37,500 | Epoch: [9][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1344)\tLoss (L1) 29.805 (13.610)\n",
      "2022-11-30 18:39:39,737 | Epoch: [9][220/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1283)\tLoss (L1) 9.681 (13.602)\n",
      "2022-11-30 18:39:41,973 | Epoch: [9][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1228)\tLoss (L1) 12.580 (13.628)\n",
      "2022-11-30 18:39:44,217 | Epoch: [9][240/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1177)\tLoss (L1) 14.311 (13.534)\n",
      "2022-11-30 18:39:46,442 | Epoch: [9][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1130)\tLoss (L1) 12.845 (13.484)\n",
      "2022-11-30 18:39:48,679 | Epoch: [9][260/297]\tTime   0.23 (  0.33)\tData 0.0010 (0.1087)\tLoss (L1) 25.234 (13.537)\n",
      "2022-11-30 18:39:50,921 | Epoch: [9][270/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1047)\tLoss (L1) 9.346 (13.501)\n",
      "2022-11-30 18:39:53,163 | Epoch: [9][280/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1010)\tLoss (L1) 9.609 (13.568)\n",
      "2022-11-30 18:39:55,406 | Epoch: [9][290/297]\tTime   0.23 (  0.32)\tData 0.0000 (0.0975)\tLoss (L1) 21.906 (13.534)\n",
      "2022-11-30 18:40:26,015 | Val: [ 0/38]\tTime 28.596 (28.596)\tLoss (MSE) 1036.453 (1036.453)\tLoss (L1) 27.906 (27.906)\n",
      "2022-11-30 18:40:26,820 | Val: [10/38]\tTime  0.081 ( 2.673)\tLoss (MSE) 1422.814 (1164.017)\tLoss (L1) 33.384 (29.867)\n",
      "2022-11-30 18:40:27,553 | Val: [20/38]\tTime  0.082 ( 1.435)\tLoss (MSE) 1346.523 (1172.817)\tLoss (L1) 32.046 (30.001)\n",
      "2022-11-30 18:40:28,290 | Val: [30/38]\tTime  0.074 ( 0.996)\tLoss (MSE) 984.553 (1162.888)\tLoss (L1) 26.861 (29.749)\n",
      "2022-11-30 18:40:29,431 |  * Overall: MSE 1159.404\tL1 29.731\tG-Mean 23.413\n",
      "2022-11-30 18:40:29,431 |  * Many: MSE 1192.651\tL1 30.321\tG-Mean 24.137\n",
      "2022-11-30 18:40:29,432 |  * Median: MSE 884.641\tL1 24.892\tG-Mean 18.216\n",
      "2022-11-30 18:40:29,432 |  * Low: MSE 233.757\tL1 12.933\tG-Mean 9.959\n",
      "2022-11-30 18:40:29,433 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:40:29,608 | Epoch #9: Train loss [13.4837]; Val loss: MSE [1159.4044], L1 [29.7313], G-Mean [23.4133]\n",
      "2022-11-30 18:40:58,982 | Epoch: [10][  0/297]\tTime  29.37 ( 29.37)\tData 29.0597 (29.0597)\tLoss (L1) 14.505 (14.505)\n",
      "2022-11-30 18:41:01,236 | Epoch: [10][ 10/297]\tTime   0.23 (  2.88)\tData 0.0000 (2.6423)\tLoss (L1) 14.427 (16.069)\n",
      "2022-11-30 18:41:03,469 | Epoch: [10][ 20/297]\tTime   0.22 (  1.61)\tData 0.0010 (1.3842)\tLoss (L1) 9.462 (13.801)\n",
      "2022-11-30 18:41:05,698 | Epoch: [10][ 30/297]\tTime   0.22 (  1.16)\tData 0.0000 (0.9377)\tLoss (L1) 11.040 (13.115)\n",
      "2022-11-30 18:41:07,924 | Epoch: [10][ 40/297]\tTime   0.22 (  0.93)\tData 0.0000 (0.7090)\tLoss (L1) 11.887 (13.324)\n",
      "2022-11-30 18:41:10,164 | Epoch: [10][ 50/297]\tTime   0.24 (  0.80)\tData 0.0000 (0.5700)\tLoss (L1) 11.622 (13.741)\n",
      "2022-11-30 18:41:12,379 | Epoch: [10][ 60/297]\tTime   0.22 (  0.70)\tData 0.0000 (0.4766)\tLoss (L1) 18.213 (14.378)\n",
      "2022-11-30 18:41:14,632 | Epoch: [10][ 70/297]\tTime   0.23 (  0.63)\tData 0.0000 (0.4095)\tLoss (L1) 14.153 (14.180)\n",
      "2022-11-30 18:41:16,882 | Epoch: [10][ 80/297]\tTime   0.22 (  0.58)\tData 0.0000 (0.3589)\tLoss (L1) 12.840 (13.881)\n",
      "2022-11-30 18:41:19,111 | Epoch: [10][ 90/297]\tTime   0.23 (  0.54)\tData 0.0000 (0.3195)\tLoss (L1) 12.371 (13.818)\n",
      "2022-11-30 18:41:21,341 | Epoch: [10][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2879)\tLoss (L1) 8.954 (13.610)\n",
      "2022-11-30 18:41:23,573 | Epoch: [10][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2619)\tLoss (L1) 18.082 (13.574)\n",
      "2022-11-30 18:41:25,806 | Epoch: [10][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2403)\tLoss (L1) 12.232 (13.373)\n",
      "2022-11-30 18:41:28,035 | Epoch: [10][130/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2220)\tLoss (L1) 17.075 (13.361)\n",
      "2022-11-30 18:41:30,271 | Epoch: [10][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2062)\tLoss (L1) 12.633 (13.432)\n",
      "2022-11-30 18:41:32,505 | Epoch: [10][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1926)\tLoss (L1) 9.307 (13.421)\n",
      "2022-11-30 18:41:34,737 | Epoch: [10][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1806)\tLoss (L1) 53.077 (13.728)\n",
      "2022-11-30 18:41:36,980 | Epoch: [10][170/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1701)\tLoss (L1) 9.595 (13.718)\n",
      "2022-11-30 18:41:39,208 | Epoch: [10][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1607)\tLoss (L1) 12.050 (13.852)\n",
      "2022-11-30 18:41:41,442 | Epoch: [10][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1523)\tLoss (L1) 12.995 (13.823)\n",
      "2022-11-30 18:41:43,677 | Epoch: [10][200/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1447)\tLoss (L1) 9.366 (13.809)\n",
      "2022-11-30 18:41:45,912 | Epoch: [10][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1379)\tLoss (L1) 10.612 (13.741)\n",
      "2022-11-30 18:41:48,147 | Epoch: [10][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1316)\tLoss (L1) 11.944 (13.638)\n",
      "2022-11-30 18:41:50,382 | Epoch: [10][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1259)\tLoss (L1) 9.204 (13.608)\n",
      "2022-11-30 18:41:52,619 | Epoch: [10][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1207)\tLoss (L1) 15.863 (13.579)\n",
      "2022-11-30 18:41:54,856 | Epoch: [10][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1159)\tLoss (L1) 8.142 (13.739)\n",
      "2022-11-30 18:41:57,092 | Epoch: [10][260/297]\tTime   0.21 (  0.34)\tData 0.0000 (0.1115)\tLoss (L1) 15.729 (13.694)\n",
      "2022-11-30 18:41:59,326 | Epoch: [10][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1074)\tLoss (L1) 9.669 (13.611)\n",
      "2022-11-30 18:42:01,567 | Epoch: [10][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1036)\tLoss (L1) 18.978 (13.599)\n",
      "2022-11-30 18:42:03,811 | Epoch: [10][290/297]\tTime   0.23 (  0.32)\tData 0.0000 (0.1000)\tLoss (L1) 9.684 (13.574)\n",
      "2022-11-30 18:42:35,099 | Val: [ 0/38]\tTime 29.213 (29.213)\tLoss (MSE) 1915.478 (1915.478)\tLoss (L1) 37.914 (37.914)\n",
      "2022-11-30 18:42:35,794 | Val: [10/38]\tTime  0.067 ( 2.719)\tLoss (MSE) 1774.460 (1636.160)\tLoss (L1) 36.413 (34.860)\n",
      "2022-11-30 18:42:36,440 | Val: [20/38]\tTime  0.065 ( 1.455)\tLoss (MSE) 1441.727 (1600.550)\tLoss (L1) 31.427 (34.151)\n",
      "2022-11-30 18:42:37,085 | Val: [30/38]\tTime  0.064 ( 1.006)\tLoss (MSE) 1388.662 (1575.301)\tLoss (L1) 31.655 (33.754)\n",
      "2022-11-30 18:42:38,127 |  * Overall: MSE 1590.915\tL1 33.922\tG-Mean 25.132\n",
      "2022-11-30 18:42:38,128 |  * Many: MSE 1653.738\tL1 34.999\tG-Mean 26.519\n",
      "2022-11-30 18:42:38,128 |  * Median: MSE 1041.209\tL1 24.307\tG-Mean 15.366\n",
      "2022-11-30 18:42:38,129 |  * Low: MSE 206.063\tL1 12.465\tG-Mean 10.001\n",
      "2022-11-30 18:42:38,130 | Best L1 Loss: 14.527\n",
      "2022-11-30 18:42:38,303 | Epoch #10: Train loss [13.5605]; Val loss: MSE [1590.9151], L1 [33.9216], G-Mean [25.1325]\n",
      "2022-11-30 18:43:07,938 | Epoch: [11][  0/297]\tTime  29.63 ( 29.63)\tData 29.3849 (29.3849)\tLoss (L1) 11.223 (11.223)\n",
      "2022-11-30 18:43:10,165 | Epoch: [11][ 10/297]\tTime   0.22 (  2.90)\tData 0.0000 (2.6722)\tLoss (L1) 15.329 (10.953)\n",
      "2022-11-30 18:43:12,395 | Epoch: [11][ 20/297]\tTime   0.22 (  1.62)\tData 0.0000 (1.3997)\tLoss (L1) 16.962 (12.760)\n",
      "2022-11-30 18:43:14,650 | Epoch: [11][ 30/297]\tTime   0.22 (  1.17)\tData 0.0000 (0.9483)\tLoss (L1) 12.003 (12.188)\n",
      "2022-11-30 18:43:16,901 | Epoch: [11][ 40/297]\tTime   0.22 (  0.94)\tData 0.0010 (0.7170)\tLoss (L1) 13.621 (12.223)\n",
      "2022-11-30 18:43:19,194 | Epoch: [11][ 50/297]\tTime   0.23 (  0.80)\tData 0.0000 (0.5764)\tLoss (L1) 9.512 (12.926)\n",
      "2022-11-30 18:43:21,513 | Epoch: [11][ 60/297]\tTime   0.23 (  0.71)\tData 0.0000 (0.4820)\tLoss (L1) 11.235 (12.912)\n",
      "2022-11-30 18:43:23,836 | Epoch: [11][ 70/297]\tTime   0.23 (  0.64)\tData 0.0000 (0.4141)\tLoss (L1) 11.355 (12.498)\n",
      "2022-11-30 18:43:26,161 | Epoch: [11][ 80/297]\tTime   0.23 (  0.59)\tData 0.0010 (0.3630)\tLoss (L1) 40.293 (13.253)\n",
      "2022-11-30 18:43:28,435 | Epoch: [11][ 90/297]\tTime   0.22 (  0.55)\tData 0.0000 (0.3231)\tLoss (L1) 18.167 (13.536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:43:30,669 | Epoch: [11][100/297]\tTime   0.22 (  0.52)\tData 0.0000 (0.2912)\tLoss (L1) 8.597 (13.788)\n",
      "2022-11-30 18:43:32,905 | Epoch: [11][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2649)\tLoss (L1) 11.564 (13.946)\n",
      "2022-11-30 18:43:35,142 | Epoch: [11][120/297]\tTime   0.22 (  0.47)\tData 0.0010 (0.2431)\tLoss (L1) 13.708 (14.122)\n",
      "2022-11-30 18:43:37,376 | Epoch: [11][130/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2245)\tLoss (L1) 8.476 (14.075)\n",
      "2022-11-30 18:43:39,611 | Epoch: [11][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2086)\tLoss (L1) 8.428 (13.907)\n",
      "2022-11-30 18:43:41,867 | Epoch: [11][150/297]\tTime   0.24 (  0.42)\tData 0.0000 (0.1948)\tLoss (L1) 13.631 (13.718)\n",
      "2022-11-30 18:43:44,198 | Epoch: [11][160/297]\tTime   0.23 (  0.41)\tData 0.0000 (0.1827)\tLoss (L1) 24.676 (13.641)\n",
      "2022-11-30 18:43:46,528 | Epoch: [11][170/297]\tTime   0.23 (  0.40)\tData 0.0000 (0.1720)\tLoss (L1) 9.260 (13.502)\n",
      "2022-11-30 18:43:48,857 | Epoch: [11][180/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1625)\tLoss (L1) 13.572 (13.366)\n",
      "2022-11-30 18:43:51,169 | Epoch: [11][190/297]\tTime   0.22 (  0.38)\tData 0.0010 (0.1540)\tLoss (L1) 12.409 (13.341)\n",
      "2022-11-30 18:43:53,403 | Epoch: [11][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1464)\tLoss (L1) 9.588 (13.249)\n",
      "2022-11-30 18:43:55,649 | Epoch: [11][210/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1394)\tLoss (L1) 24.048 (13.200)\n",
      "2022-11-30 18:43:57,890 | Epoch: [11][220/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1331)\tLoss (L1) 14.243 (13.169)\n",
      "2022-11-30 18:44:00,135 | Epoch: [11][230/297]\tTime   0.23 (  0.35)\tData 0.0010 (0.1274)\tLoss (L1) 11.980 (13.100)\n",
      "2022-11-30 18:44:02,373 | Epoch: [11][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1221)\tLoss (L1) 11.522 (13.031)\n",
      "2022-11-30 18:44:04,629 | Epoch: [11][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1173)\tLoss (L1) 13.087 (13.023)\n",
      "2022-11-30 18:44:06,858 | Epoch: [11][260/297]\tTime   0.22 (  0.34)\tData 0.0010 (0.1128)\tLoss (L1) 11.644 (12.950)\n",
      "2022-11-30 18:44:09,096 | Epoch: [11][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1086)\tLoss (L1) 11.691 (12.962)\n",
      "2022-11-30 18:44:11,332 | Epoch: [11][280/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1048)\tLoss (L1) 9.690 (12.865)\n",
      "2022-11-30 18:44:13,570 | Epoch: [11][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1012)\tLoss (L1) 18.165 (12.808)\n",
      "2022-11-30 18:44:43,294 | Val: [ 0/38]\tTime 27.778 (27.778)\tLoss (MSE) 199.662 (199.662)\tLoss (L1) 11.074 (11.074)\n",
      "2022-11-30 18:44:43,993 | Val: [10/38]\tTime  0.064 ( 2.589)\tLoss (MSE) 304.296 (265.848)\tLoss (L1) 13.465 (11.863)\n",
      "2022-11-30 18:44:44,638 | Val: [20/38]\tTime  0.065 ( 1.387)\tLoss (MSE) 281.582 (252.452)\tLoss (L1) 13.282 (11.794)\n",
      "2022-11-30 18:44:45,283 | Val: [30/38]\tTime  0.064 ( 0.960)\tLoss (MSE) 272.014 (249.199)\tLoss (L1) 12.057 (11.809)\n",
      "2022-11-30 18:44:46,340 |  * Overall: MSE 244.842\tL1 11.764\tG-Mean 7.277\n",
      "2022-11-30 18:44:46,341 |  * Many: MSE 226.408\tL1 11.366\tG-Mean 7.082\n",
      "2022-11-30 18:44:46,341 |  * Median: MSE 406.521\tL1 15.311\tG-Mean 9.366\n",
      "2022-11-30 18:44:46,342 |  * Low: MSE 646.578\tL1 19.859\tG-Mean 11.021\n",
      "2022-11-30 18:44:46,343 | Best L1 Loss: 11.764\n",
      "2022-11-30 18:44:46,515 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:44:46,597 | Epoch #11: Train loss [12.7718]; Val loss: MSE [244.8418], L1 [11.7643], G-Mean [7.2768]\n",
      "2022-11-30 18:45:16,500 | Epoch: [12][  0/297]\tTime  29.90 ( 29.90)\tData 29.6672 (29.6672)\tLoss (L1) 10.679 (10.679)\n",
      "2022-11-30 18:45:18,721 | Epoch: [12][ 10/297]\tTime   0.22 (  2.92)\tData 0.0000 (2.6974)\tLoss (L1) 7.949 (10.213)\n",
      "2022-11-30 18:45:20,951 | Epoch: [12][ 20/297]\tTime   0.22 (  1.64)\tData 0.0000 (1.4130)\tLoss (L1) 14.856 (10.716)\n",
      "2022-11-30 18:45:23,177 | Epoch: [12][ 30/297]\tTime   0.21 (  1.18)\tData 0.0000 (0.9573)\tLoss (L1) 9.550 (11.788)\n",
      "2022-11-30 18:45:25,412 | Epoch: [12][ 40/297]\tTime   0.21 (  0.95)\tData 0.0000 (0.7238)\tLoss (L1) 9.967 (12.795)\n",
      "2022-11-30 18:45:27,639 | Epoch: [12][ 50/297]\tTime   0.22 (  0.80)\tData 0.0000 (0.5819)\tLoss (L1) 9.097 (12.841)\n",
      "2022-11-30 18:45:29,869 | Epoch: [12][ 60/297]\tTime   0.22 (  0.71)\tData 0.0000 (0.4866)\tLoss (L1) 12.532 (12.484)\n",
      "2022-11-30 18:45:32,096 | Epoch: [12][ 70/297]\tTime   0.22 (  0.64)\tData 0.0000 (0.4181)\tLoss (L1) 15.571 (12.451)\n",
      "2022-11-30 18:45:34,328 | Epoch: [12][ 80/297]\tTime   0.22 (  0.59)\tData 0.0010 (0.3665)\tLoss (L1) 14.062 (12.451)\n",
      "2022-11-30 18:45:36,563 | Epoch: [12][ 90/297]\tTime   0.22 (  0.55)\tData 0.0000 (0.3262)\tLoss (L1) 11.132 (12.222)\n",
      "2022-11-30 18:45:38,797 | Epoch: [12][100/297]\tTime   0.22 (  0.52)\tData 0.0000 (0.2939)\tLoss (L1) 11.799 (12.433)\n",
      "2022-11-30 18:45:41,033 | Epoch: [12][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2675)\tLoss (L1) 13.364 (12.398)\n",
      "2022-11-30 18:45:43,268 | Epoch: [12][120/297]\tTime   0.22 (  0.47)\tData 0.0010 (0.2454)\tLoss (L1) 35.616 (12.476)\n",
      "2022-11-30 18:45:45,501 | Epoch: [12][130/297]\tTime   0.22 (  0.45)\tData 0.0010 (0.2267)\tLoss (L1) 11.324 (12.639)\n",
      "2022-11-30 18:45:47,734 | Epoch: [12][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2106)\tLoss (L1) 10.304 (12.644)\n",
      "2022-11-30 18:45:49,969 | Epoch: [12][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1967)\tLoss (L1) 8.478 (12.663)\n",
      "2022-11-30 18:45:52,219 | Epoch: [12][160/297]\tTime   0.24 (  0.41)\tData 0.0010 (0.1845)\tLoss (L1) 16.328 (12.768)\n",
      "2022-11-30 18:45:54,449 | Epoch: [12][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1737)\tLoss (L1) 8.800 (12.755)\n",
      "2022-11-30 18:45:56,693 | Epoch: [12][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1641)\tLoss (L1) 10.629 (12.723)\n",
      "2022-11-30 18:45:58,937 | Epoch: [12][190/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1555)\tLoss (L1) 7.537 (12.598)\n",
      "2022-11-30 18:46:01,183 | Epoch: [12][200/297]\tTime   0.22 (  0.37)\tData 0.0010 (0.1478)\tLoss (L1) 10.578 (12.553)\n",
      "2022-11-30 18:46:03,420 | Epoch: [12][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1408)\tLoss (L1) 10.577 (12.530)\n",
      "2022-11-30 18:46:05,662 | Epoch: [12][220/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1344)\tLoss (L1) 9.651 (12.436)\n",
      "2022-11-30 18:46:07,904 | Epoch: [12][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1286)\tLoss (L1) 11.610 (12.487)\n",
      "2022-11-30 18:46:10,137 | Epoch: [12][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1233)\tLoss (L1) 12.548 (12.403)\n",
      "2022-11-30 18:46:12,379 | Epoch: [12][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1184)\tLoss (L1) 7.824 (12.406)\n",
      "2022-11-30 18:46:14,621 | Epoch: [12][260/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1139)\tLoss (L1) 11.429 (12.321)\n",
      "2022-11-30 18:46:16,862 | Epoch: [12][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1097)\tLoss (L1) 12.091 (12.312)\n",
      "2022-11-30 18:46:19,101 | Epoch: [12][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1058)\tLoss (L1) 20.544 (12.319)\n",
      "2022-11-30 18:46:21,344 | Epoch: [12][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1021)\tLoss (L1) 13.696 (12.360)\n",
      "2022-11-30 18:46:51,627 | Val: [ 0/38]\tTime 28.296 (28.296)\tLoss (MSE) 227.522 (227.522)\tLoss (L1) 12.371 (12.371)\n",
      "2022-11-30 18:46:52,421 | Val: [10/38]\tTime  0.067 ( 2.645)\tLoss (MSE) 287.004 (308.592)\tLoss (L1) 14.064 (13.288)\n",
      "2022-11-30 18:46:53,145 | Val: [20/38]\tTime  0.080 ( 1.420)\tLoss (MSE) 471.079 (308.575)\tLoss (L1) 15.617 (13.159)\n",
      "2022-11-30 18:46:53,813 | Val: [30/38]\tTime  0.065 ( 0.983)\tLoss (MSE) 175.177 (293.408)\tLoss (L1) 10.214 (12.917)\n",
      "2022-11-30 18:46:54,952 |  * Overall: MSE 284.689\tL1 12.827\tG-Mean 8.282\n",
      "2022-11-30 18:46:54,953 |  * Many: MSE 275.012\tL1 12.594\tG-Mean 8.117\n",
      "2022-11-30 18:46:54,954 |  * Median: MSE 375.983\tL1 14.960\tG-Mean 9.922\n",
      "2022-11-30 18:46:54,954 |  * Low: MSE 418.965\tL1 16.853\tG-Mean 12.204\n",
      "2022-11-30 18:46:54,955 | Best L1 Loss: 11.764\n",
      "2022-11-30 18:46:55,155 | Epoch #12: Train loss [12.3864]; Val loss: MSE [284.6890], L1 [12.8274], G-Mean [8.2823]\n",
      "2022-11-30 18:47:25,582 | Epoch: [13][  0/297]\tTime  30.42 ( 30.42)\tData 30.0672 (30.0672)\tLoss (L1) 9.283 (9.283)\n",
      "2022-11-30 18:47:27,858 | Epoch: [13][ 10/297]\tTime   0.24 (  2.97)\tData 0.0000 (2.7337)\tLoss (L1) 8.509 (12.348)\n",
      "2022-11-30 18:47:30,161 | Epoch: [13][ 20/297]\tTime   0.22 (  1.67)\tData 0.0000 (1.4320)\tLoss (L1) 7.550 (11.619)\n",
      "2022-11-30 18:47:32,457 | Epoch: [13][ 30/297]\tTime   0.22 (  1.20)\tData 0.0000 (0.9701)\tLoss (L1) 12.547 (11.481)\n",
      "2022-11-30 18:47:34,804 | Epoch: [13][ 40/297]\tTime   0.23 (  0.97)\tData 0.0000 (0.7335)\tLoss (L1) 9.761 (11.510)\n",
      "2022-11-30 18:47:37,148 | Epoch: [13][ 50/297]\tTime   0.23 (  0.82)\tData 0.0000 (0.5897)\tLoss (L1) 8.827 (12.416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:47:39,380 | Epoch: [13][ 60/297]\tTime   0.22 (  0.72)\tData 0.0000 (0.4931)\tLoss (L1) 12.245 (12.301)\n",
      "2022-11-30 18:47:41,773 | Epoch: [13][ 70/297]\tTime   0.25 (  0.66)\tData 0.0000 (0.4236)\tLoss (L1) 11.881 (13.172)\n",
      "2022-11-30 18:47:44,107 | Epoch: [13][ 80/297]\tTime   0.23 (  0.60)\tData 0.0010 (0.3714)\tLoss (L1) 11.258 (13.011)\n",
      "2022-11-30 18:47:46,432 | Epoch: [13][ 90/297]\tTime   0.23 (  0.56)\tData 0.0000 (0.3306)\tLoss (L1) 30.925 (13.207)\n",
      "2022-11-30 18:47:48,790 | Epoch: [13][100/297]\tTime   0.26 (  0.53)\tData 0.0000 (0.2979)\tLoss (L1) 9.554 (13.225)\n",
      "2022-11-30 18:47:51,160 | Epoch: [13][110/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2710)\tLoss (L1) 11.774 (13.348)\n",
      "2022-11-30 18:47:53,565 | Epoch: [13][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2487)\tLoss (L1) 9.469 (13.193)\n",
      "2022-11-30 18:47:55,803 | Epoch: [13][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2297)\tLoss (L1) 8.927 (13.269)\n",
      "2022-11-30 18:47:58,045 | Epoch: [13][140/297]\tTime   0.23 (  0.45)\tData 0.0000 (0.2134)\tLoss (L1) 11.010 (13.066)\n",
      "2022-11-30 18:48:00,278 | Epoch: [13][150/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.1993)\tLoss (L1) 10.052 (12.901)\n",
      "2022-11-30 18:48:02,532 | Epoch: [13][160/297]\tTime   0.24 (  0.42)\tData 0.0000 (0.1869)\tLoss (L1) 10.546 (12.886)\n",
      "2022-11-30 18:48:04,904 | Epoch: [13][170/297]\tTime   0.26 (  0.41)\tData 0.0000 (0.1760)\tLoss (L1) 11.979 (12.782)\n",
      "2022-11-30 18:48:07,235 | Epoch: [13][180/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1663)\tLoss (L1) 14.325 (12.790)\n",
      "2022-11-30 18:48:09,559 | Epoch: [13][190/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1576)\tLoss (L1) 7.759 (12.642)\n",
      "2022-11-30 18:48:11,905 | Epoch: [13][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1498)\tLoss (L1) 11.508 (12.536)\n",
      "2022-11-30 18:48:14,232 | Epoch: [13][210/297]\tTime   0.25 (  0.37)\tData 0.0010 (0.1427)\tLoss (L1) 11.543 (12.667)\n",
      "2022-11-30 18:48:16,544 | Epoch: [13][220/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1363)\tLoss (L1) 17.999 (12.790)\n",
      "2022-11-30 18:48:18,941 | Epoch: [13][230/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1304)\tLoss (L1) 8.662 (12.725)\n",
      "2022-11-30 18:48:21,347 | Epoch: [13][240/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1250)\tLoss (L1) 8.391 (12.636)\n",
      "2022-11-30 18:48:23,746 | Epoch: [13][250/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1200)\tLoss (L1) 7.679 (12.592)\n",
      "2022-11-30 18:48:26,167 | Epoch: [13][260/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1154)\tLoss (L1) 10.277 (12.726)\n",
      "2022-11-30 18:48:28,530 | Epoch: [13][270/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1112)\tLoss (L1) 14.879 (12.676)\n",
      "2022-11-30 18:48:30,842 | Epoch: [13][280/297]\tTime   0.25 (  0.34)\tData 0.0000 (0.1072)\tLoss (L1) 9.761 (12.577)\n",
      "2022-11-30 18:48:33,083 | Epoch: [13][290/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1035)\tLoss (L1) 10.583 (12.484)\n",
      "2022-11-30 18:49:04,058 | Val: [ 0/38]\tTime 29.001 (29.001)\tLoss (MSE) 169.779 (169.779)\tLoss (L1) 10.548 (10.548)\n",
      "2022-11-30 18:49:04,760 | Val: [10/38]\tTime  0.064 ( 2.700)\tLoss (MSE) 238.451 (231.215)\tLoss (L1) 12.649 (11.389)\n",
      "2022-11-30 18:49:05,407 | Val: [20/38]\tTime  0.064 ( 1.445)\tLoss (MSE) 241.136 (226.276)\tLoss (L1) 12.785 (11.386)\n",
      "2022-11-30 18:49:06,051 | Val: [30/38]\tTime  0.064 ( 1.000)\tLoss (MSE) 211.792 (226.625)\tLoss (L1) 10.685 (11.410)\n",
      "2022-11-30 18:49:07,107 |  * Overall: MSE 220.704\tL1 11.320\tG-Mean 7.020\n",
      "2022-11-30 18:49:07,107 |  * Many: MSE 214.668\tL1 11.214\tG-Mean 6.988\n",
      "2022-11-30 18:49:07,108 |  * Median: MSE 276.353\tL1 12.242\tG-Mean 7.323\n",
      "2022-11-30 18:49:07,108 |  * Low: MSE 320.005\tL1 13.791\tG-Mean 7.498\n",
      "2022-11-30 18:49:07,109 | Best L1 Loss: 11.320\n",
      "2022-11-30 18:49:07,281 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:49:07,353 | Epoch #13: Train loss [12.4534]; Val loss: MSE [220.7042], L1 [11.3200], G-Mean [7.0197]\n",
      "2022-11-30 18:49:36,338 | Epoch: [14][  0/297]\tTime  28.98 ( 28.98)\tData 28.6694 (28.6694)\tLoss (L1) 7.322 (7.322)\n",
      "2022-11-30 18:49:38,951 | Epoch: [14][ 10/297]\tTime   0.27 (  2.87)\tData 0.0010 (2.6069)\tLoss (L1) 9.835 (12.731)\n",
      "2022-11-30 18:49:41,503 | Epoch: [14][ 20/297]\tTime   0.24 (  1.63)\tData 0.0010 (1.3657)\tLoss (L1) 10.712 (13.091)\n",
      "2022-11-30 18:49:43,872 | Epoch: [14][ 30/297]\tTime   0.22 (  1.18)\tData 0.0000 (0.9252)\tLoss (L1) 11.798 (12.553)\n",
      "2022-11-30 18:49:46,103 | Epoch: [14][ 40/297]\tTime   0.22 (  0.95)\tData 0.0000 (0.6996)\tLoss (L1) 8.638 (12.577)\n",
      "2022-11-30 18:49:48,330 | Epoch: [14][ 50/297]\tTime   0.22 (  0.80)\tData 0.0000 (0.5625)\tLoss (L1) 12.555 (12.999)\n",
      "2022-11-30 18:49:50,561 | Epoch: [14][ 60/297]\tTime   0.22 (  0.71)\tData 0.0000 (0.4703)\tLoss (L1) 12.814 (13.104)\n",
      "2022-11-30 18:49:52,791 | Epoch: [14][ 70/297]\tTime   0.22 (  0.64)\tData 0.0000 (0.4041)\tLoss (L1) 10.884 (13.045)\n",
      "2022-11-30 18:49:55,023 | Epoch: [14][ 80/297]\tTime   0.22 (  0.59)\tData 0.0010 (0.3542)\tLoss (L1) 9.837 (13.370)\n",
      "2022-11-30 18:49:57,260 | Epoch: [14][ 90/297]\tTime   0.22 (  0.55)\tData 0.0000 (0.3154)\tLoss (L1) 14.911 (13.376)\n",
      "2022-11-30 18:49:59,504 | Epoch: [14][100/297]\tTime   0.23 (  0.52)\tData 0.0000 (0.2842)\tLoss (L1) 11.448 (13.181)\n",
      "2022-11-30 18:50:01,743 | Epoch: [14][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2586)\tLoss (L1) 11.018 (12.963)\n",
      "2022-11-30 18:50:03,969 | Epoch: [14][120/297]\tTime   0.22 (  0.47)\tData 0.0000 (0.2372)\tLoss (L1) 28.340 (13.179)\n",
      "2022-11-30 18:50:06,204 | Epoch: [14][130/297]\tTime   0.22 (  0.45)\tData 0.0010 (0.2191)\tLoss (L1) 13.176 (12.990)\n",
      "2022-11-30 18:50:08,433 | Epoch: [14][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2036)\tLoss (L1) 17.585 (12.823)\n",
      "2022-11-30 18:50:10,669 | Epoch: [14][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1901)\tLoss (L1) 11.491 (12.615)\n",
      "2022-11-30 18:50:12,903 | Epoch: [14][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1783)\tLoss (L1) 9.010 (12.536)\n",
      "2022-11-30 18:50:15,140 | Epoch: [14][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1679)\tLoss (L1) 9.285 (12.398)\n",
      "2022-11-30 18:50:17,371 | Epoch: [14][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1586)\tLoss (L1) 10.451 (12.306)\n",
      "2022-11-30 18:50:19,607 | Epoch: [14][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1503)\tLoss (L1) 10.603 (12.288)\n",
      "2022-11-30 18:50:21,850 | Epoch: [14][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1428)\tLoss (L1) 11.042 (12.465)\n",
      "2022-11-30 18:50:24,091 | Epoch: [14][210/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1361)\tLoss (L1) 14.971 (12.525)\n",
      "2022-11-30 18:50:26,325 | Epoch: [14][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1299)\tLoss (L1) 12.154 (12.489)\n",
      "2022-11-30 18:50:28,563 | Epoch: [14][230/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1243)\tLoss (L1) 10.774 (12.415)\n",
      "2022-11-30 18:50:30,797 | Epoch: [14][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1191)\tLoss (L1) 9.999 (12.304)\n",
      "2022-11-30 18:50:33,037 | Epoch: [14][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1144)\tLoss (L1) 9.551 (12.241)\n",
      "2022-11-30 18:50:35,274 | Epoch: [14][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1100)\tLoss (L1) 21.395 (12.202)\n",
      "2022-11-30 18:50:37,508 | Epoch: [14][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1060)\tLoss (L1) 9.552 (12.186)\n",
      "2022-11-30 18:50:39,749 | Epoch: [14][280/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1022)\tLoss (L1) 9.374 (12.175)\n",
      "2022-11-30 18:50:41,983 | Epoch: [14][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.0987)\tLoss (L1) 9.036 (12.123)\n",
      "2022-11-30 18:51:13,901 | Val: [ 0/38]\tTime 29.946 (29.946)\tLoss (MSE) 125.674 (125.674)\tLoss (L1) 9.080 (9.080)\n",
      "2022-11-30 18:51:14,778 | Val: [10/38]\tTime  0.064 ( 2.802)\tLoss (MSE) 205.458 (196.331)\tLoss (L1) 11.311 (10.440)\n",
      "2022-11-30 18:51:15,424 | Val: [20/38]\tTime  0.063 ( 1.499)\tLoss (MSE) 192.032 (181.947)\tLoss (L1) 11.469 (10.281)\n",
      "2022-11-30 18:51:16,071 | Val: [30/38]\tTime  0.064 ( 1.036)\tLoss (MSE) 196.427 (182.260)\tLoss (L1) 10.097 (10.276)\n",
      "2022-11-30 18:51:17,150 |  * Overall: MSE 178.430\tL1 10.180\tG-Mean 6.526\n",
      "2022-11-30 18:51:17,151 |  * Many: MSE 164.268\tL1 9.811\tG-Mean 6.361\n",
      "2022-11-30 18:51:17,151 |  * Median: MSE 303.129\tL1 13.352\tG-Mean 8.135\n",
      "2022-11-30 18:51:17,152 |  * Low: MSE 481.295\tL1 18.961\tG-Mean 11.945\n",
      "2022-11-30 18:51:17,153 | Best L1 Loss: 10.180\n",
      "2022-11-30 18:51:17,334 | ===> Saving current best checkpoint...\n",
      "2022-11-30 18:51:17,404 | Epoch #14: Train loss [12.1010]; Val loss: MSE [178.4304], L1 [10.1800], G-Mean [6.5260]\n",
      "2022-11-30 18:51:46,176 | Epoch: [15][  0/297]\tTime  28.77 ( 28.77)\tData 28.4395 (28.4395)\tLoss (L1) 14.723 (14.723)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:51:48,433 | Epoch: [15][ 10/297]\tTime   0.22 (  2.82)\tData 0.0000 (2.5856)\tLoss (L1) 9.209 (11.505)\n",
      "2022-11-30 18:51:50,663 | Epoch: [15][ 20/297]\tTime   0.22 (  1.58)\tData 0.0000 (1.3545)\tLoss (L1) 8.221 (11.401)\n",
      "2022-11-30 18:51:52,893 | Epoch: [15][ 30/297]\tTime   0.22 (  1.14)\tData 0.0000 (0.9176)\tLoss (L1) 10.404 (11.335)\n",
      "2022-11-30 18:51:55,126 | Epoch: [15][ 40/297]\tTime   0.21 (  0.92)\tData 0.0000 (0.6938)\tLoss (L1) 7.092 (10.933)\n",
      "2022-11-30 18:51:57,361 | Epoch: [15][ 50/297]\tTime   0.23 (  0.78)\tData 0.0000 (0.5578)\tLoss (L1) 7.868 (10.717)\n",
      "2022-11-30 18:51:59,599 | Epoch: [15][ 60/297]\tTime   0.22 (  0.69)\tData 0.0000 (0.4664)\tLoss (L1) 9.433 (10.680)\n",
      "2022-11-30 18:52:01,831 | Epoch: [15][ 70/297]\tTime   0.22 (  0.63)\tData 0.0000 (0.4007)\tLoss (L1) 15.869 (11.005)\n",
      "2022-11-30 18:52:04,105 | Epoch: [15][ 80/297]\tTime   0.22 (  0.58)\tData 0.0000 (0.3513)\tLoss (L1) 7.737 (10.838)\n",
      "2022-11-30 18:52:06,332 | Epoch: [15][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3127)\tLoss (L1) 21.438 (10.903)\n",
      "2022-11-30 18:52:08,681 | Epoch: [15][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2817)\tLoss (L1) 12.746 (10.943)\n",
      "2022-11-30 18:52:10,998 | Epoch: [15][110/297]\tTime   0.28 (  0.48)\tData 0.0010 (0.2564)\tLoss (L1) 13.569 (10.959)\n",
      "2022-11-30 18:52:13,228 | Epoch: [15][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2352)\tLoss (L1) 14.278 (11.197)\n",
      "2022-11-30 18:52:15,463 | Epoch: [15][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2173)\tLoss (L1) 9.556 (11.128)\n",
      "2022-11-30 18:52:17,694 | Epoch: [15][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2019)\tLoss (L1) 8.909 (11.327)\n",
      "2022-11-30 18:52:19,927 | Epoch: [15][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1885)\tLoss (L1) 8.928 (11.219)\n",
      "2022-11-30 18:52:22,164 | Epoch: [15][160/297]\tTime   0.23 (  0.40)\tData 0.0000 (0.1768)\tLoss (L1) 10.060 (11.442)\n",
      "2022-11-30 18:52:24,400 | Epoch: [15][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1665)\tLoss (L1) 19.853 (11.579)\n",
      "2022-11-30 18:52:26,679 | Epoch: [15][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1573)\tLoss (L1) 11.076 (11.462)\n",
      "2022-11-30 18:52:28,951 | Epoch: [15][190/297]\tTime   0.26 (  0.37)\tData 0.0000 (0.1491)\tLoss (L1) 11.527 (11.372)\n",
      "2022-11-30 18:52:31,354 | Epoch: [15][200/297]\tTime   0.25 (  0.37)\tData 0.0000 (0.1417)\tLoss (L1) 18.188 (11.526)\n",
      "2022-11-30 18:52:33,776 | Epoch: [15][210/297]\tTime   0.28 (  0.36)\tData 0.0000 (0.1350)\tLoss (L1) 11.679 (11.478)\n",
      "2022-11-30 18:52:36,011 | Epoch: [15][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1289)\tLoss (L1) 11.192 (11.564)\n",
      "2022-11-30 18:52:38,253 | Epoch: [15][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1233)\tLoss (L1) 10.573 (11.577)\n",
      "2022-11-30 18:52:40,489 | Epoch: [15][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1182)\tLoss (L1) 13.504 (11.574)\n",
      "2022-11-30 18:52:42,726 | Epoch: [15][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1135)\tLoss (L1) 10.280 (11.566)\n",
      "2022-11-30 18:52:44,967 | Epoch: [15][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1091)\tLoss (L1) 13.227 (11.531)\n",
      "2022-11-30 18:52:47,204 | Epoch: [15][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1051)\tLoss (L1) 14.887 (11.850)\n",
      "2022-11-30 18:52:49,441 | Epoch: [15][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1014)\tLoss (L1) 11.770 (12.045)\n",
      "2022-11-30 18:52:51,678 | Epoch: [15][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0979)\tLoss (L1) 13.640 (12.021)\n",
      "2022-11-30 18:53:23,622 | Val: [ 0/38]\tTime 29.925 (29.925)\tLoss (MSE) 1426.542 (1426.542)\tLoss (L1) 32.186 (32.186)\n",
      "2022-11-30 18:53:24,303 | Val: [10/38]\tTime  0.064 ( 2.782)\tLoss (MSE) 1383.882 (1226.731)\tLoss (L1) 31.938 (29.901)\n",
      "2022-11-30 18:53:24,947 | Val: [20/38]\tTime  0.063 ( 1.488)\tLoss (MSE) 1188.875 (1197.551)\tLoss (L1) 27.938 (29.205)\n",
      "2022-11-30 18:53:25,593 | Val: [30/38]\tTime  0.063 ( 1.029)\tLoss (MSE) 1017.425 (1184.710)\tLoss (L1) 26.668 (28.935)\n",
      "2022-11-30 18:53:26,650 |  * Overall: MSE 1191.619\tL1 29.020\tG-Mean 21.048\n",
      "2022-11-30 18:53:26,651 |  * Many: MSE 1233.568\tL1 29.833\tG-Mean 21.971\n",
      "2022-11-30 18:53:26,651 |  * Median: MSE 828.451\tL1 21.793\tG-Mean 14.364\n",
      "2022-11-30 18:53:26,652 |  * Low: MSE 220.506\tL1 12.519\tG-Mean 8.856\n",
      "2022-11-30 18:53:26,653 | Best L1 Loss: 10.180\n",
      "2022-11-30 18:53:26,828 | Epoch #15: Train loss [12.1092]; Val loss: MSE [1191.6190], L1 [29.0201], G-Mean [21.0479]\n",
      "2022-11-30 18:53:55,725 | Epoch: [16][  0/297]\tTime  28.89 ( 28.89)\tData 28.6992 (28.6992)\tLoss (L1) 9.229 (9.229)\n",
      "2022-11-30 18:53:58,066 | Epoch: [16][ 10/297]\tTime   0.23 (  2.84)\tData 0.0010 (2.6097)\tLoss (L1) 11.316 (10.430)\n",
      "2022-11-30 18:54:00,723 | Epoch: [16][ 20/297]\tTime   0.28 (  1.61)\tData 0.0000 (1.3670)\tLoss (L1) 11.267 (10.244)\n",
      "2022-11-30 18:54:03,223 | Epoch: [16][ 30/297]\tTime   0.22 (  1.17)\tData 0.0000 (0.9261)\tLoss (L1) 8.890 (11.057)\n",
      "2022-11-30 18:54:05,728 | Epoch: [16][ 40/297]\tTime   0.23 (  0.95)\tData 0.0000 (0.7002)\tLoss (L1) 10.874 (11.210)\n",
      "2022-11-30 18:54:08,283 | Epoch: [16][ 50/297]\tTime   0.25 (  0.81)\tData 0.0000 (0.5629)\tLoss (L1) 10.798 (11.305)\n",
      "2022-11-30 18:54:10,606 | Epoch: [16][ 60/297]\tTime   0.23 (  0.72)\tData 0.0000 (0.4707)\tLoss (L1) 11.403 (11.650)\n",
      "2022-11-30 18:54:13,087 | Epoch: [16][ 70/297]\tTime   0.24 (  0.65)\tData 0.0000 (0.4044)\tLoss (L1) 9.591 (11.828)\n",
      "2022-11-30 18:54:15,546 | Epoch: [16][ 80/297]\tTime   0.22 (  0.60)\tData 0.0000 (0.3545)\tLoss (L1) 11.384 (12.084)\n",
      "2022-11-30 18:54:18,046 | Epoch: [16][ 90/297]\tTime   0.26 (  0.56)\tData 0.0000 (0.3156)\tLoss (L1) 9.961 (12.040)\n",
      "2022-11-30 18:54:20,318 | Epoch: [16][100/297]\tTime   0.23 (  0.53)\tData 0.0000 (0.2843)\tLoss (L1) 9.621 (11.894)\n",
      "2022-11-30 18:54:22,559 | Epoch: [16][110/297]\tTime   0.21 (  0.50)\tData 0.0000 (0.2587)\tLoss (L1) 8.959 (12.041)\n",
      "2022-11-30 18:54:24,793 | Epoch: [16][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2374)\tLoss (L1) 16.634 (12.183)\n",
      "2022-11-30 18:54:27,029 | Epoch: [16][130/297]\tTime   0.23 (  0.46)\tData 0.0000 (0.2193)\tLoss (L1) 10.694 (12.158)\n",
      "2022-11-30 18:54:29,258 | Epoch: [16][140/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2037)\tLoss (L1) 11.132 (12.175)\n",
      "2022-11-30 18:54:31,496 | Epoch: [16][150/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.1903)\tLoss (L1) 10.425 (12.265)\n",
      "2022-11-30 18:54:33,728 | Epoch: [16][160/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1784)\tLoss (L1) 8.367 (12.211)\n",
      "2022-11-30 18:54:35,960 | Epoch: [16][170/297]\tTime   0.22 (  0.40)\tData 0.0010 (0.1680)\tLoss (L1) 8.159 (12.157)\n",
      "2022-11-30 18:54:38,194 | Epoch: [16][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1588)\tLoss (L1) 7.762 (12.004)\n",
      "2022-11-30 18:54:40,433 | Epoch: [16][190/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1504)\tLoss (L1) 8.897 (11.954)\n",
      "2022-11-30 18:54:42,668 | Epoch: [16][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1430)\tLoss (L1) 16.300 (11.878)\n",
      "2022-11-30 18:54:44,899 | Epoch: [16][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1362)\tLoss (L1) 13.250 (11.932)\n",
      "2022-11-30 18:54:47,137 | Epoch: [16][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1300)\tLoss (L1) 7.603 (11.808)\n",
      "2022-11-30 18:54:49,373 | Epoch: [16][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1244)\tLoss (L1) 10.629 (11.815)\n",
      "2022-11-30 18:54:51,612 | Epoch: [16][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1193)\tLoss (L1) 9.355 (11.707)\n",
      "2022-11-30 18:54:53,848 | Epoch: [16][250/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1145)\tLoss (L1) 9.764 (11.666)\n",
      "2022-11-30 18:54:56,092 | Epoch: [16][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1102)\tLoss (L1) 18.203 (11.640)\n",
      "2022-11-30 18:54:58,335 | Epoch: [16][270/297]\tTime   0.21 (  0.34)\tData 0.0010 (0.1061)\tLoss (L1) 12.306 (11.632)\n",
      "2022-11-30 18:55:00,573 | Epoch: [16][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1023)\tLoss (L1) 12.154 (11.718)\n",
      "2022-11-30 18:55:02,821 | Epoch: [16][290/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.0988)\tLoss (L1) 9.576 (11.825)\n",
      "2022-11-30 18:55:33,213 | Val: [ 0/38]\tTime 28.433 (28.433)\tLoss (MSE) 187.222 (187.222)\tLoss (L1) 11.100 (11.100)\n",
      "2022-11-30 18:55:34,224 | Val: [10/38]\tTime  0.065 ( 2.677)\tLoss (MSE) 365.820 (314.594)\tLoss (L1) 14.400 (13.094)\n",
      "2022-11-30 18:55:34,969 | Val: [20/38]\tTime  0.064 ( 1.438)\tLoss (MSE) 272.625 (294.520)\tLoss (L1) 13.986 (12.992)\n",
      "2022-11-30 18:55:35,703 | Val: [30/38]\tTime  0.072 ( 0.998)\tLoss (MSE) 295.333 (294.280)\tLoss (L1) 12.451 (12.965)\n",
      "2022-11-30 18:55:36,833 |  * Overall: MSE 291.364\tL1 12.930\tG-Mean 8.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:55:36,833 |  * Many: MSE 251.005\tL1 12.173\tG-Mean 7.750\n",
      "2022-11-30 18:55:36,834 |  * Median: MSE 670.264\tL1 19.878\tG-Mean 12.492\n",
      "2022-11-30 18:55:36,834 |  * Low: MSE 873.420\tL1 25.801\tG-Mean 19.450\n",
      "2022-11-30 18:55:36,836 | Best L1 Loss: 10.180\n",
      "2022-11-30 18:55:37,015 | Epoch #16: Train loss [11.8425]; Val loss: MSE [291.3638], L1 [12.9300], G-Mean [8.1264]\n",
      "2022-11-30 18:56:07,951 | Epoch: [17][  0/297]\tTime  30.93 ( 30.93)\tData 30.7209 (30.7209)\tLoss (L1) 7.372 (7.372)\n",
      "2022-11-30 18:56:10,386 | Epoch: [17][ 10/297]\tTime   0.22 (  3.03)\tData 0.0010 (2.7931)\tLoss (L1) 9.192 (9.485)\n",
      "2022-11-30 18:56:12,740 | Epoch: [17][ 20/297]\tTime   0.25 (  1.70)\tData 0.0010 (1.4631)\tLoss (L1) 11.832 (9.542)\n",
      "2022-11-30 18:56:15,130 | Epoch: [17][ 30/297]\tTime   0.23 (  1.23)\tData 0.0000 (0.9912)\tLoss (L1) 11.785 (9.956)\n",
      "2022-11-30 18:56:17,386 | Epoch: [17][ 40/297]\tTime   0.23 (  0.98)\tData 0.0010 (0.7495)\tLoss (L1) 7.365 (10.465)\n",
      "2022-11-30 18:56:19,646 | Epoch: [17][ 50/297]\tTime   0.22 (  0.84)\tData 0.0000 (0.6026)\tLoss (L1) 8.987 (10.923)\n",
      "2022-11-30 18:56:21,959 | Epoch: [17][ 60/297]\tTime   0.22 (  0.74)\tData 0.0000 (0.5038)\tLoss (L1) 6.745 (11.288)\n",
      "2022-11-30 18:56:24,188 | Epoch: [17][ 70/297]\tTime   0.22 (  0.66)\tData 0.0000 (0.4329)\tLoss (L1) 12.885 (11.169)\n",
      "2022-11-30 18:56:26,422 | Epoch: [17][ 80/297]\tTime   0.22 (  0.61)\tData 0.0000 (0.3795)\tLoss (L1) 11.669 (11.394)\n",
      "2022-11-30 18:56:28,654 | Epoch: [17][ 90/297]\tTime   0.22 (  0.57)\tData 0.0000 (0.3378)\tLoss (L1) 15.547 (11.564)\n",
      "2022-11-30 18:56:30,887 | Epoch: [17][100/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.3043)\tLoss (L1) 20.129 (11.553)\n",
      "2022-11-30 18:56:33,119 | Epoch: [17][110/297]\tTime   0.22 (  0.51)\tData 0.0010 (0.2769)\tLoss (L1) 11.525 (11.615)\n",
      "2022-11-30 18:56:35,511 | Epoch: [17][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2541)\tLoss (L1) 8.001 (11.453)\n",
      "2022-11-30 18:56:37,885 | Epoch: [17][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2347)\tLoss (L1) 34.082 (11.583)\n",
      "2022-11-30 18:56:40,248 | Epoch: [17][140/297]\tTime   0.25 (  0.45)\tData 0.0000 (0.2181)\tLoss (L1) 13.370 (11.619)\n",
      "2022-11-30 18:56:42,566 | Epoch: [17][150/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2036)\tLoss (L1) 11.600 (11.514)\n",
      "2022-11-30 18:56:44,885 | Epoch: [17][160/297]\tTime   0.23 (  0.42)\tData 0.0000 (0.1910)\tLoss (L1) 8.997 (11.398)\n",
      "2022-11-30 18:56:47,115 | Epoch: [17][170/297]\tTime   0.23 (  0.41)\tData 0.0000 (0.1799)\tLoss (L1) 10.779 (11.330)\n",
      "2022-11-30 18:56:49,346 | Epoch: [17][180/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1699)\tLoss (L1) 7.829 (11.273)\n",
      "2022-11-30 18:56:51,580 | Epoch: [17][190/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1610)\tLoss (L1) 14.103 (11.254)\n",
      "2022-11-30 18:56:53,818 | Epoch: [17][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1530)\tLoss (L1) 19.787 (11.333)\n",
      "2022-11-30 18:56:56,065 | Epoch: [17][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1458)\tLoss (L1) 10.340 (11.332)\n",
      "2022-11-30 18:56:58,319 | Epoch: [17][220/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1392)\tLoss (L1) 9.210 (11.309)\n",
      "2022-11-30 18:57:00,554 | Epoch: [17][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1332)\tLoss (L1) 14.584 (11.576)\n",
      "2022-11-30 18:57:02,793 | Epoch: [17][240/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1277)\tLoss (L1) 14.551 (11.662)\n",
      "2022-11-30 18:57:05,032 | Epoch: [17][250/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1226)\tLoss (L1) 10.687 (11.632)\n",
      "2022-11-30 18:57:07,273 | Epoch: [17][260/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1179)\tLoss (L1) 11.705 (11.775)\n",
      "2022-11-30 18:57:09,512 | Epoch: [17][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1136)\tLoss (L1) 13.282 (11.844)\n",
      "2022-11-30 18:57:11,748 | Epoch: [17][280/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1095)\tLoss (L1) 9.960 (11.927)\n",
      "2022-11-30 18:57:13,979 | Epoch: [17][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1058)\tLoss (L1) 9.497 (12.057)\n",
      "2022-11-30 18:57:43,714 | Val: [ 0/38]\tTime 27.764 (27.764)\tLoss (MSE) 398.219 (398.219)\tLoss (L1) 17.437 (17.437)\n",
      "2022-11-30 18:57:44,419 | Val: [10/38]\tTime  0.065 ( 2.588)\tLoss (MSE) 547.319 (451.109)\tLoss (L1) 20.023 (17.769)\n",
      "2022-11-30 18:57:45,064 | Val: [20/38]\tTime  0.065 ( 1.386)\tLoss (MSE) 517.584 (454.939)\tLoss (L1) 18.703 (17.836)\n",
      "2022-11-30 18:57:45,710 | Val: [30/38]\tTime  0.065 ( 0.960)\tLoss (MSE) 521.054 (448.966)\tLoss (L1) 18.914 (17.748)\n",
      "2022-11-30 18:57:46,761 |  * Overall: MSE 445.867\tL1 17.726\tG-Mean 13.083\n",
      "2022-11-30 18:57:46,761 |  * Many: MSE 456.120\tL1 18.077\tG-Mean 13.548\n",
      "2022-11-30 18:57:46,761 |  * Median: MSE 366.334\tL1 14.764\tG-Mean 9.706\n",
      "2022-11-30 18:57:46,762 |  * Low: MSE 98.283\tL1 8.637\tG-Mean 5.565\n",
      "2022-11-30 18:57:46,763 | Best L1 Loss: 10.180\n",
      "2022-11-30 18:57:46,938 | Epoch #17: Train loss [12.0029]; Val loss: MSE [445.8671], L1 [17.7256], G-Mean [13.0830]\n",
      "2022-11-30 18:58:18,685 | Epoch: [18][  0/297]\tTime  31.75 ( 31.75)\tData 31.4852 (31.4852)\tLoss (L1) 9.696 (9.696)\n",
      "2022-11-30 18:58:20,911 | Epoch: [18][ 10/297]\tTime   0.22 (  3.09)\tData 0.0000 (2.8626)\tLoss (L1) 9.538 (12.022)\n",
      "2022-11-30 18:58:23,141 | Epoch: [18][ 20/297]\tTime   0.22 (  1.72)\tData 0.0000 (1.4995)\tLoss (L1) 9.871 (11.344)\n",
      "2022-11-30 18:58:25,375 | Epoch: [18][ 30/297]\tTime   0.22 (  1.24)\tData 0.0000 (1.0158)\tLoss (L1) 8.295 (11.865)\n",
      "2022-11-30 18:58:27,602 | Epoch: [18][ 40/297]\tTime   0.22 (  0.99)\tData 0.0000 (0.7681)\tLoss (L1) 11.375 (11.491)\n",
      "2022-11-30 18:58:29,833 | Epoch: [18][ 50/297]\tTime   0.22 (  0.84)\tData 0.0000 (0.6175)\tLoss (L1) 12.064 (11.269)\n",
      "2022-11-30 18:58:32,161 | Epoch: [18][ 60/297]\tTime   0.22 (  0.74)\tData 0.0000 (0.5162)\tLoss (L1) 7.340 (11.030)\n",
      "2022-11-30 18:58:34,485 | Epoch: [18][ 70/297]\tTime   0.29 (  0.67)\tData 0.0000 (0.4436)\tLoss (L1) 36.218 (11.498)\n",
      "2022-11-30 18:58:36,828 | Epoch: [18][ 80/297]\tTime   0.22 (  0.62)\tData 0.0010 (0.3888)\tLoss (L1) 25.966 (11.961)\n",
      "2022-11-30 18:58:39,060 | Epoch: [18][ 90/297]\tTime   0.22 (  0.57)\tData 0.0010 (0.3461)\tLoss (L1) 9.626 (11.926)\n",
      "2022-11-30 18:58:41,291 | Epoch: [18][100/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3119)\tLoss (L1) 10.351 (11.970)\n",
      "2022-11-30 18:58:43,525 | Epoch: [18][110/297]\tTime   0.23 (  0.51)\tData 0.0000 (0.2838)\tLoss (L1) 8.596 (11.863)\n",
      "2022-11-30 18:58:45,755 | Epoch: [18][120/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2603)\tLoss (L1) 9.242 (11.781)\n",
      "2022-11-30 18:58:47,989 | Epoch: [18][130/297]\tTime   0.22 (  0.47)\tData 0.0000 (0.2405)\tLoss (L1) 9.592 (11.867)\n",
      "2022-11-30 18:58:50,220 | Epoch: [18][140/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2234)\tLoss (L1) 13.550 (11.785)\n",
      "2022-11-30 18:58:52,455 | Epoch: [18][150/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2087)\tLoss (L1) 9.022 (11.853)\n",
      "2022-11-30 18:58:54,697 | Epoch: [18][160/297]\tTime   0.22 (  0.42)\tData 0.0010 (0.1957)\tLoss (L1) 9.436 (11.692)\n",
      "2022-11-30 18:58:56,945 | Epoch: [18][170/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1843)\tLoss (L1) 8.113 (11.630)\n",
      "2022-11-30 18:58:59,192 | Epoch: [18][180/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1741)\tLoss (L1) 7.575 (11.688)\n",
      "2022-11-30 18:59:01,431 | Epoch: [18][190/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1650)\tLoss (L1) 9.588 (11.631)\n",
      "2022-11-30 18:59:03,662 | Epoch: [18][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1568)\tLoss (L1) 22.831 (11.689)\n",
      "2022-11-30 18:59:05,915 | Epoch: [18][210/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1494)\tLoss (L1) 10.551 (11.675)\n",
      "2022-11-30 18:59:08,147 | Epoch: [18][220/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1426)\tLoss (L1) 11.136 (11.631)\n",
      "2022-11-30 18:59:10,386 | Epoch: [18][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1365)\tLoss (L1) 18.000 (11.779)\n",
      "2022-11-30 18:59:12,618 | Epoch: [18][240/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1308)\tLoss (L1) 12.508 (11.718)\n",
      "2022-11-30 18:59:14,877 | Epoch: [18][250/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1256)\tLoss (L1) 9.470 (11.713)\n",
      "2022-11-30 18:59:17,138 | Epoch: [18][260/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1208)\tLoss (L1) 9.695 (11.712)\n",
      "2022-11-30 18:59:19,380 | Epoch: [18][270/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1163)\tLoss (L1) 9.429 (11.636)\n",
      "2022-11-30 18:59:21,617 | Epoch: [18][280/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1122)\tLoss (L1) 16.839 (11.581)\n",
      "2022-11-30 18:59:23,851 | Epoch: [18][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1084)\tLoss (L1) 11.403 (11.558)\n",
      "2022-11-30 18:59:53,788 | Val: [ 0/38]\tTime 27.873 (27.873)\tLoss (MSE) 168.412 (168.412)\tLoss (L1) 10.107 (10.107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 18:59:54,472 | Val: [10/38]\tTime  0.065 ( 2.596)\tLoss (MSE) 220.741 (210.431)\tLoss (L1) 11.684 (10.798)\n",
      "2022-11-30 18:59:55,126 | Val: [20/38]\tTime  0.066 ( 1.391)\tLoss (MSE) 234.000 (209.902)\tLoss (L1) 11.094 (10.916)\n",
      "2022-11-30 18:59:55,781 | Val: [30/38]\tTime  0.065 ( 0.963)\tLoss (MSE) 162.800 (204.407)\tLoss (L1) 9.074 (10.738)\n",
      "2022-11-30 18:59:56,968 |  * Overall: MSE 199.538\tL1 10.638\tG-Mean 6.676\n",
      "2022-11-30 18:59:56,968 |  * Many: MSE 197.827\tL1 10.581\tG-Mean 6.604\n",
      "2022-11-30 18:59:56,969 |  * Median: MSE 217.744\tL1 11.166\tG-Mean 7.345\n",
      "2022-11-30 18:59:56,970 |  * Low: MSE 198.687\tL1 11.465\tG-Mean 8.500\n",
      "2022-11-30 18:59:56,972 | Best L1 Loss: 10.180\n",
      "2022-11-30 18:59:57,197 | Epoch #18: Train loss [11.5572]; Val loss: MSE [199.5384], L1 [10.6377], G-Mean [6.6764]\n",
      "2022-11-30 19:00:28,269 | Epoch: [19][  0/297]\tTime  31.07 ( 31.07)\tData 30.7664 (30.7664)\tLoss (L1) 10.162 (10.162)\n",
      "2022-11-30 19:00:30,513 | Epoch: [19][ 10/297]\tTime   0.22 (  3.03)\tData 0.0010 (2.7974)\tLoss (L1) 22.726 (10.904)\n",
      "2022-11-30 19:00:32,742 | Epoch: [19][ 20/297]\tTime   0.22 (  1.69)\tData 0.0000 (1.4654)\tLoss (L1) 8.237 (10.425)\n",
      "2022-11-30 19:00:34,970 | Epoch: [19][ 30/297]\tTime   0.22 (  1.22)\tData 0.0000 (0.9927)\tLoss (L1) 10.740 (11.236)\n",
      "2022-11-30 19:00:37,200 | Epoch: [19][ 40/297]\tTime   0.22 (  0.98)\tData 0.0000 (0.7506)\tLoss (L1) 9.823 (11.877)\n",
      "2022-11-30 19:00:39,428 | Epoch: [19][ 50/297]\tTime   0.22 (  0.83)\tData 0.0000 (0.6035)\tLoss (L1) 23.485 (12.155)\n",
      "2022-11-30 19:00:41,658 | Epoch: [19][ 60/297]\tTime   0.22 (  0.73)\tData 0.0000 (0.5046)\tLoss (L1) 11.993 (12.346)\n",
      "2022-11-30 19:00:43,886 | Epoch: [19][ 70/297]\tTime   0.22 (  0.66)\tData 0.0000 (0.4335)\tLoss (L1) 9.648 (12.045)\n",
      "2022-11-30 19:00:46,117 | Epoch: [19][ 80/297]\tTime   0.22 (  0.60)\tData 0.0000 (0.3800)\tLoss (L1) 24.594 (11.956)\n",
      "2022-11-30 19:00:48,348 | Epoch: [19][ 90/297]\tTime   0.22 (  0.56)\tData 0.0000 (0.3383)\tLoss (L1) 10.459 (12.228)\n",
      "2022-11-30 19:00:50,577 | Epoch: [19][100/297]\tTime   0.23 (  0.53)\tData 0.0000 (0.3048)\tLoss (L1) 14.708 (12.151)\n",
      "2022-11-30 19:00:52,807 | Epoch: [19][110/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2774)\tLoss (L1) 10.011 (12.105)\n",
      "2022-11-30 19:00:55,056 | Epoch: [19][120/297]\tTime   0.22 (  0.48)\tData 0.0010 (0.2545)\tLoss (L1) 7.119 (12.131)\n",
      "2022-11-30 19:00:57,306 | Epoch: [19][130/297]\tTime   0.22 (  0.46)\tData 0.0010 (0.2351)\tLoss (L1) 10.747 (12.100)\n",
      "2022-11-30 19:00:59,544 | Epoch: [19][140/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2184)\tLoss (L1) 7.273 (12.011)\n",
      "2022-11-30 19:01:01,784 | Epoch: [19][150/297]\tTime   0.23 (  0.43)\tData 0.0010 (0.2039)\tLoss (L1) 8.547 (11.883)\n",
      "2022-11-30 19:01:04,022 | Epoch: [19][160/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1913)\tLoss (L1) 8.651 (11.901)\n",
      "2022-11-30 19:01:06,261 | Epoch: [19][170/297]\tTime   0.23 (  0.40)\tData 0.0000 (0.1801)\tLoss (L1) 9.265 (11.987)\n",
      "2022-11-30 19:01:08,490 | Epoch: [19][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1702)\tLoss (L1) 7.923 (11.949)\n",
      "2022-11-30 19:01:10,722 | Epoch: [19][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1613)\tLoss (L1) 17.689 (11.870)\n",
      "2022-11-30 19:01:12,955 | Epoch: [19][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1533)\tLoss (L1) 7.638 (11.744)\n",
      "2022-11-30 19:01:15,224 | Epoch: [19][210/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1460)\tLoss (L1) 11.451 (11.671)\n",
      "2022-11-30 19:01:17,489 | Epoch: [19][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1394)\tLoss (L1) 13.309 (11.552)\n",
      "2022-11-30 19:01:19,723 | Epoch: [19][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1334)\tLoss (L1) 7.071 (11.440)\n",
      "2022-11-30 19:01:21,961 | Epoch: [19][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1278)\tLoss (L1) 11.051 (11.405)\n",
      "2022-11-30 19:01:24,200 | Epoch: [19][250/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1228)\tLoss (L1) 9.268 (11.421)\n",
      "2022-11-30 19:01:26,437 | Epoch: [19][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1181)\tLoss (L1) 11.943 (11.408)\n",
      "2022-11-30 19:01:28,674 | Epoch: [19][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1137)\tLoss (L1) 25.132 (11.479)\n",
      "2022-11-30 19:01:30,908 | Epoch: [19][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1097)\tLoss (L1) 18.601 (11.443)\n",
      "2022-11-30 19:01:33,146 | Epoch: [19][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1059)\tLoss (L1) 11.702 (11.426)\n",
      "2022-11-30 19:02:03,996 | Val: [ 0/38]\tTime 28.907 (28.907)\tLoss (MSE) 231.650 (231.650)\tLoss (L1) 12.274 (12.274)\n",
      "2022-11-30 19:02:04,683 | Val: [10/38]\tTime  0.065 ( 2.690)\tLoss (MSE) 367.601 (281.393)\tLoss (L1) 14.890 (12.872)\n",
      "2022-11-30 19:02:05,332 | Val: [20/38]\tTime  0.063 ( 1.440)\tLoss (MSE) 340.447 (280.575)\tLoss (L1) 14.210 (12.921)\n",
      "2022-11-30 19:02:05,975 | Val: [30/38]\tTime  0.064 ( 0.996)\tLoss (MSE) 237.777 (275.752)\tLoss (L1) 11.594 (12.777)\n",
      "2022-11-30 19:02:07,041 |  * Overall: MSE 268.816\tL1 12.670\tG-Mean 8.035\n",
      "2022-11-30 19:02:07,041 |  * Many: MSE 272.492\tL1 12.783\tG-Mean 8.168\n",
      "2022-11-30 19:02:07,042 |  * Median: MSE 234.573\tL1 11.549\tG-Mean 6.754\n",
      "2022-11-30 19:02:07,042 |  * Low: MSE 212.570\tL1 11.779\tG-Mean 8.042\n",
      "2022-11-30 19:02:07,043 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:02:07,213 | Epoch #19: Train loss [11.3830]; Val loss: MSE [268.8155], L1 [12.6704], G-Mean [8.0351]\n",
      "2022-11-30 19:02:35,463 | Epoch: [20][  0/297]\tTime  28.25 ( 28.25)\tData 27.9652 (27.9652)\tLoss (L1) 13.480 (13.480)\n",
      "2022-11-30 19:02:37,712 | Epoch: [20][ 10/297]\tTime   0.22 (  2.77)\tData 0.0000 (2.5427)\tLoss (L1) 11.196 (12.248)\n",
      "2022-11-30 19:02:39,944 | Epoch: [20][ 20/297]\tTime   0.23 (  1.56)\tData 0.0000 (1.3320)\tLoss (L1) 15.463 (11.771)\n",
      "2022-11-30 19:02:42,172 | Epoch: [20][ 30/297]\tTime   0.22 (  1.13)\tData 0.0000 (0.9024)\tLoss (L1) 10.018 (11.510)\n",
      "2022-11-30 19:02:44,399 | Epoch: [20][ 40/297]\tTime   0.22 (  0.91)\tData 0.0000 (0.6824)\tLoss (L1) 12.570 (11.064)\n",
      "2022-11-30 19:02:46,626 | Epoch: [20][ 50/297]\tTime   0.22 (  0.77)\tData 0.0000 (0.5486)\tLoss (L1) 8.036 (10.885)\n",
      "2022-11-30 19:02:48,857 | Epoch: [20][ 60/297]\tTime   0.23 (  0.68)\tData 0.0000 (0.4586)\tLoss (L1) 6.619 (10.816)\n",
      "2022-11-30 19:02:51,089 | Epoch: [20][ 70/297]\tTime   0.22 (  0.62)\tData 0.0000 (0.3941)\tLoss (L1) 8.096 (11.020)\n",
      "2022-11-30 19:02:53,321 | Epoch: [20][ 80/297]\tTime   0.22 (  0.57)\tData 0.0000 (0.3454)\tLoss (L1) 9.843 (11.650)\n",
      "2022-11-30 19:02:55,568 | Epoch: [20][ 90/297]\tTime   0.23 (  0.53)\tData 0.0010 (0.3075)\tLoss (L1) 8.690 (11.673)\n",
      "2022-11-30 19:02:57,788 | Epoch: [20][100/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2771)\tLoss (L1) 10.567 (11.582)\n",
      "2022-11-30 19:03:00,029 | Epoch: [20][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2521)\tLoss (L1) 8.586 (11.553)\n",
      "2022-11-30 19:03:02,262 | Epoch: [20][120/297]\tTime   0.22 (  0.45)\tData 0.0010 (0.2313)\tLoss (L1) 10.451 (11.568)\n",
      "2022-11-30 19:03:04,497 | Epoch: [20][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2137)\tLoss (L1) 8.388 (11.437)\n",
      "2022-11-30 19:03:06,733 | Epoch: [20][140/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1985)\tLoss (L1) 11.716 (11.412)\n",
      "2022-11-30 19:03:08,969 | Epoch: [20][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1854)\tLoss (L1) 10.533 (11.377)\n",
      "2022-11-30 19:03:11,203 | Epoch: [20][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1739)\tLoss (L1) 7.746 (11.302)\n",
      "2022-11-30 19:03:13,438 | Epoch: [20][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1638)\tLoss (L1) 12.258 (11.184)\n",
      "2022-11-30 19:03:15,698 | Epoch: [20][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1547)\tLoss (L1) 10.676 (11.158)\n",
      "2022-11-30 19:03:17,951 | Epoch: [20][190/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1466)\tLoss (L1) 8.747 (11.061)\n",
      "2022-11-30 19:03:20,185 | Epoch: [20][200/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1393)\tLoss (L1) 12.431 (10.993)\n",
      "2022-11-30 19:03:22,419 | Epoch: [20][210/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1327)\tLoss (L1) 9.805 (10.933)\n",
      "2022-11-30 19:03:24,656 | Epoch: [20][220/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1267)\tLoss (L1) 11.382 (10.914)\n",
      "2022-11-30 19:03:26,891 | Epoch: [20][230/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1212)\tLoss (L1) 20.459 (10.903)\n",
      "2022-11-30 19:03:29,128 | Epoch: [20][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1162)\tLoss (L1) 7.291 (10.820)\n",
      "2022-11-30 19:03:31,365 | Epoch: [20][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1116)\tLoss (L1) 7.622 (10.705)\n",
      "2022-11-30 19:03:33,600 | Epoch: [20][260/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1073)\tLoss (L1) 8.852 (10.937)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:03:35,834 | Epoch: [20][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1034)\tLoss (L1) 12.302 (10.943)\n",
      "2022-11-30 19:03:38,070 | Epoch: [20][280/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0997)\tLoss (L1) 9.841 (11.006)\n",
      "2022-11-30 19:03:40,403 | Epoch: [20][290/297]\tTime   0.30 (  0.32)\tData 0.0000 (0.0963)\tLoss (L1) 9.078 (11.105)\n",
      "2022-11-30 19:04:11,716 | Val: [ 0/38]\tTime 29.201 (29.201)\tLoss (MSE) 296.440 (296.440)\tLoss (L1) 14.448 (14.448)\n",
      "2022-11-30 19:04:12,415 | Val: [10/38]\tTime  0.063 ( 2.718)\tLoss (MSE) 446.120 (352.405)\tLoss (L1) 15.906 (14.571)\n",
      "2022-11-30 19:04:13,061 | Val: [20/38]\tTime  0.065 ( 1.455)\tLoss (MSE) 525.347 (345.750)\tLoss (L1) 17.390 (14.446)\n",
      "2022-11-30 19:04:13,704 | Val: [30/38]\tTime  0.063 ( 1.006)\tLoss (MSE) 293.975 (343.783)\tLoss (L1) 12.926 (14.333)\n",
      "2022-11-30 19:04:14,766 |  * Overall: MSE 339.891\tL1 14.200\tG-Mean 9.179\n",
      "2022-11-30 19:04:14,766 |  * Many: MSE 337.766\tL1 14.176\tG-Mean 9.208\n",
      "2022-11-30 19:04:14,767 |  * Median: MSE 356.602\tL1 14.249\tG-Mean 8.675\n",
      "2022-11-30 19:04:14,767 |  * Low: MSE 409.237\tL1 16.701\tG-Mean 12.120\n",
      "2022-11-30 19:04:14,768 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:04:14,947 | Epoch #20: Train loss [11.1886]; Val loss: MSE [339.8910], L1 [14.2001], G-Mean [9.1791]\n",
      "2022-11-30 19:04:43,151 | Epoch: [21][  0/297]\tTime  28.20 ( 28.20)\tData 27.8812 (27.8812)\tLoss (L1) 8.406 (8.406)\n",
      "2022-11-30 19:04:45,487 | Epoch: [21][ 10/297]\tTime   0.23 (  2.78)\tData 0.0000 (2.5349)\tLoss (L1) 9.825 (12.199)\n",
      "2022-11-30 19:04:47,815 | Epoch: [21][ 20/297]\tTime   0.23 (  1.57)\tData 0.0000 (1.3279)\tLoss (L1) 9.736 (11.461)\n",
      "2022-11-30 19:04:50,137 | Epoch: [21][ 30/297]\tTime   0.23 (  1.14)\tData 0.0000 (0.8996)\tLoss (L1) 17.828 (11.125)\n",
      "2022-11-30 19:04:52,461 | Epoch: [21][ 40/297]\tTime   0.23 (  0.91)\tData 0.0000 (0.6802)\tLoss (L1) 9.575 (10.883)\n",
      "2022-11-30 19:04:54,795 | Epoch: [21][ 50/297]\tTime   0.23 (  0.78)\tData 0.0000 (0.5469)\tLoss (L1) 8.048 (10.598)\n",
      "2022-11-30 19:04:57,125 | Epoch: [21][ 60/297]\tTime   0.24 (  0.69)\tData 0.0010 (0.4573)\tLoss (L1) 24.928 (10.911)\n",
      "2022-11-30 19:04:59,454 | Epoch: [21][ 70/297]\tTime   0.23 (  0.63)\tData 0.0000 (0.3930)\tLoss (L1) 6.678 (10.722)\n",
      "2022-11-30 19:05:01,778 | Epoch: [21][ 80/297]\tTime   0.23 (  0.58)\tData 0.0000 (0.3445)\tLoss (L1) 23.605 (11.043)\n",
      "2022-11-30 19:05:04,012 | Epoch: [21][ 90/297]\tTime   0.22 (  0.54)\tData 0.0000 (0.3066)\tLoss (L1) 15.423 (10.868)\n",
      "2022-11-30 19:05:06,248 | Epoch: [21][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2763)\tLoss (L1) 9.306 (10.973)\n",
      "2022-11-30 19:05:08,482 | Epoch: [21][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2514)\tLoss (L1) 10.849 (10.940)\n",
      "2022-11-30 19:05:10,716 | Epoch: [21][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2307)\tLoss (L1) 34.938 (11.001)\n",
      "2022-11-30 19:05:12,948 | Epoch: [21][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2131)\tLoss (L1) 9.149 (11.021)\n",
      "2022-11-30 19:05:15,209 | Epoch: [21][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.1980)\tLoss (L1) 10.332 (11.067)\n",
      "2022-11-30 19:05:17,464 | Epoch: [21][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1849)\tLoss (L1) 7.403 (11.179)\n",
      "2022-11-30 19:05:19,697 | Epoch: [21][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1734)\tLoss (L1) 9.244 (11.173)\n",
      "2022-11-30 19:05:21,936 | Epoch: [21][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1633)\tLoss (L1) 9.669 (11.082)\n",
      "2022-11-30 19:05:24,172 | Epoch: [21][180/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1542)\tLoss (L1) 9.080 (11.109)\n",
      "2022-11-30 19:05:26,406 | Epoch: [21][190/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1462)\tLoss (L1) 9.269 (11.215)\n",
      "2022-11-30 19:05:28,643 | Epoch: [21][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1389)\tLoss (L1) 11.875 (11.206)\n",
      "2022-11-30 19:05:30,870 | Epoch: [21][210/297]\tTime   0.22 (  0.36)\tData 0.0010 (0.1323)\tLoss (L1) 9.542 (11.228)\n",
      "2022-11-30 19:05:33,405 | Epoch: [21][220/297]\tTime   0.28 (  0.36)\tData 0.0000 (0.1264)\tLoss (L1) 10.924 (11.158)\n",
      "2022-11-30 19:05:35,782 | Epoch: [21][230/297]\tTime   0.23 (  0.35)\tData 0.0010 (0.1209)\tLoss (L1) 13.263 (11.122)\n",
      "2022-11-30 19:05:38,118 | Epoch: [21][240/297]\tTime   0.24 (  0.35)\tData 0.0000 (0.1159)\tLoss (L1) 14.860 (11.096)\n",
      "2022-11-30 19:05:40,450 | Epoch: [21][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1113)\tLoss (L1) 29.754 (11.203)\n",
      "2022-11-30 19:05:42,782 | Epoch: [21][260/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1070)\tLoss (L1) 9.444 (11.163)\n",
      "2022-11-30 19:05:45,050 | Epoch: [21][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1031)\tLoss (L1) 11.578 (11.139)\n",
      "2022-11-30 19:05:47,286 | Epoch: [21][280/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.0994)\tLoss (L1) 10.649 (11.050)\n",
      "2022-11-30 19:05:49,517 | Epoch: [21][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0960)\tLoss (L1) 12.421 (11.006)\n",
      "2022-11-30 19:06:19,717 | Val: [ 0/38]\tTime 28.236 (28.236)\tLoss (MSE) 2327.750 (2327.750)\tLoss (L1) 41.165 (41.165)\n",
      "2022-11-30 19:06:20,429 | Val: [10/38]\tTime  0.064 ( 2.632)\tLoss (MSE) 2119.324 (2042.213)\tLoss (L1) 39.608 (38.705)\n",
      "2022-11-30 19:06:21,078 | Val: [20/38]\tTime  0.064 ( 1.409)\tLoss (MSE) 1964.643 (2001.289)\tLoss (L1) 36.678 (38.022)\n",
      "2022-11-30 19:06:21,721 | Val: [30/38]\tTime  0.063 ( 0.975)\tLoss (MSE) 1677.866 (1965.159)\tLoss (L1) 35.289 (37.527)\n",
      "2022-11-30 19:06:22,781 |  * Overall: MSE 1985.451\tL1 37.817\tG-Mean 27.657\n",
      "2022-11-30 19:06:22,782 |  * Many: MSE 2037.701\tL1 38.475\tG-Mean 28.270\n",
      "2022-11-30 19:06:22,782 |  * Median: MSE 1562.701\tL1 32.533\tG-Mean 23.279\n",
      "2022-11-30 19:06:22,783 |  * Low: MSE 422.332\tL1 17.662\tG-Mean 13.539\n",
      "2022-11-30 19:06:22,784 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:06:22,968 | Epoch #21: Train loss [10.9960]; Val loss: MSE [1985.4508], L1 [37.8174], G-Mean [27.6570]\n",
      "2022-11-30 19:06:51,297 | Epoch: [22][  0/297]\tTime  28.33 ( 28.33)\tData 28.0545 (28.0545)\tLoss (L1) 13.687 (13.687)\n",
      "2022-11-30 19:06:53,543 | Epoch: [22][ 10/297]\tTime   0.22 (  2.78)\tData 0.0000 (2.5509)\tLoss (L1) 15.308 (12.271)\n",
      "2022-11-30 19:06:55,776 | Epoch: [22][ 20/297]\tTime   0.23 (  1.56)\tData 0.0000 (1.3363)\tLoss (L1) 12.521 (11.521)\n",
      "2022-11-30 19:06:58,007 | Epoch: [22][ 30/297]\tTime   0.22 (  1.13)\tData 0.0010 (0.9053)\tLoss (L1) 9.791 (10.836)\n",
      "2022-11-30 19:07:00,238 | Epoch: [22][ 40/297]\tTime   0.22 (  0.91)\tData 0.0000 (0.6845)\tLoss (L1) 12.988 (10.770)\n",
      "2022-11-30 19:07:02,470 | Epoch: [22][ 50/297]\tTime   0.22 (  0.77)\tData 0.0000 (0.5503)\tLoss (L1) 13.516 (10.911)\n",
      "2022-11-30 19:07:04,698 | Epoch: [22][ 60/297]\tTime   0.23 (  0.68)\tData 0.0000 (0.4601)\tLoss (L1) 10.835 (11.067)\n",
      "2022-11-30 19:07:06,930 | Epoch: [22][ 70/297]\tTime   0.22 (  0.62)\tData 0.0000 (0.3953)\tLoss (L1) 15.001 (11.184)\n",
      "2022-11-30 19:07:09,161 | Epoch: [22][ 80/297]\tTime   0.22 (  0.57)\tData 0.0000 (0.3465)\tLoss (L1) 10.166 (11.015)\n",
      "2022-11-30 19:07:11,391 | Epoch: [22][ 90/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.3085)\tLoss (L1) 8.936 (11.142)\n",
      "2022-11-30 19:07:13,624 | Epoch: [22][100/297]\tTime   0.23 (  0.50)\tData 0.0010 (0.2780)\tLoss (L1) 6.609 (11.221)\n",
      "2022-11-30 19:07:15,877 | Epoch: [22][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2529)\tLoss (L1) 11.781 (11.150)\n",
      "2022-11-30 19:07:18,139 | Epoch: [22][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2321)\tLoss (L1) 6.902 (11.138)\n",
      "2022-11-30 19:07:20,371 | Epoch: [22][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2144)\tLoss (L1) 6.574 (10.990)\n",
      "2022-11-30 19:07:22,606 | Epoch: [22][140/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1992)\tLoss (L1) 10.816 (11.012)\n",
      "2022-11-30 19:07:24,835 | Epoch: [22][150/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1860)\tLoss (L1) 17.735 (10.982)\n",
      "2022-11-30 19:07:27,070 | Epoch: [22][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1744)\tLoss (L1) 8.967 (10.927)\n",
      "2022-11-30 19:07:29,307 | Epoch: [22][170/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1643)\tLoss (L1) 7.833 (10.901)\n",
      "2022-11-30 19:07:31,540 | Epoch: [22][180/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1552)\tLoss (L1) 10.119 (10.779)\n",
      "2022-11-30 19:07:33,775 | Epoch: [22][190/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1471)\tLoss (L1) 14.139 (10.730)\n",
      "2022-11-30 19:07:36,012 | Epoch: [22][200/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1398)\tLoss (L1) 7.289 (10.892)\n",
      "2022-11-30 19:07:38,247 | Epoch: [22][210/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1331)\tLoss (L1) 12.181 (10.983)\n",
      "2022-11-30 19:07:40,480 | Epoch: [22][220/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1271)\tLoss (L1) 11.500 (10.972)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:07:42,717 | Epoch: [22][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1216)\tLoss (L1) 12.281 (10.955)\n",
      "2022-11-30 19:07:44,953 | Epoch: [22][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1166)\tLoss (L1) 16.880 (11.084)\n",
      "2022-11-30 19:07:47,185 | Epoch: [22][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1120)\tLoss (L1) 12.339 (11.145)\n",
      "2022-11-30 19:07:49,422 | Epoch: [22][260/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1077)\tLoss (L1) 8.472 (11.101)\n",
      "2022-11-30 19:07:51,662 | Epoch: [22][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1037)\tLoss (L1) 12.144 (11.232)\n",
      "2022-11-30 19:07:54,005 | Epoch: [22][280/297]\tTime   0.23 (  0.32)\tData 0.0000 (0.1000)\tLoss (L1) 11.793 (11.249)\n",
      "2022-11-30 19:07:56,376 | Epoch: [22][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0966)\tLoss (L1) 11.440 (11.269)\n",
      "2022-11-30 19:08:25,861 | Val: [ 0/38]\tTime 27.418 (27.418)\tLoss (MSE) 160.274 (160.274)\tLoss (L1) 9.429 (9.429)\n",
      "2022-11-30 19:08:26,575 | Val: [10/38]\tTime  0.065 ( 2.558)\tLoss (MSE) 200.932 (241.491)\tLoss (L1) 10.730 (11.253)\n",
      "2022-11-30 19:08:27,221 | Val: [20/38]\tTime  0.064 ( 1.370)\tLoss (MSE) 222.598 (223.578)\tLoss (L1) 11.886 (11.083)\n",
      "2022-11-30 19:08:27,865 | Val: [30/38]\tTime  0.064 ( 0.949)\tLoss (MSE) 248.416 (225.767)\tLoss (L1) 11.285 (11.174)\n",
      "2022-11-30 19:08:28,922 |  * Overall: MSE 220.643\tL1 11.068\tG-Mean 6.748\n",
      "2022-11-30 19:08:28,922 |  * Many: MSE 193.076\tL1 10.422\tG-Mean 6.438\n",
      "2022-11-30 19:08:28,923 |  * Median: MSE 483.430\tL1 17.242\tG-Mean 10.682\n",
      "2022-11-30 19:08:28,924 |  * Low: MSE 570.673\tL1 18.995\tG-Mean 10.619\n",
      "2022-11-30 19:08:28,925 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:08:29,096 | Epoch #22: Train loss [11.2556]; Val loss: MSE [220.6431], L1 [11.0677], G-Mean [6.7477]\n",
      "2022-11-30 19:08:57,597 | Epoch: [23][  0/297]\tTime  28.50 ( 28.50)\tData 28.3033 (28.3033)\tLoss (L1) 10.997 (10.997)\n",
      "2022-11-30 19:08:59,826 | Epoch: [23][ 10/297]\tTime   0.22 (  2.79)\tData 0.0000 (2.5732)\tLoss (L1) 31.907 (14.019)\n",
      "2022-11-30 19:09:02,064 | Epoch: [23][ 20/297]\tTime   0.22 (  1.57)\tData 0.0000 (1.3480)\tLoss (L1) 8.541 (12.675)\n",
      "2022-11-30 19:09:04,297 | Epoch: [23][ 30/297]\tTime   0.22 (  1.14)\tData 0.0000 (0.9132)\tLoss (L1) 7.440 (11.964)\n",
      "2022-11-30 19:09:06,536 | Epoch: [23][ 40/297]\tTime   0.22 (  0.91)\tData 0.0000 (0.6906)\tLoss (L1) 8.936 (11.930)\n",
      "2022-11-30 19:09:08,767 | Epoch: [23][ 50/297]\tTime   0.22 (  0.78)\tData 0.0000 (0.5552)\tLoss (L1) 7.140 (11.520)\n",
      "2022-11-30 19:09:11,002 | Epoch: [23][ 60/297]\tTime   0.23 (  0.69)\tData 0.0010 (0.4642)\tLoss (L1) 9.423 (11.153)\n",
      "2022-11-30 19:09:13,232 | Epoch: [23][ 70/297]\tTime   0.22 (  0.62)\tData 0.0000 (0.3989)\tLoss (L1) 13.449 (11.201)\n",
      "2022-11-30 19:09:15,484 | Epoch: [23][ 80/297]\tTime   0.22 (  0.57)\tData 0.0000 (0.3496)\tLoss (L1) 8.311 (11.110)\n",
      "2022-11-30 19:09:17,731 | Epoch: [23][ 90/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.3112)\tLoss (L1) 8.184 (10.994)\n",
      "2022-11-30 19:09:19,968 | Epoch: [23][100/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2804)\tLoss (L1) 10.839 (11.093)\n",
      "2022-11-30 19:09:22,198 | Epoch: [23][110/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2552)\tLoss (L1) 8.812 (11.201)\n",
      "2022-11-30 19:09:24,431 | Epoch: [23][120/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2341)\tLoss (L1) 7.507 (11.067)\n",
      "2022-11-30 19:09:26,667 | Epoch: [23][130/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2163)\tLoss (L1) 12.029 (11.009)\n",
      "2022-11-30 19:09:28,906 | Epoch: [23][140/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.2009)\tLoss (L1) 11.971 (11.018)\n",
      "2022-11-30 19:09:31,141 | Epoch: [23][150/297]\tTime   0.22 (  0.41)\tData 0.0010 (0.1876)\tLoss (L1) 7.537 (10.879)\n",
      "2022-11-30 19:09:33,423 | Epoch: [23][160/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1760)\tLoss (L1) 11.602 (10.862)\n",
      "2022-11-30 19:09:35,658 | Epoch: [23][170/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1657)\tLoss (L1) 8.688 (11.037)\n",
      "2022-11-30 19:09:37,920 | Epoch: [23][180/297]\tTime   0.24 (  0.38)\tData 0.0000 (0.1566)\tLoss (L1) 16.176 (10.967)\n",
      "2022-11-30 19:09:40,314 | Epoch: [23][190/297]\tTime   0.23 (  0.37)\tData 0.0000 (0.1484)\tLoss (L1) 8.958 (10.968)\n",
      "2022-11-30 19:09:42,619 | Epoch: [23][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1410)\tLoss (L1) 14.268 (10.959)\n",
      "2022-11-30 19:09:44,858 | Epoch: [23][210/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1343)\tLoss (L1) 10.204 (10.908)\n",
      "2022-11-30 19:09:47,099 | Epoch: [23][220/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1283)\tLoss (L1) 6.280 (10.909)\n",
      "2022-11-30 19:09:49,333 | Epoch: [23][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1227)\tLoss (L1) 8.333 (10.868)\n",
      "2022-11-30 19:09:51,569 | Epoch: [23][240/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1176)\tLoss (L1) 12.509 (10.901)\n",
      "2022-11-30 19:09:53,813 | Epoch: [23][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1129)\tLoss (L1) 10.062 (10.982)\n",
      "2022-11-30 19:09:56,059 | Epoch: [23][260/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1086)\tLoss (L1) 9.853 (11.003)\n",
      "2022-11-30 19:09:58,293 | Epoch: [23][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1046)\tLoss (L1) 8.390 (11.005)\n",
      "2022-11-30 19:10:00,534 | Epoch: [23][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1009)\tLoss (L1) 7.416 (10.969)\n",
      "2022-11-30 19:10:02,777 | Epoch: [23][290/297]\tTime   0.22 (  0.32)\tData 0.0000 (0.0975)\tLoss (L1) 9.540 (10.974)\n",
      "2022-11-30 19:10:33,863 | Val: [ 0/38]\tTime 29.122 (29.122)\tLoss (MSE) 228.132 (228.132)\tLoss (L1) 11.188 (11.188)\n",
      "2022-11-30 19:10:34,834 | Val: [10/38]\tTime  0.086 ( 2.736)\tLoss (MSE) 295.254 (259.774)\tLoss (L1) 13.139 (12.155)\n",
      "2022-11-30 19:10:35,688 | Val: [20/38]\tTime  0.084 ( 1.474)\tLoss (MSE) 350.882 (272.066)\tLoss (L1) 13.744 (12.464)\n",
      "2022-11-30 19:10:36,503 | Val: [30/38]\tTime  0.065 ( 1.025)\tLoss (MSE) 227.820 (268.751)\tLoss (L1) 11.405 (12.369)\n",
      "2022-11-30 19:10:37,631 |  * Overall: MSE 260.150\tL1 12.188\tG-Mean 7.474\n",
      "2022-11-30 19:10:37,631 |  * Many: MSE 262.427\tL1 12.267\tG-Mean 7.494\n",
      "2022-11-30 19:10:37,632 |  * Median: MSE 250.316\tL1 11.669\tG-Mean 7.340\n",
      "2022-11-30 19:10:37,632 |  * Low: MSE 89.500\tL1 8.385\tG-Mean 6.573\n",
      "2022-11-30 19:10:37,633 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:10:37,810 | Epoch #23: Train loss [10.9734]; Val loss: MSE [260.1500], L1 [12.1878], G-Mean [7.4739]\n",
      "2022-11-30 19:11:07,942 | Epoch: [24][  0/297]\tTime  30.13 ( 30.13)\tData 29.7653 (29.7653)\tLoss (L1) 8.966 (8.966)\n",
      "2022-11-30 19:11:10,234 | Epoch: [24][ 10/297]\tTime   0.23 (  2.95)\tData 0.0000 (2.7062)\tLoss (L1) 11.337 (10.174)\n",
      "2022-11-30 19:11:12,476 | Epoch: [24][ 20/297]\tTime   0.22 (  1.65)\tData 0.0000 (1.4176)\tLoss (L1) 9.565 (10.410)\n",
      "2022-11-30 19:11:14,708 | Epoch: [24][ 30/297]\tTime   0.22 (  1.19)\tData 0.0000 (0.9603)\tLoss (L1) 8.439 (9.892)\n",
      "2022-11-30 19:11:16,976 | Epoch: [24][ 40/297]\tTime   0.22 (  0.96)\tData 0.0010 (0.7262)\tLoss (L1) 12.228 (10.389)\n",
      "2022-11-30 19:11:19,202 | Epoch: [24][ 50/297]\tTime   0.22 (  0.81)\tData 0.0000 (0.5838)\tLoss (L1) 14.481 (10.405)\n",
      "2022-11-30 19:11:21,526 | Epoch: [24][ 60/297]\tTime   0.22 (  0.72)\tData 0.0000 (0.4881)\tLoss (L1) 8.388 (10.383)\n",
      "2022-11-30 19:11:23,782 | Epoch: [24][ 70/297]\tTime   0.22 (  0.65)\tData 0.0000 (0.4194)\tLoss (L1) 9.847 (10.590)\n",
      "2022-11-30 19:11:26,362 | Epoch: [24][ 80/297]\tTime   0.27 (  0.60)\tData 0.0000 (0.3676)\tLoss (L1) 8.452 (10.641)\n",
      "2022-11-30 19:11:29,136 | Epoch: [24][ 90/297]\tTime   0.28 (  0.56)\tData 0.0000 (0.3273)\tLoss (L1) 13.846 (11.081)\n",
      "2022-11-30 19:11:31,921 | Epoch: [24][100/297]\tTime   0.28 (  0.54)\tData 0.0000 (0.2949)\tLoss (L1) 8.765 (11.013)\n",
      "2022-11-30 19:11:34,700 | Epoch: [24][110/297]\tTime   0.28 (  0.51)\tData 0.0000 (0.2683)\tLoss (L1) 8.006 (11.055)\n",
      "2022-11-30 19:11:37,493 | Epoch: [24][120/297]\tTime   0.28 (  0.49)\tData 0.0010 (0.2462)\tLoss (L1) 7.734 (11.040)\n",
      "2022-11-30 19:11:40,270 | Epoch: [24][130/297]\tTime   0.27 (  0.48)\tData 0.0000 (0.2274)\tLoss (L1) 10.351 (11.000)\n",
      "2022-11-30 19:11:43,063 | Epoch: [24][140/297]\tTime   0.28 (  0.46)\tData 0.0000 (0.2113)\tLoss (L1) 8.042 (11.097)\n",
      "2022-11-30 19:11:45,850 | Epoch: [24][150/297]\tTime   0.28 (  0.45)\tData 0.0000 (0.1973)\tLoss (L1) 16.443 (11.228)\n",
      "2022-11-30 19:11:48,646 | Epoch: [24][160/297]\tTime   0.28 (  0.44)\tData 0.0010 (0.1851)\tLoss (L1) 8.259 (11.144)\n",
      "2022-11-30 19:11:51,576 | Epoch: [24][170/297]\tTime   0.31 (  0.43)\tData 0.0000 (0.1742)\tLoss (L1) 10.678 (11.118)\n",
      "2022-11-30 19:11:54,514 | Epoch: [24][180/297]\tTime   0.29 (  0.42)\tData 0.0000 (0.1646)\tLoss (L1) 19.755 (11.124)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:11:57,471 | Epoch: [24][190/297]\tTime   0.30 (  0.42)\tData 0.0000 (0.1560)\tLoss (L1) 9.375 (11.115)\n",
      "2022-11-30 19:12:00,413 | Epoch: [24][200/297]\tTime   0.30 (  0.41)\tData 0.0000 (0.1483)\tLoss (L1) 9.121 (11.079)\n",
      "2022-11-30 19:12:03,344 | Epoch: [24][210/297]\tTime   0.29 (  0.41)\tData 0.0000 (0.1413)\tLoss (L1) 7.306 (11.031)\n",
      "2022-11-30 19:12:06,286 | Epoch: [24][220/297]\tTime   0.29 (  0.40)\tData 0.0000 (0.1349)\tLoss (L1) 7.656 (11.014)\n",
      "2022-11-30 19:12:09,217 | Epoch: [24][230/297]\tTime   0.30 (  0.40)\tData 0.0000 (0.1290)\tLoss (L1) 10.346 (11.002)\n",
      "2022-11-30 19:12:12,152 | Epoch: [24][240/297]\tTime   0.29 (  0.39)\tData 0.0000 (0.1237)\tLoss (L1) 8.434 (11.057)\n",
      "2022-11-30 19:12:15,071 | Epoch: [24][250/297]\tTime   0.29 (  0.39)\tData 0.0000 (0.1188)\tLoss (L1) 7.799 (11.039)\n",
      "2022-11-30 19:12:17,995 | Epoch: [24][260/297]\tTime   0.29 (  0.38)\tData 0.0000 (0.1142)\tLoss (L1) 7.518 (11.039)\n",
      "2022-11-30 19:12:20,919 | Epoch: [24][270/297]\tTime   0.29 (  0.38)\tData 0.0000 (0.1100)\tLoss (L1) 9.870 (10.961)\n",
      "2022-11-30 19:12:23,951 | Epoch: [24][280/297]\tTime   0.30 (  0.38)\tData 0.0000 (0.1061)\tLoss (L1) 12.885 (10.942)\n",
      "2022-11-30 19:12:26,939 | Epoch: [24][290/297]\tTime   0.31 (  0.38)\tData 0.0000 (0.1025)\tLoss (L1) 8.056 (10.899)\n",
      "2022-11-30 19:12:59,487 | Val: [ 0/38]\tTime 29.994 (29.994)\tLoss (MSE) 172.498 (172.498)\tLoss (L1) 10.407 (10.407)\n",
      "2022-11-30 19:13:00,178 | Val: [10/38]\tTime  0.064 ( 2.790)\tLoss (MSE) 297.267 (234.402)\tLoss (L1) 13.618 (11.382)\n",
      "2022-11-30 19:13:00,866 | Val: [20/38]\tTime  0.065 ( 1.494)\tLoss (MSE) 237.349 (230.117)\tLoss (L1) 11.493 (11.424)\n",
      "2022-11-30 19:13:01,573 | Val: [30/38]\tTime  0.075 ( 1.035)\tLoss (MSE) 175.972 (225.079)\tLoss (L1) 10.524 (11.299)\n",
      "2022-11-30 19:13:02,643 |  * Overall: MSE 219.225\tL1 11.168\tG-Mean 6.927\n",
      "2022-11-30 19:13:02,643 |  * Many: MSE 219.864\tL1 11.178\tG-Mean 6.940\n",
      "2022-11-30 19:13:02,644 |  * Median: MSE 219.231\tL1 11.198\tG-Mean 6.777\n",
      "2022-11-30 19:13:02,644 |  * Low: MSE 138.254\tL1 9.446\tG-Mean 7.024\n",
      "2022-11-30 19:13:02,645 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:13:02,820 | Epoch #24: Train loss [10.8733]; Val loss: MSE [219.2247], L1 [11.1677], G-Mean [6.9269]\n",
      "2022-11-30 19:13:32,388 | Epoch: [25][  0/297]\tTime  29.57 ( 29.57)\tData 29.2540 (29.2540)\tLoss (L1) 7.913 (7.913)\n",
      "2022-11-30 19:13:34,675 | Epoch: [25][ 10/297]\tTime   0.23 (  2.90)\tData 0.0000 (2.6597)\tLoss (L1) 19.992 (11.698)\n",
      "2022-11-30 19:13:36,955 | Epoch: [25][ 20/297]\tTime   0.22 (  1.63)\tData 0.0000 (1.3933)\tLoss (L1) 7.962 (10.502)\n",
      "2022-11-30 19:13:39,236 | Epoch: [25][ 30/297]\tTime   0.22 (  1.17)\tData 0.0000 (0.9439)\tLoss (L1) 7.866 (10.497)\n",
      "2022-11-30 19:13:41,551 | Epoch: [25][ 40/297]\tTime   0.24 (  0.94)\tData 0.0000 (0.7137)\tLoss (L1) 25.536 (11.022)\n",
      "2022-11-30 19:13:43,872 | Epoch: [25][ 50/297]\tTime   0.23 (  0.80)\tData 0.0000 (0.5738)\tLoss (L1) 10.410 (11.045)\n",
      "2022-11-30 19:13:46,307 | Epoch: [25][ 60/297]\tTime   0.30 (  0.71)\tData 0.0000 (0.4798)\tLoss (L1) 14.190 (10.857)\n",
      "2022-11-30 19:13:48,704 | Epoch: [25][ 70/297]\tTime   0.26 (  0.65)\tData 0.0000 (0.4122)\tLoss (L1) 9.503 (11.097)\n",
      "2022-11-30 19:13:51,270 | Epoch: [25][ 80/297]\tTime   0.25 (  0.60)\tData 0.0000 (0.3614)\tLoss (L1) 11.903 (11.179)\n",
      "2022-11-30 19:13:53,866 | Epoch: [25][ 90/297]\tTime   0.26 (  0.56)\tData 0.0000 (0.3217)\tLoss (L1) 14.610 (10.978)\n",
      "2022-11-30 19:13:56,370 | Epoch: [25][100/297]\tTime   0.23 (  0.53)\tData 0.0010 (0.2899)\tLoss (L1) 9.006 (10.761)\n",
      "2022-11-30 19:13:58,732 | Epoch: [25][110/297]\tTime   0.27 (  0.50)\tData 0.0000 (0.2638)\tLoss (L1) 6.298 (10.601)\n",
      "2022-11-30 19:14:01,254 | Epoch: [25][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2420)\tLoss (L1) 10.413 (10.509)\n",
      "2022-11-30 19:14:03,488 | Epoch: [25][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2235)\tLoss (L1) 10.012 (10.454)\n",
      "2022-11-30 19:14:05,923 | Epoch: [25][140/297]\tTime   0.30 (  0.45)\tData 0.0000 (0.2077)\tLoss (L1) 10.575 (10.484)\n",
      "2022-11-30 19:14:08,157 | Epoch: [25][150/297]\tTime   0.23 (  0.43)\tData 0.0000 (0.1939)\tLoss (L1) 7.883 (10.608)\n",
      "2022-11-30 19:14:10,469 | Epoch: [25][160/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1819)\tLoss (L1) 10.297 (10.592)\n",
      "2022-11-30 19:14:12,703 | Epoch: [25][170/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1713)\tLoss (L1) 13.298 (10.583)\n",
      "2022-11-30 19:14:15,026 | Epoch: [25][180/297]\tTime   0.24 (  0.40)\tData 0.0000 (0.1618)\tLoss (L1) 7.609 (10.670)\n",
      "2022-11-30 19:14:17,514 | Epoch: [25][190/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1533)\tLoss (L1) 9.799 (10.746)\n",
      "2022-11-30 19:14:19,809 | Epoch: [25][200/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1457)\tLoss (L1) 10.098 (10.702)\n",
      "2022-11-30 19:14:22,076 | Epoch: [25][210/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1388)\tLoss (L1) 6.468 (10.642)\n",
      "2022-11-30 19:14:24,329 | Epoch: [25][220/297]\tTime   0.23 (  0.37)\tData 0.0010 (0.1326)\tLoss (L1) 6.998 (10.608)\n",
      "2022-11-30 19:14:26,608 | Epoch: [25][230/297]\tTime   0.23 (  0.36)\tData 0.0010 (0.1268)\tLoss (L1) 9.042 (10.552)\n",
      "2022-11-30 19:14:28,857 | Epoch: [25][240/297]\tTime   0.23 (  0.36)\tData 0.0000 (0.1216)\tLoss (L1) 9.802 (10.521)\n",
      "2022-11-30 19:14:31,096 | Epoch: [25][250/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1167)\tLoss (L1) 6.772 (10.499)\n",
      "2022-11-30 19:14:33,345 | Epoch: [25][260/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1123)\tLoss (L1) 10.237 (10.505)\n",
      "2022-11-30 19:14:35,611 | Epoch: [25][270/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1081)\tLoss (L1) 11.144 (10.463)\n",
      "2022-11-30 19:14:37,884 | Epoch: [25][280/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1043)\tLoss (L1) 13.680 (10.564)\n",
      "2022-11-30 19:14:40,132 | Epoch: [25][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1007)\tLoss (L1) 11.680 (10.566)\n",
      "2022-11-30 19:15:14,132 | Val: [ 0/38]\tTime 31.975 (31.975)\tLoss (MSE) 184.192 (184.192)\tLoss (L1) 10.432 (10.432)\n",
      "2022-11-30 19:15:14,850 | Val: [10/38]\tTime  0.070 ( 2.972)\tLoss (MSE) 272.017 (252.561)\tLoss (L1) 13.053 (11.982)\n",
      "2022-11-30 19:15:15,512 | Val: [20/38]\tTime  0.064 ( 1.588)\tLoss (MSE) 286.590 (261.929)\tLoss (L1) 12.270 (12.265)\n",
      "2022-11-30 19:15:16,197 | Val: [30/38]\tTime  0.063 ( 1.098)\tLoss (MSE) 182.112 (253.748)\tLoss (L1) 10.482 (12.054)\n",
      "2022-11-30 19:15:17,308 |  * Overall: MSE 251.311\tL1 12.029\tG-Mean 7.620\n",
      "2022-11-30 19:15:17,308 |  * Many: MSE 256.572\tL1 12.212\tG-Mean 7.744\n",
      "2022-11-30 19:15:17,309 |  * Median: MSE 206.736\tL1 10.364\tG-Mean 6.705\n",
      "2022-11-30 19:15:17,309 |  * Low: MSE 117.890\tL1 8.796\tG-Mean 4.579\n",
      "2022-11-30 19:15:17,310 | Best L1 Loss: 10.180\n",
      "2022-11-30 19:15:17,494 | Epoch #25: Train loss [10.5530]; Val loss: MSE [251.3111], L1 [12.0293], G-Mean [7.6200]\n",
      "2022-11-30 19:15:47,783 | Epoch: [26][  0/297]\tTime  30.29 ( 30.29)\tData 30.1853 (30.1853)\tLoss (L1) 7.733 (7.733)\n",
      "2022-11-30 19:15:50,110 | Epoch: [26][ 10/297]\tTime   0.22 (  2.96)\tData 0.0000 (2.7443)\tLoss (L1) 10.722 (11.135)\n",
      "2022-11-30 19:15:52,477 | Epoch: [26][ 20/297]\tTime   0.22 (  1.67)\tData 0.0010 (1.4376)\tLoss (L1) 7.286 (11.801)\n",
      "2022-11-30 19:15:54,979 | Epoch: [26][ 30/297]\tTime   0.27 (  1.21)\tData 0.0000 (0.9739)\tLoss (L1) 7.705 (11.479)\n",
      "2022-11-30 19:15:57,294 | Epoch: [26][ 40/297]\tTime   0.22 (  0.97)\tData 0.0000 (0.7364)\tLoss (L1) 10.982 (11.106)\n",
      "2022-11-30 19:15:59,698 | Epoch: [26][ 50/297]\tTime   0.28 (  0.83)\tData 0.0000 (0.5921)\tLoss (L1) 10.640 (11.384)\n",
      "2022-11-30 19:16:02,154 | Epoch: [26][ 60/297]\tTime   0.22 (  0.73)\tData 0.0000 (0.4950)\tLoss (L1) 9.217 (11.483)\n",
      "2022-11-30 19:16:04,568 | Epoch: [26][ 70/297]\tTime   0.23 (  0.66)\tData 0.0000 (0.4253)\tLoss (L1) 9.091 (11.517)\n",
      "2022-11-30 19:16:07,116 | Epoch: [26][ 80/297]\tTime   0.25 (  0.61)\tData 0.0000 (0.3729)\tLoss (L1) 12.095 (11.279)\n",
      "2022-11-30 19:16:09,532 | Epoch: [26][ 90/297]\tTime   0.22 (  0.57)\tData 0.0000 (0.3319)\tLoss (L1) 8.265 (11.413)\n",
      "2022-11-30 19:16:11,820 | Epoch: [26][100/297]\tTime   0.24 (  0.54)\tData 0.0000 (0.2991)\tLoss (L1) 13.263 (11.203)\n",
      "2022-11-30 19:16:14,185 | Epoch: [26][110/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2722)\tLoss (L1) 12.403 (11.065)\n",
      "2022-11-30 19:16:16,622 | Epoch: [26][120/297]\tTime   0.25 (  0.49)\tData 0.0000 (0.2497)\tLoss (L1) 8.559 (11.046)\n",
      "2022-11-30 19:16:18,865 | Epoch: [26][130/297]\tTime   0.22 (  0.47)\tData 0.0000 (0.2306)\tLoss (L1) 8.261 (10.905)\n",
      "2022-11-30 19:16:21,118 | Epoch: [26][140/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2143)\tLoss (L1) 12.310 (10.903)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:16:23,580 | Epoch: [26][150/297]\tTime   0.24 (  0.44)\tData 0.0000 (0.2001)\tLoss (L1) 16.965 (10.893)\n",
      "2022-11-30 19:16:25,863 | Epoch: [26][160/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1877)\tLoss (L1) 10.499 (11.003)\n",
      "2022-11-30 19:16:28,101 | Epoch: [26][170/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1767)\tLoss (L1) 10.981 (11.044)\n",
      "2022-11-30 19:16:30,334 | Epoch: [26][180/297]\tTime   0.22 (  0.40)\tData 0.0010 (0.1670)\tLoss (L1) 13.140 (11.057)\n",
      "2022-11-30 19:16:32,567 | Epoch: [26][190/297]\tTime   0.22 (  0.39)\tData 0.0010 (0.1582)\tLoss (L1) 11.876 (11.004)\n",
      "2022-11-30 19:16:34,804 | Epoch: [26][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1504)\tLoss (L1) 9.249 (11.010)\n",
      "2022-11-30 19:16:37,040 | Epoch: [26][210/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1433)\tLoss (L1) 8.535 (11.049)\n",
      "2022-11-30 19:16:39,276 | Epoch: [26][220/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1368)\tLoss (L1) 10.616 (11.089)\n",
      "2022-11-30 19:16:41,512 | Epoch: [26][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1309)\tLoss (L1) 7.694 (11.036)\n",
      "2022-11-30 19:16:43,750 | Epoch: [26][240/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1254)\tLoss (L1) 8.083 (11.055)\n",
      "2022-11-30 19:16:45,984 | Epoch: [26][250/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1205)\tLoss (L1) 7.892 (10.954)\n",
      "2022-11-30 19:16:48,221 | Epoch: [26][260/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1158)\tLoss (L1) 6.930 (10.866)\n",
      "2022-11-30 19:16:50,456 | Epoch: [26][270/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1116)\tLoss (L1) 11.242 (10.831)\n",
      "2022-11-30 19:16:52,696 | Epoch: [26][280/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1076)\tLoss (L1) 8.174 (10.796)\n",
      "2022-11-30 19:16:54,936 | Epoch: [26][290/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1039)\tLoss (L1) 7.980 (10.805)\n",
      "2022-11-30 19:17:24,631 | Val: [ 0/38]\tTime 27.658 (27.658)\tLoss (MSE) 123.235 (123.235)\tLoss (L1) 8.838 (8.838)\n",
      "2022-11-30 19:17:25,412 | Val: [10/38]\tTime  0.065 ( 2.585)\tLoss (MSE) 150.232 (172.906)\tLoss (L1) 9.930 (9.639)\n",
      "2022-11-30 19:17:26,057 | Val: [20/38]\tTime  0.064 ( 1.385)\tLoss (MSE) 246.713 (175.269)\tLoss (L1) 11.657 (9.813)\n",
      "2022-11-30 19:17:26,704 | Val: [30/38]\tTime  0.064 ( 0.959)\tLoss (MSE) 136.559 (174.315)\tLoss (L1) 8.778 (9.795)\n",
      "2022-11-30 19:17:27,761 |  * Overall: MSE 169.011\tL1 9.690\tG-Mean 6.032\n",
      "2022-11-30 19:17:27,762 |  * Many: MSE 162.146\tL1 9.518\tG-Mean 5.926\n",
      "2022-11-30 19:17:27,762 |  * Median: MSE 239.439\tL1 11.350\tG-Mean 7.038\n",
      "2022-11-30 19:17:27,763 |  * Low: MSE 196.607\tL1 11.576\tG-Mean 8.910\n",
      "2022-11-30 19:17:27,764 | Best L1 Loss: 9.690\n",
      "2022-11-30 19:17:27,937 | ===> Saving current best checkpoint...\n",
      "2022-11-30 19:17:28,010 | Epoch #26: Train loss [10.8528]; Val loss: MSE [169.0111], L1 [9.6899], G-Mean [6.0315]\n",
      "2022-11-30 19:17:57,425 | Epoch: [27][  0/297]\tTime  29.41 ( 29.41)\tData 29.1102 (29.1102)\tLoss (L1) 11.095 (11.095)\n",
      "2022-11-30 19:17:59,663 | Epoch: [27][ 10/297]\tTime   0.22 (  2.88)\tData 0.0000 (2.6466)\tLoss (L1) 10.394 (10.578)\n",
      "2022-11-30 19:18:01,902 | Epoch: [27][ 20/297]\tTime   0.22 (  1.61)\tData 0.0000 (1.3863)\tLoss (L1) 7.977 (10.198)\n",
      "2022-11-30 19:18:04,140 | Epoch: [27][ 30/297]\tTime   0.23 (  1.17)\tData 0.0000 (0.9391)\tLoss (L1) 9.004 (10.103)\n",
      "2022-11-30 19:18:06,623 | Epoch: [27][ 40/297]\tTime   0.23 (  0.94)\tData 0.0000 (0.7101)\tLoss (L1) 15.958 (9.868)\n",
      "2022-11-30 19:18:08,961 | Epoch: [27][ 50/297]\tTime   0.24 (  0.80)\tData 0.0000 (0.5709)\tLoss (L1) 8.763 (9.745)\n",
      "2022-11-30 19:18:11,235 | Epoch: [27][ 60/297]\tTime   0.22 (  0.71)\tData 0.0010 (0.4773)\tLoss (L1) 10.357 (9.681)\n",
      "2022-11-30 19:18:13,502 | Epoch: [27][ 70/297]\tTime   0.22 (  0.64)\tData 0.0000 (0.4101)\tLoss (L1) 8.042 (9.675)\n",
      "2022-11-30 19:18:15,730 | Epoch: [27][ 80/297]\tTime   0.22 (  0.59)\tData 0.0000 (0.3595)\tLoss (L1) 10.301 (9.598)\n",
      "2022-11-30 19:18:17,963 | Epoch: [27][ 90/297]\tTime   0.22 (  0.55)\tData 0.0000 (0.3200)\tLoss (L1) 12.444 (9.568)\n",
      "2022-11-30 19:18:20,196 | Epoch: [27][100/297]\tTime   0.22 (  0.52)\tData 0.0010 (0.2884)\tLoss (L1) 9.654 (9.507)\n",
      "2022-11-30 19:18:22,431 | Epoch: [27][110/297]\tTime   0.22 (  0.49)\tData 0.0000 (0.2624)\tLoss (L1) 7.195 (9.496)\n",
      "2022-11-30 19:18:24,812 | Epoch: [27][120/297]\tTime   0.25 (  0.47)\tData 0.0000 (0.2407)\tLoss (L1) 20.035 (9.568)\n",
      "2022-11-30 19:18:27,147 | Epoch: [27][130/297]\tTime   0.22 (  0.45)\tData 0.0000 (0.2224)\tLoss (L1) 7.126 (9.588)\n",
      "2022-11-30 19:18:29,414 | Epoch: [27][140/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2066)\tLoss (L1) 10.001 (9.894)\n",
      "2022-11-30 19:18:31,675 | Epoch: [27][150/297]\tTime   0.23 (  0.42)\tData 0.0000 (0.1929)\tLoss (L1) 26.892 (10.056)\n",
      "2022-11-30 19:18:33,909 | Epoch: [27][160/297]\tTime   0.22 (  0.41)\tData 0.0000 (0.1810)\tLoss (L1) 13.717 (10.038)\n",
      "2022-11-30 19:18:36,144 | Epoch: [27][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1704)\tLoss (L1) 8.865 (10.040)\n",
      "2022-11-30 19:18:38,382 | Epoch: [27][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1610)\tLoss (L1) 10.681 (10.012)\n",
      "2022-11-30 19:18:40,618 | Epoch: [27][190/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1526)\tLoss (L1) 10.269 (10.126)\n",
      "2022-11-30 19:18:42,855 | Epoch: [27][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1450)\tLoss (L1) 12.786 (10.198)\n",
      "2022-11-30 19:18:45,092 | Epoch: [27][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1381)\tLoss (L1) 7.580 (10.169)\n",
      "2022-11-30 19:18:47,325 | Epoch: [27][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1319)\tLoss (L1) 8.786 (10.189)\n",
      "2022-11-30 19:18:49,559 | Epoch: [27][230/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1262)\tLoss (L1) 13.519 (10.230)\n",
      "2022-11-30 19:18:51,793 | Epoch: [27][240/297]\tTime   0.22 (  0.35)\tData 0.0000 (0.1209)\tLoss (L1) 15.490 (10.256)\n",
      "2022-11-30 19:18:54,040 | Epoch: [27][250/297]\tTime   0.23 (  0.34)\tData 0.0000 (0.1161)\tLoss (L1) 10.562 (10.387)\n",
      "2022-11-30 19:18:56,275 | Epoch: [27][260/297]\tTime   0.23 (  0.34)\tData 0.0010 (0.1117)\tLoss (L1) 11.774 (10.401)\n",
      "2022-11-30 19:18:58,521 | Epoch: [27][270/297]\tTime   0.24 (  0.33)\tData 0.0000 (0.1076)\tLoss (L1) 10.462 (10.402)\n",
      "2022-11-30 19:19:00,751 | Epoch: [27][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1037)\tLoss (L1) 9.262 (10.585)\n",
      "2022-11-30 19:19:02,996 | Epoch: [27][290/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1002)\tLoss (L1) 9.490 (10.562)\n",
      "2022-11-30 19:19:32,686 | Val: [ 0/38]\tTime 27.743 (27.743)\tLoss (MSE) 201.075 (201.075)\tLoss (L1) 11.303 (11.303)\n",
      "2022-11-30 19:19:33,371 | Val: [10/38]\tTime  0.064 ( 2.584)\tLoss (MSE) 377.256 (268.304)\tLoss (L1) 14.117 (11.946)\n",
      "2022-11-30 19:19:34,027 | Val: [20/38]\tTime  0.065 ( 1.385)\tLoss (MSE) 348.381 (261.125)\tLoss (L1) 13.083 (11.923)\n",
      "2022-11-30 19:19:34,680 | Val: [30/38]\tTime  0.065 ( 0.959)\tLoss (MSE) 224.318 (256.642)\tLoss (L1) 11.614 (11.919)\n",
      "2022-11-30 19:19:35,806 |  * Overall: MSE 252.361\tL1 11.832\tG-Mean 7.315\n",
      "2022-11-30 19:19:35,806 |  * Many: MSE 253.621\tL1 11.843\tG-Mean 7.310\n",
      "2022-11-30 19:19:35,807 |  * Median: MSE 249.044\tL1 11.931\tG-Mean 7.529\n",
      "2022-11-30 19:19:35,807 |  * Low: MSE 132.615\tL1 9.191\tG-Mean 5.703\n",
      "2022-11-30 19:19:35,808 | Best L1 Loss: 9.690\n",
      "2022-11-30 19:19:35,989 | Epoch #27: Train loss [10.5617]; Val loss: MSE [252.3613], L1 [11.8318], G-Mean [7.3154]\n",
      "2022-11-30 19:20:05,291 | Epoch: [28][  0/297]\tTime  29.30 ( 29.30)\tData 29.0211 (29.0211)\tLoss (L1) 7.995 (7.995)\n",
      "2022-11-30 19:20:07,547 | Epoch: [28][ 10/297]\tTime   0.22 (  2.87)\tData 0.0000 (2.6385)\tLoss (L1) 9.859 (10.692)\n",
      "2022-11-30 19:20:09,776 | Epoch: [28][ 20/297]\tTime   0.23 (  1.61)\tData 0.0000 (1.3821)\tLoss (L1) 14.857 (10.106)\n",
      "2022-11-30 19:20:12,008 | Epoch: [28][ 30/297]\tTime   0.22 (  1.16)\tData 0.0000 (0.9363)\tLoss (L1) 8.131 (9.728)\n",
      "2022-11-30 19:20:14,333 | Epoch: [28][ 40/297]\tTime   0.22 (  0.94)\tData 0.0010 (0.7080)\tLoss (L1) 11.167 (10.088)\n",
      "2022-11-30 19:20:16,566 | Epoch: [28][ 50/297]\tTime   0.22 (  0.80)\tData 0.0000 (0.5692)\tLoss (L1) 7.900 (10.257)\n",
      "2022-11-30 19:20:18,796 | Epoch: [28][ 60/297]\tTime   0.22 (  0.70)\tData 0.0010 (0.4759)\tLoss (L1) 8.190 (10.205)\n",
      "2022-11-30 19:20:21,121 | Epoch: [28][ 70/297]\tTime   0.22 (  0.64)\tData 0.0010 (0.4089)\tLoss (L1) 10.136 (10.446)\n",
      "2022-11-30 19:20:23,401 | Epoch: [28][ 80/297]\tTime   0.22 (  0.59)\tData 0.0000 (0.3585)\tLoss (L1) 16.541 (10.482)\n",
      "2022-11-30 19:20:25,689 | Epoch: [28][ 90/297]\tTime   0.28 (  0.55)\tData 0.0000 (0.3191)\tLoss (L1) 6.677 (10.403)\n",
      "2022-11-30 19:20:27,985 | Epoch: [28][100/297]\tTime   0.22 (  0.51)\tData 0.0000 (0.2875)\tLoss (L1) 10.574 (10.254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:20:30,275 | Epoch: [28][110/297]\tTime   0.23 (  0.49)\tData 0.0000 (0.2616)\tLoss (L1) 5.043 (10.422)\n",
      "2022-11-30 19:20:32,676 | Epoch: [28][120/297]\tTime   0.23 (  0.47)\tData 0.0000 (0.2400)\tLoss (L1) 10.762 (10.370)\n",
      "2022-11-30 19:20:35,014 | Epoch: [28][130/297]\tTime   0.23 (  0.45)\tData 0.0000 (0.2217)\tLoss (L1) 8.699 (10.296)\n",
      "2022-11-30 19:20:37,261 | Epoch: [28][140/297]\tTime   0.22 (  0.43)\tData 0.0000 (0.2060)\tLoss (L1) 9.454 (10.439)\n",
      "2022-11-30 19:20:39,532 | Epoch: [28][150/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1924)\tLoss (L1) 8.275 (10.400)\n",
      "2022-11-30 19:20:41,778 | Epoch: [28][160/297]\tTime   0.23 (  0.41)\tData 0.0000 (0.1804)\tLoss (L1) 8.285 (10.405)\n",
      "2022-11-30 19:20:44,025 | Epoch: [28][170/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1699)\tLoss (L1) 6.396 (10.361)\n",
      "2022-11-30 19:20:46,265 | Epoch: [28][180/297]\tTime   0.22 (  0.39)\tData 0.0000 (0.1605)\tLoss (L1) 6.270 (10.245)\n",
      "2022-11-30 19:20:48,508 | Epoch: [28][190/297]\tTime   0.23 (  0.38)\tData 0.0000 (0.1521)\tLoss (L1) 9.672 (10.358)\n",
      "2022-11-30 19:20:50,751 | Epoch: [28][200/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1445)\tLoss (L1) 33.966 (10.424)\n",
      "2022-11-30 19:20:52,994 | Epoch: [28][210/297]\tTime   0.23 (  0.36)\tData 0.0010 (0.1377)\tLoss (L1) 11.289 (10.386)\n",
      "2022-11-30 19:20:55,253 | Epoch: [28][220/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1315)\tLoss (L1) 7.731 (10.466)\n",
      "2022-11-30 19:20:57,534 | Epoch: [28][230/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1258)\tLoss (L1) 8.119 (10.472)\n",
      "2022-11-30 19:20:59,796 | Epoch: [28][240/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1206)\tLoss (L1) 7.639 (10.446)\n",
      "2022-11-30 19:21:02,040 | Epoch: [28][250/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1158)\tLoss (L1) 7.604 (10.375)\n",
      "2022-11-30 19:21:04,286 | Epoch: [28][260/297]\tTime   0.22 (  0.34)\tData 0.0000 (0.1114)\tLoss (L1) 9.548 (10.411)\n",
      "2022-11-30 19:21:06,527 | Epoch: [28][270/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1073)\tLoss (L1) 12.393 (10.409)\n",
      "2022-11-30 19:21:08,780 | Epoch: [28][280/297]\tTime   0.22 (  0.33)\tData 0.0000 (0.1035)\tLoss (L1) 8.171 (10.365)\n",
      "2022-11-30 19:21:11,033 | Epoch: [28][290/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.0999)\tLoss (L1) 9.468 (10.298)\n",
      "2022-11-30 19:21:41,839 | Val: [ 0/38]\tTime 28.831 (28.831)\tLoss (MSE) 119.498 (119.498)\tLoss (L1) 9.367 (9.367)\n",
      "2022-11-30 19:21:42,510 | Val: [10/38]\tTime  0.064 ( 2.682)\tLoss (MSE) 214.447 (213.081)\tLoss (L1) 10.847 (10.836)\n",
      "2022-11-30 19:21:43,159 | Val: [20/38]\tTime  0.064 ( 1.436)\tLoss (MSE) 170.490 (202.091)\tLoss (L1) 10.992 (10.696)\n",
      "2022-11-30 19:21:43,806 | Val: [30/38]\tTime  0.064 ( 0.993)\tLoss (MSE) 255.475 (202.230)\tLoss (L1) 11.559 (10.702)\n",
      "2022-11-30 19:21:44,848 |  * Overall: MSE 194.900\tL1 10.537\tG-Mean 6.785\n",
      "2022-11-30 19:21:44,849 |  * Many: MSE 176.395\tL1 10.121\tG-Mean 6.598\n",
      "2022-11-30 19:21:44,849 |  * Median: MSE 373.889\tL1 14.455\tG-Mean 8.779\n",
      "2022-11-30 19:21:44,850 |  * Low: MSE 399.038\tL1 16.379\tG-Mean 10.634\n",
      "2022-11-30 19:21:44,851 | Best L1 Loss: 9.690\n",
      "2022-11-30 19:21:45,020 | Epoch #28: Train loss [10.3328]; Val loss: MSE [194.9004], L1 [10.5366], G-Mean [6.7847]\n",
      "2022-11-30 19:22:14,765 | Epoch: [29][  0/297]\tTime  29.74 ( 29.74)\tData 29.5994 (29.5994)\tLoss (L1) 8.317 (8.317)\n",
      "2022-11-30 19:22:17,351 | Epoch: [29][ 10/297]\tTime   0.26 (  2.94)\tData 0.0000 (2.6982)\tLoss (L1) 8.412 (9.465)\n",
      "2022-11-30 19:22:19,833 | Epoch: [29][ 20/297]\tTime   0.26 (  1.66)\tData 0.0000 (1.4134)\tLoss (L1) 19.796 (9.399)\n",
      "2022-11-30 19:22:22,358 | Epoch: [29][ 30/297]\tTime   0.23 (  1.20)\tData 0.0010 (0.9575)\tLoss (L1) 10.758 (9.960)\n",
      "2022-11-30 19:22:24,897 | Epoch: [29][ 40/297]\tTime   0.22 (  0.97)\tData 0.0000 (0.7240)\tLoss (L1) 10.144 (9.961)\n",
      "2022-11-30 19:22:27,267 | Epoch: [29][ 50/297]\tTime   0.22 (  0.83)\tData 0.0010 (0.5821)\tLoss (L1) 12.937 (9.733)\n",
      "2022-11-30 19:22:29,586 | Epoch: [29][ 60/297]\tTime   0.22 (  0.73)\tData 0.0000 (0.4867)\tLoss (L1) 8.970 (9.691)\n",
      "2022-11-30 19:22:31,854 | Epoch: [29][ 70/297]\tTime   0.23 (  0.66)\tData 0.0000 (0.4182)\tLoss (L1) 8.921 (9.671)\n",
      "2022-11-30 19:22:34,088 | Epoch: [29][ 80/297]\tTime   0.22 (  0.61)\tData 0.0000 (0.3666)\tLoss (L1) 7.759 (9.572)\n",
      "2022-11-30 19:22:36,316 | Epoch: [29][ 90/297]\tTime   0.22 (  0.56)\tData 0.0000 (0.3263)\tLoss (L1) 9.464 (9.550)\n",
      "2022-11-30 19:22:38,549 | Epoch: [29][100/297]\tTime   0.22 (  0.53)\tData 0.0000 (0.2940)\tLoss (L1) 9.821 (9.743)\n",
      "2022-11-30 19:22:40,783 | Epoch: [29][110/297]\tTime   0.22 (  0.50)\tData 0.0000 (0.2675)\tLoss (L1) 8.520 (9.693)\n",
      "2022-11-30 19:22:43,017 | Epoch: [29][120/297]\tTime   0.22 (  0.48)\tData 0.0000 (0.2454)\tLoss (L1) 11.506 (9.669)\n",
      "2022-11-30 19:22:45,250 | Epoch: [29][130/297]\tTime   0.22 (  0.46)\tData 0.0000 (0.2267)\tLoss (L1) 11.144 (9.793)\n",
      "2022-11-30 19:22:47,483 | Epoch: [29][140/297]\tTime   0.22 (  0.44)\tData 0.0000 (0.2106)\tLoss (L1) 9.444 (9.751)\n",
      "2022-11-30 19:22:49,717 | Epoch: [29][150/297]\tTime   0.23 (  0.43)\tData 0.0000 (0.1967)\tLoss (L1) 7.410 (9.760)\n",
      "2022-11-30 19:22:51,949 | Epoch: [29][160/297]\tTime   0.22 (  0.42)\tData 0.0000 (0.1845)\tLoss (L1) 7.226 (9.805)\n",
      "2022-11-30 19:22:54,194 | Epoch: [29][170/297]\tTime   0.23 (  0.40)\tData 0.0010 (0.1737)\tLoss (L1) 7.210 (9.805)\n",
      "2022-11-30 19:22:56,547 | Epoch: [29][180/297]\tTime   0.22 (  0.40)\tData 0.0000 (0.1641)\tLoss (L1) 8.090 (9.776)\n",
      "2022-11-30 19:22:58,855 | Epoch: [29][190/297]\tTime   0.23 (  0.39)\tData 0.0000 (0.1556)\tLoss (L1) 11.728 (9.794)\n",
      "2022-11-30 19:23:01,179 | Epoch: [29][200/297]\tTime   0.22 (  0.38)\tData 0.0000 (0.1478)\tLoss (L1) 10.581 (9.852)\n",
      "2022-11-30 19:23:03,422 | Epoch: [29][210/297]\tTime   0.22 (  0.37)\tData 0.0000 (0.1408)\tLoss (L1) 8.415 (10.019)\n",
      "2022-11-30 19:23:05,747 | Epoch: [29][220/297]\tTime   0.24 (  0.37)\tData 0.0000 (0.1345)\tLoss (L1) 7.243 (10.059)\n",
      "2022-11-30 19:23:08,010 | Epoch: [29][230/297]\tTime   0.22 (  0.36)\tData 0.0000 (0.1286)\tLoss (L1) 9.069 (10.086)\n",
      "2022-11-30 19:23:10,263 | Epoch: [29][240/297]\tTime   0.23 (  0.35)\tData 0.0010 (0.1233)\tLoss (L1) 10.187 (10.146)\n",
      "2022-11-30 19:23:12,537 | Epoch: [29][250/297]\tTime   0.23 (  0.35)\tData 0.0000 (0.1184)\tLoss (L1) 8.847 (10.107)\n",
      "2022-11-30 19:23:14,971 | Epoch: [29][260/297]\tTime   0.35 (  0.34)\tData 0.0010 (0.1139)\tLoss (L1) 7.513 (10.168)\n",
      "2022-11-30 19:23:17,477 | Epoch: [29][270/297]\tTime   0.23 (  0.34)\tData 0.0010 (0.1097)\tLoss (L1) 6.778 (10.190)\n",
      "2022-11-30 19:23:19,824 | Epoch: [29][280/297]\tTime   0.24 (  0.34)\tData 0.0000 (0.1058)\tLoss (L1) 7.314 (10.223)\n",
      "2022-11-30 19:23:22,161 | Epoch: [29][290/297]\tTime   0.23 (  0.33)\tData 0.0000 (0.1022)\tLoss (L1) 6.252 (10.206)\n",
      "2022-11-30 19:23:51,802 | Val: [ 0/38]\tTime 27.627 (27.627)\tLoss (MSE) 108.249 (108.249)\tLoss (L1) 8.346 (8.346)\n",
      "2022-11-30 19:23:52,511 | Val: [10/38]\tTime  0.064 ( 2.576)\tLoss (MSE) 167.750 (188.135)\tLoss (L1) 10.012 (9.871)\n",
      "2022-11-30 19:23:53,162 | Val: [20/38]\tTime  0.064 ( 1.380)\tLoss (MSE) 239.983 (181.466)\tLoss (L1) 11.659 (9.897)\n",
      "2022-11-30 19:23:53,812 | Val: [30/38]\tTime  0.065 ( 0.956)\tLoss (MSE) 153.077 (177.873)\tLoss (L1) 8.756 (9.872)\n",
      "2022-11-30 19:23:54,939 |  * Overall: MSE 171.655\tL1 9.753\tG-Mean 6.013\n",
      "2022-11-30 19:23:54,940 |  * Many: MSE 164.280\tL1 9.557\tG-Mean 5.845\n",
      "2022-11-30 19:23:54,940 |  * Median: MSE 249.574\tL1 11.727\tG-Mean 7.877\n",
      "2022-11-30 19:23:54,940 |  * Low: MSE 174.408\tL1 10.940\tG-Mean 8.645\n",
      "2022-11-30 19:23:54,942 | Best L1 Loss: 9.690\n",
      "2022-11-30 19:23:55,143 | Epoch #29: Train loss [10.2620]; Val loss: MSE [171.6554], L1 [9.7526], G-Mean [6.0129]\n",
      "2022-11-30 19:23:55,144 | ========================================================================================================================\n",
      "2022-11-30 19:23:55,145 | Test best model on testset...\n",
      "2022-11-30 19:23:55,306 | Loaded best model, epoch 27, best val loss 9.6899\n",
      "2022-11-30 19:23:55,307 | TestSet:\n",
      "2022-11-30 19:23:59,880 | Test: [0/7]\tTime  4.573 ( 4.573)\tLoss (MSE) 281.750 (281.750)\tLoss (L1) 10.159 (10.159)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:154: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:155: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train and test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [4], line 147\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded best model, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, best val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestSet:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m test_loss_mse, test_loss_l1, test_loss_gmean \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest loss: MSE [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], L1 [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_l1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], G-Mean [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_gmean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestSet for age < 18:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [3], line 92\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(val_loader, model, train_labels, prefix, mode)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mprint_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     90\u001b[0m         progress\u001b[38;5;241m.\u001b[39mdisplay(idx)\n\u001b[1;32m---> 92\u001b[0m shot_dict \u001b[38;5;241m=\u001b[39m \u001b[43mshot_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m loss_gmean \u001b[38;5;241m=\u001b[39m gmean(np\u001b[38;5;241m.\u001b[39mhstack(losses_all), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m * Overall: MSE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses_mse\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mL1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses_l1\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mG-Mean \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_gmean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [3], line 157\u001b[0m, in \u001b[0;36mshot_metrics\u001b[1;34m(preds, labels, train_labels, many_shot_thr, low_shot_thr, mode)\u001b[0m\n\u001b[0;32m    154\u001b[0m shot_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(low_shot_mse) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(low_shot_cnt)\n\u001b[0;32m    155\u001b[0m shot_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(low_shot_l1) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(low_shot_cnt)\n\u001b[1;32m--> 157\u001b[0m shot_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m gmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow_shot_gmean\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m shot_dict\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newenv\\lib\\site-packages\\numpy\\core\\shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "#train and test\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f13e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56d98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 19:27:00,074 | =====> Preparing data...\n",
      "2022-11-30 19:27:00,074 | UTKFaces\n",
      "2022-11-30 19:27:00,154 | Using re-weighting: [INVERSE]\n",
      "2022-11-30 19:27:00,154 | Using LDS: [GAUSSIAN] (9/1)\n",
      "2022-11-30 19:27:00,173 | worker = 16\n",
      "2022-11-30 19:27:00,173 | Training data size: 14225\n",
      "2022-11-30 19:27:00,174 | Validation data size: 3793\n",
      "2022-11-30 19:27:00,174 | Test data size: 5690\n",
      "2022-11-30 19:27:00,175 | Test data size age < 18: 4233\n",
      "2022-11-30 19:27:00,175 | Test data size age >= 80: 673\n",
      "2022-11-30 19:27:00,176 | =====> Building model...\n",
      "2022-11-30 19:27:00,324 | ========================================================================================================================\n",
      "2022-11-30 19:27:00,325 | Test best model on testset...\n",
      "2022-11-30 19:27:00,502 | Loaded best model, epoch 27, best val loss 9.6899\n",
      "2022-11-30 19:27:00,502 | TestSet:\n",
      "2022-11-30 19:27:28,553 | Test: [ 0/89]\tTime 28.049 (28.049)\tLoss (MSE) 220.430 (220.430)\tLoss (L1) 9.582 (9.582)\n",
      "2022-11-30 19:27:29,261 | Test: [10/89]\tTime  0.070 ( 2.614)\tLoss (MSE) 92.499 (145.710)\tLoss (L1) 7.705 (7.570)\n",
      "2022-11-30 19:27:29,947 | Test: [20/89]\tTime  0.069 ( 1.402)\tLoss (MSE) 58.930 (118.760)\tLoss (L1) 6.249 (7.425)\n",
      "2022-11-30 19:27:30,624 | Test: [30/89]\tTime  0.068 ( 0.972)\tLoss (MSE) 118.879 (123.656)\tLoss (L1) 9.073 (7.855)\n",
      "2022-11-30 19:27:31,299 | Test: [40/89]\tTime  0.067 ( 0.751)\tLoss (MSE) 116.793 (122.164)\tLoss (L1) 9.051 (7.991)\n",
      "2022-11-30 19:27:31,976 | Test: [50/89]\tTime  0.067 ( 0.617)\tLoss (MSE) 155.321 (129.420)\tLoss (L1) 9.972 (8.366)\n",
      "2022-11-30 19:27:32,648 | Test: [60/89]\tTime  0.067 ( 0.527)\tLoss (MSE) 172.210 (134.710)\tLoss (L1) 11.016 (8.650)\n",
      "2022-11-30 19:27:33,319 | Test: [70/89]\tTime  0.065 ( 0.462)\tLoss (MSE) 204.634 (144.104)\tLoss (L1) 11.509 (9.035)\n",
      "2022-11-30 19:27:33,988 | Test: [80/89]\tTime  0.067 ( 0.413)\tLoss (MSE) 218.008 (153.524)\tLoss (L1) 11.903 (9.370)\n",
      "2022-11-30 19:27:36,018 |  * Overall: MSE 156.166\tL1 9.417\tG-Mean 5.890\n",
      "2022-11-30 19:27:36,019 |  * Many: MSE 144.583\tL1 9.085\tG-Mean 5.727\n",
      "2022-11-30 19:27:36,019 |  * Median: MSE 199.608\tL1 10.691\tG-Mean 6.546\n",
      "2022-11-30 19:27:36,020 |  * Low: MSE 235.802\tL1 11.445\tG-Mean 7.090\n",
      "2022-11-30 19:27:36,022 | Test loss: MSE [156.1663], L1 [9.4174], G-Mean [5.8896]\n",
      "\n",
      "2022-11-30 19:27:36,023 | TestSet for age < 18:\n",
      "2022-11-30 19:28:04,729 | Test: [ 0/67]\tTime 28.706 (28.706)\tLoss (MSE) 120.162 (120.162)\tLoss (L1) 7.198 (7.198)\n",
      "2022-11-30 19:28:05,423 | Test: [10/67]\tTime  0.065 ( 2.673)\tLoss (MSE) 290.925 (151.430)\tLoss (L1) 10.609 (7.915)\n",
      "2022-11-30 19:28:06,076 | Test: [20/67]\tTime  0.064 ( 1.431)\tLoss (MSE) 61.413 (144.868)\tLoss (L1) 5.448 (7.856)\n",
      "2022-11-30 19:28:06,724 | Test: [30/67]\tTime  0.065 ( 0.990)\tLoss (MSE) 143.502 (127.642)\tLoss (L1) 6.460 (7.186)\n",
      "2022-11-30 19:28:07,379 | Test: [40/67]\tTime  0.064 ( 0.765)\tLoss (MSE) 57.645 (115.944)\tLoss (L1) 4.878 (6.740)\n",
      "2022-11-30 19:28:08,024 | Test: [50/67]\tTime  0.064 ( 0.627)\tLoss (MSE) 105.455 (112.886)\tLoss (L1) 6.707 (6.667)\n",
      "2022-11-30 19:28:08,666 | Test: [60/67]\tTime  0.065 ( 0.535)\tLoss (MSE) 191.597 (116.244)\tLoss (L1) 7.735 (6.756)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:154: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['low']['mse'] = np.sum(low_shot_mse) / np.sum(low_shot_cnt)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:155: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['low']['l1'] = np.sum(low_shot_l1) / np.sum(low_shot_cnt)\n",
      "2022-11-30 19:28:10,164 |  * Overall: MSE 118.820\tL1 6.841\tG-Mean 3.757\n",
      "2022-11-30 19:28:10,165 |  * Many: MSE 113.797\tL1 6.581\tG-Mean 3.590\n",
      "2022-11-30 19:28:10,165 |  * Median: MSE 134.518\tL1 7.653\tG-Mean 4.329\n",
      "2022-11-30 19:28:10,166 |  * Low: MSE nan\tL1 nan\tG-Mean 0.000\n",
      "2022-11-30 19:28:10,167 | Test loss: MSE [118.8197], L1 [6.8411], G-Mean [3.7566]\n",
      "\n",
      "2022-11-30 19:28:10,168 | TestSet for age >= 80:\n",
      "2022-11-30 19:28:14,517 | Test: [ 0/11]\tTime  4.348 ( 4.348)\tLoss (MSE) 233.955 (233.955)\tLoss (L1) 11.247 (11.247)\n",
      "2022-11-30 19:28:16,173 | Test: [10/11]\tTime  0.688 ( 0.546)\tLoss (MSE) 144.801 (188.027)\tLoss (L1) 7.030 (10.492)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:148: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['many']['mse'] = np.sum(many_shot_mse) / np.sum(many_shot_cnt)\n",
      "C:\\Users\\Theo\\AppData\\Local\\Temp\\ipykernel_25000\\137651877.py:149: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shot_dict['many']['l1'] = np.sum(many_shot_l1) / np.sum(many_shot_cnt)\n",
      "2022-11-30 19:28:16,828 |  * Overall: MSE 188.027\tL1 10.492\tG-Mean 6.788\n",
      "2022-11-30 19:28:16,829 |  * Many: MSE nan\tL1 nan\tG-Mean 0.000\n",
      "2022-11-30 19:28:16,829 |  * Median: MSE 188.438\tL1 10.701\tG-Mean 7.362\n",
      "2022-11-30 19:28:16,829 |  * Low: MSE 187.386\tL1 10.167\tG-Mean 5.982\n",
      "2022-11-30 19:28:16,830 | Test loss: MSE [188.0271], L1 [10.4921], G-Mean [6.7884]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#test only\n",
    "package = None\n",
    "#package = 'agedb_resnet50_fds_gau_9_1_0_1_0.9_adam_l1_0.001_64'\n",
    "#package = 'agedb_resnet50_adam_l1_0.001_64'\n",
    "#package = 'agedb_resnet50_lds_gau_9_1_adam_l1_0.001_64'\n",
    "if args.gpu is not None:\n",
    "    print(f\"Use GPU: {args.gpu} for training\")\n",
    "\n",
    "# Data\n",
    "print('=====> Preparing data...')\n",
    "print(\"UTKFaces\")\n",
    "df = reload_data()\n",
    "\n",
    "split_size1=0.6\n",
    "split_size2=0.4\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "df_train = df.sample(frac=split_size1, random_state=random_seed)\n",
    "test_val_df = df.drop(df_train.index)\n",
    "df_val = test_val_df.sample(frac=split_size2, random_state=random_seed)\n",
    "df_test = test_val_df.drop(df_val.index)\n",
    "df_test_18, df_test_80 = df[df['age'] <18],df[df['age'] >=80]\n",
    "\n",
    "train_labels = df_train['age']\n",
    "\n",
    "\n",
    "train_dataset = AgeDB(data_dir=PATH, df=df_train, img_size=args.img_size, split='train',\n",
    "                      reweight=args.reweight, lds=args.lds, lds_kernel=args.lds_kernel, lds_ks=args.lds_ks, lds_sigma=args.lds_sigma)\n",
    "val_dataset = AgeDB(data_dir=PATH, df=df_val, img_size=args.img_size, split='val')\n",
    "test_dataset = AgeDB(data_dir=PATH, df=df_test, img_size=args.img_size, split='test')\n",
    "test_dataset_18 = AgeDB(data_dir=PATH, df=df_test_18, img_size=args.img_size, split='test')\n",
    "test_dataset_80 = AgeDB(data_dir=PATH, df=df_test_80, img_size=args.img_size, split='test')\n",
    "\n",
    "print('worker = ' +str(args.workers) )\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                        num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_loader_18 = DataLoader(test_dataset_18, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "test_loader_80 = DataLoader(test_dataset_80, batch_size=args.batch_size, shuffle=False,\n",
    "                         num_workers=args.workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Validation data size: {len(val_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")\n",
    "print(f\"Test data size age < 18: {len(test_dataset_18)}\")\n",
    "print(f\"Test data size age >= 80: {len(test_dataset_80)}\")\n",
    "\n",
    "# Model\n",
    "print('=====> Building model...')\n",
    "\n",
    "model = resnet50(fds=args.fds, bucket_num=args.bucket_num, bucket_start=args.bucket_start,\n",
    "                 start_update=args.start_update, start_smooth=args.start_smooth,\n",
    "                 kernel=args.fds_kernel, ks=args.fds_ks, sigma=args.fds_sigma, momentum=args.fds_mmt)\n",
    "\"\"\"\n",
    "model=resnet50(fds=False, bucket_num=100, bucket_start=3,\n",
    "                 start_update=0, start_smooth=1,\n",
    "                 kernel='gaussian', ks=9, sigma=1, momentum=0.9)\n",
    "\"\"\"\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# evaluate only\n",
    "if args.evaluate:\n",
    "    assert args.resume, 'Specify a trained model using [args.resume]'\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    print(f\"===> Checkpoint '{args.resume}' loaded (epoch [{checkpoint['epoch']}]), testing...\")\n",
    "    validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "\n",
    "\n",
    "if args.retrain_fc:\n",
    "    assert args.reweight != 'none' and args.pretrained\n",
    "    print('===> Retrain last regression layer only!')\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'fc' not in name and 'linear' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Loss and optimizer\n",
    "if not args.retrain_fc:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    # optimize only the last linear layer\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    names = list(filter(lambda k: k is not None, [k if v.requires_grad else None for k, v in model.module.named_parameters()]))\n",
    "    assert 1 <= len(parameters) <= 2  # fc.weight, fc.bias\n",
    "    print(f'===> Only optimize parameters: {names}')\n",
    "    optimizer = torch.optim.Adam(parameters, lr=args.lr) if args.optimizer == 'adam' else \\\n",
    "        torch.optim.SGD(parameters, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.pretrained:\n",
    "    checkpoint = torch.load(args.pretrained, map_location=\"cpu\")\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint['state_dict'].items():\n",
    "        if 'linear' not in k and 'fc' not in k:\n",
    "            new_state_dict[k] = v\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(f'===> Pretrained weights found in total: [{len(list(new_state_dict.keys()))}]')\n",
    "    print(f'===> Pre-trained model loaded: {args.pretrained}')\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(f\"===> Loading checkpoint '{args.resume}'\")\n",
    "        checkpoint = torch.load(args.resume) if args.gpu is None else \\\n",
    "            torch.load(args.resume, map_location=torch.device(f'cuda:{str(args.gpu)}'))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        args.best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(f\"===> Loaded checkpoint '{args.resume}' (Epoch [{checkpoint['epoch']}])\")\n",
    "    else:\n",
    "        print(f\"===> No checkpoint found at '{args.resume}'\")\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "# test with best checkpoint\n",
    "print(\"=\" * 120)\n",
    "print(\"Test best model on testset...\")\n",
    "checkpoint = torch.load(f\"{args.store_root}/{package if package!=None else args.store_name}/ckpt.best.pth.tar\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(f\"Loaded best model, epoch {checkpoint['epoch']}, best val loss {checkpoint['best_loss']:.4f}\")\n",
    "print(\"TestSet:\")\n",
    "test_loss_mse, test_loss_l1, test_loss_gmean = validate(test_loader, model, train_labels=train_labels, prefix='Test')\n",
    "print(f\"Test loss: MSE [{test_loss_mse:.4f}], L1 [{test_loss_l1:.4f}], G-Mean [{test_loss_gmean:.4f}]\\n\")\n",
    "print(\"TestSet for age < 18:\")\n",
    "test_loss_mse_18, test_loss_l1_18, test_loss_gmean_18 = validate(test_loader_18, model, train_labels=train_labels, prefix='Test',mode = 'low')\n",
    "print(f\"Test loss: MSE [{test_loss_mse_18:.4f}], L1 [{test_loss_l1_18:.4f}], G-Mean [{test_loss_gmean_18:.4f}]\\n\")\n",
    "print(\"TestSet for age >= 80:\")\n",
    "test_loss_mse_80, test_loss_l1_80, test_loss_gmean_80 = validate(test_loader_80, model, train_labels=train_labels, prefix='Test',mode = 'high')\n",
    "print(f\"Test loss: MSE [{test_loss_mse_80:.4f}], L1 [{test_loss_l1_80:.4f}], G-Mean [{test_loss_gmean_80:.4f}]\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64f148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
